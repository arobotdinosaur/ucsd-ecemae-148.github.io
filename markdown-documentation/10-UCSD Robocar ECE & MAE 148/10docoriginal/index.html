
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.0">
    
    
      
        <title>10docoriginal - ECE/MAE148</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.9f615399.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.649f08f9.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#table-of-contents" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="ECE/MAE148" class="md-header__button md-logo" aria-label="ECE/MAE148" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ECE/MAE148
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              10docoriginal
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="ECE/MAE148" class="md-nav__button md-logo" aria-label="ECE/MAE148" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ECE/MAE148
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Documentation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Documentation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DonkeySimulator
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Jetson Initial Setup
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../60-OpenCV%20CUDA%20Accelerated/60docfinal/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OpenCV With CUDA Acceleration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../60-OpenCV%20CUDA%20Accelerated/60doccommonmark/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OpenCV With CUDA Acceleration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../15-VESC%20Setup%20Instructions/15-VESC%20Setup%20Instructions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VESC Setup
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../10doc/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DonkeyCar Installation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../10docfinal/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DonkeyCar Installation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../11doc/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GPS Laps
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10UCSDRobocarECEMAE148.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    HTML TEST Robocar Setup
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    UCSD Robocar Framework
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../ROS2_Guide_Book.pdf" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ROS2 Guidebook
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Controller
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Winter 23
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Winter 23
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../win23team1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team 1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../win23team2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team 2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../win23team3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team 3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../win23team4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team 4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../win23team5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team 5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../win23team6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team 6
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../win23team7/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team 7
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../win23team8/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team 8
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../win23team9/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team 9
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../win23team12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team 12
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../win23team13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team 13
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../win23team14/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team 14
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../win23team15/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team 15
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<p><strong>UCSD RoboCar ECE &amp; MAE 148</strong></p>
<p>Version 20.7 - 23Oct2022</p>
<p>Prepared by</p>
<p>Dr. Jack Silberman</p>
<p>Department of Electrical and Computer Engineering</p>
<p>and</p>
<p>Dominic Nightingale</p>
<p>Department of Mechanical and Aerospace Engineering</p>
<p>University of California, San Diego</p>
<p>9500 Gilman Dr, La Jolla, CA 92093</p>
<p><img alt="" src="../10images/media/image45.png" />{width="3.3984372265966756in"
height="0.8055555555555556in"}</p>
<p><img alt="" src="../10images/media/image21.png" />{width="1.4427088801399826in"
height="1.4427088801399826in"}</p>
<h1 id="table-of-contents">Table of Contents</h1>
<p><strong><a href="#table-of-contents">Table of Contents</a> 2</strong></p>
<p><strong><a href="#introduction">Introduction</a> 4</strong></p>
<p><strong><a href="#single-board-computer-sbc-basic-setup">Single Board Computer (SBC) Basic
Setup</a> 5</strong></p>
<blockquote>
<p><a href="#jetson-nano-jtn-configuration">Jetson Nano (JTN) Configuration</a> 5</p>
<p><a href="#jetson-xavier-nx-jnx-configuration">Jetson Xavier NX Configuration</a>
5</p>
</blockquote>
<p><strong><a href="#hardware-setup">Hardware Setup</a> 6</strong></p>
<blockquote>
<p><a href="#jetson-nano-gpio-header-pinout">Jetson Nano GPIO Header PINOUT</a> 6</p>
<p><a href="#vesc">VESC</a> 7</p>
<p><a href="#_s281f6m5z2vd">VESC Hardware V6.x</a> 8</p>
<p><a href="#_fh8ltsnwmmrd">Motor Detection</a> 10</p>
<p><a href="#_2d2r0k5t70f4">Sensor Detection</a> 14</p>
<p><a href="#_9ymkdmchduhq">Enable Servo Control For Steering</a> 16</p>
<p><a href="#_ohr8s4phbsb0">VESC Hardware V4.12 (if you have V6, skip this)</a> 17</p>
<p><a href="#pwm-controller">PWM Controller</a> 18</p>
<p><a href="#wiring-adafruit-board">Wiring adafruit board</a> 19</p>
<p><a href="#detecting-the-pwm-controller">Detecting the PWM controller</a> 20</p>
<p><a href="#emergency-stop---relay">Emergency stop - Relay</a> 21</p>
<p><a href="#logitech-f710-controller">Logitech F710 controller</a> 23</p>
<p><a href="#other-joystick-controllers">Other JoyStick Controllers</a> 23</p>
<p><a href="#ld06-lidar">LD06 Lidar</a> 25</p>
<p><a href="#laser-map">Laser Map</a> 25</p>
<p><a href="#mechanical-drawing">Mechanical drawing</a> 25</p>
</blockquote>
<p><strong><a href="#_8sb9o86fl4d1">Install OpenCV from Source</a> 26</strong></p>
<p><strong><a href="#donkeycar-ai-framework">DonkeyCar AI Framework</a> 26</strong></p>
<blockquote>
<p><a href="#setting-up-the-donkeycar-ai-framework">Setting up the DonkeyCar AI
Framework</a> 26</p>
<p><a href="#create-a-virtual-environment-for-the-donkeycar-ai-framework.">Create a virtual environment for the DonkeyCar AI
Framework.</a>
27</p>
<p><a href="#_vgqx7t3x8yes">Confirm openCV build from previous steps</a> 28</p>
<p><a href="#tensorflow">Tensorflow</a> 29</p>
<p><a href="#_k329co33iptx">PyTorch</a> 32</p>
<p><a href="#section-9">Installing Donkeycar AI Framework</a> 35</p>
<p><a href="#create-a-car">Create a Car</a> 36</p>
<p><a href="#_jok809vjuom8">Donkeycar manage.py</a> 37</p>
<p><a href="#_nkz3143022ot">Modifying PWM board configuration</a> 37</p>
<p><a href="#modifying-camera">Modifying Camera</a> 37</p>
<p><a href="#quick-test">Quick Test</a> 38</p>
<p><a href="#modifying-joystick">Modifying Joystick</a> 39</p>
<p><a href="#calibration-of-the-throttle-and-steering">Calibration of the Throttle and
Steering</a> 41</p>
<p><a href="#begin-calibration">Begin Calibration</a> 42</p>
<p><a href="#saving-car-configuration-file">Saving car configuration file</a> 44</p>
<p><a href="#driving-the-robot-to-collect-data">Driving the Robot to Collect
data</a> 46</p>
</blockquote>
<p><strong><a href="#backup-of-the-usd-card">Backup of the uSD Card</a> 50</strong></p>
<blockquote>
<p><a href="#if-needed-we-have-an-jtn-usd-card-image-ready-for-plug-and-play">If needed, we have an uSD Card Image Ready for Plug and
Play</a>
52</p>
</blockquote>
<p><strong><a href="#ros-with-docker">ROS with Docker</a> 53</strong></p>
<p><strong><a href="#supporting-material">Supporting material</a> 54</strong></p>
<h1 id="_1"></h1>
<h1 id="introduction">Introduction</h1>
<p>This document was derived from the DIY RoboCar - Donkey Car Framework</p>
<p>Reference information can be found at
<a href="http://docs.donkeycar.com">[http://docs.donkeycar.com]{.underline}</a></p>
<p>At UC San Diego's Introduction to Autonomous Vehicles class (ECE
MAE148), we use an AI Framework called Donkey Car which is based on Deep
Learning / Human behavior cloning as well as we do traditional
programming using Robot Operating System (ROS2).</p>
<p>DonkeyCar can be seen as the "hello world" of affordable scaled
autonomous cars</p>
<p>We have added other features into our UCSD scale robot cars that are not
found</p>
<p>at the Default Donkey car build such as a wireless emergency off switch.
Therefore, please follow</p>
<p>the instructions found in this document vs. the default Donkey built.</p>
<p>Another framework we use called UCSD Robocar is primarily maintained and
developed by Dominic Nightingale right here at UC San Diego. UCSD
Robocar uses ROS and ROS2 for controlling our scaled robot cars which
can vary from traditional programming or machine learning to achieve an
objective. The framework works with a vast selection of sensors and
actuation methods in our inventory making it a robust framework to use
across various platforms. Has been tested on 1/16, 1/10, 1/5 scaled
robot cars and soon our go-karts.</p>
<p>As August 2019 we transitioned from the single board computer (SBC)
called Raspberry PI to the</p>
<p>Nvidia Jetson Nano. If you are using a Raspberry PI, then search in this
document for</p>
<p>Raspberry PI (RPI or PI) Configuration.</p>
<p>On 28Aug19, we updated the instructions to include Raspberry PI 4B</p>
<p>Depending on your Single Board Computer, Jetson Xavier NX, Jetson Nano,</p>
<p>then follow the related instructions.</p>
<h1 id="_2"></h1>
<h1 id="single-board-computer-sbc-basic-setup">Single Board Computer (SBC) Basic Setup</h1>
<p>We will be using the Ubuntu Linux distribution. In the class you have
access to a virtual</p>
<p>machine image file with Ubuntu.</p>
<p>We won't install ROS2 directly into the SBC, we will be using Docker
images and containers.</p>
<p>You will install OpenCV from source as part of your learning on
compiling and building</p>
<p>software from source.</p>
<h2 id="jetson-nano-jtn-configuration">Jetson Nano (JTN) Configuration</h2>
<p><a href="https://docs.google.com/document/d/1TF1uGAFeDARNwbPkgg9xg4NvWPJvrsHW2I_IN9dWQH0/edit?usp=sharing">[Instructions to configure the Jetson
Nano]{.underline}</a></p>
<h2 id="jetson-xavier-nx-jnx-configuration">Jetson Xavier NX (JNX) Configuration</h2>
<p><a href="https://docs.google.com/document/d/1mXgN9DcAj30HAsbfrHNCP-YEYqKWPTbcUssRI1Xab1A/edit?usp=sharing">[Instructions to Configure the Jetson Xaviver
NX]{.underline}</a></p>
<p><a href="https://developer.nvidia.com/embedded/jetpack-archive">[Archive location to previous JetPack
versions]{.underline}</a></p>
<h2 id="editing-remotely-with-jupyter-notebooks">Editing Remotely with Jupyter Notebooks</h2>
<p>Install Jupyter notebook on your Jetson:</p>
<hr />
<p>sudo apt install jupyter-notebook</p>
<hr />
<hr />
<p><a href="https://ljvmiranda921.github.io/notebook/2018/01/31/running-a-jupyter-notebook/">[https://ljvmiranda921.github.io/notebook/2018/01/31/running-a-jupyter-notebook/]{.underline}</a></p>
<p>Help document for editing using Jupyter notebook:</p>
<p><a href="https://docs.google.com/document/d/1ZNACJvKmQNnN7QNMwgqnzjrs9JDdFbiqVFHAuhgillQ/edit?usp=sharing">[Conifguring Jupyter Notebook on
SSH]{.underline}</a></p>
<h1 id="hardware-setup">Hardware Setup</h1>
<p>You should consider breaking the work on building the robots per team
member:</p>
<p>a)  Someone could start to build OpenCV GPU accelerated in parallel
    while you build the robot. <a href="https://docs.google.com/document/d/1HX2zmjbVsyLnliEQ8wp97Y453g5qNAYHWtFQiKQ0elA/edit?usp=sharing">[It will take several hours building
    OpenCV from
    source]{.underline}</a>.
    Try to divide the work by team members ...</p>
<p>b)  Start designing, 3D Printing, Laser Cutting the parts</p>
<p>As of Spring 2022, we upgraded all the ECE MAE 148 robots to use VESCs.</p>
<p>If you are using a regular Electronic Speed Controller (ESC) vs. a VESC
you may need to use a I2C PWM board to generate reliable PWM to the ESC
and Steering Servo, look for the PWM Controller text below.</p>
<h2 id="jetson-nano-gpio-header-pinout">Jetson Nano GPIO Header PINOUT</h2>
<p>I2C and UART pins are connected to hardware and should not be
reassigned. By default, all other pins</p>
<p>(except power) are assigned as GPIO. Pins labeled with other functions
are recommended functions if</p>
<p>using a different device tree. Here's <a href="https://www.jetsonhacks.com/wp-content/uploads/2019/05/Jetson-Nano-GPIO-mapping.xlsx">[a spreadsheet map to
RPi]{.underline}</a>
to help.</p>
<p><img alt="" src="../10images/media/image3.png" />{width="2.8347725284339456in"
height="5.244792213473316in"}</p>
<p>sudo usermod -aG i2c jetson</p>
<p>sudo reboot now</p>
<h2 id="vesc">VESC</h2>
<p>VESC is a super cool Electronic Speed Controller (ESC) that runs open
source code with</p>
<p>significantly more capabilities than a regular RC Car ESC.</p>
<p>VESCs are very popular for electrical skateboards, DIY electrical
scooters, and robotics.</p>
<p>For robotics, one of the capabilities we will use the most is Odometry
(speed and position)</p>
<p>based on the sensors on brushless motors (sensored) or to some extent,
specially using the latest VESCs and firmware, it is also available with
brushless motors without sensors (sensorless).</p>
<p><a href="https://vesc-project.com/">[https://vesc-project.com/]{.underline}</a></p>
<p><a href="https://docs.google.com/document/d/1Y5DdvWdtFjbeyGIVJyWAb8Wrq6M1MkCkMPiDgT4LoQY/edit?usp=sharing">[VESC Setup
Instructions]{.underline}</a></p>
<h2 id="logitech-f710-controller">Logitech F710 controller</h2>
<p>Place your Logitech F710 controller on the [x mode]{.mark}</p>
<p>(look for small switch in one of the controller face)</p>
<p>Connect the USB JoyStick Dongle into the JTN and then list the input
devices again</p>
<p>ls /dev/input</p>
<blockquote>
<p>(env) jetson@ucsdrobocar00:\~/projects/d3\$ ls /dev/input</p>
<p>by-id event0 event2 event4 mice mouse1</p>
<p>by-path event1 event3 [js0]{.mark} mouse0</p>
</blockquote>
<p>We are looking for a js0</p>
<h3 id="other-joystick-controllers">Other JoyStick Controllers</h3>
<p>JoyStick Controllers - either the Logitech F-10 or PS4</p>
<p>Make sure your myconfig.py on your car directory reflects your
controller</p>
<p><strong>At the SBC</strong></p>
<p>Connecting the LogiTech Controller is as easy as plugging in the USB
dongle at the SBC.</p>
<p>Or if you have a PS4 controller</p>
<p>Connecting a PS4 Controller - Bluetooth</p>
<p>Deactivate the virtual environment if you are using one</p>
<p>deactivate</p>
<p>sudo apt-get install bluetooth libbluetooth3 libusb-dev</p>
<p>sudo systemctl enable bluetooth.service</p>
<blockquote>
<p>[Need these for XBox Controller, skip for PS4]{.mark}</p>
<p>[sudo apt install sysfsutils]{.mark}</p>
<p>[sudo nano /etc/sysfs.conf]{.mark}</p>
<p>[add this line at the end of the file:]{.mark}</p>
<p>[/module/bluetooth/parameters/disable_ertm=1]{.mark}</p>
<p>[Reboot your SBC and see if ertm was disabled.]{.mark}</p>
<p>[cat /sys/module/bluetooth/parameters/disable_ertm]{.mark}</p>
<p>[The result should print Y]{.mark}</p>
</blockquote>
<p><strong>[At the PS4 controller]{.mark}</strong></p>
<p>[Press the Share and PS buttons at the same time.]{.mark}</p>
<p>[The controller light will flash like a strobe light. That means it is
in the pairing mode]{.mark}</p>
<p>[If the SBC is not seeing the PS4, you should try a mUSB cable between
the SBC and the]{.mark}</p>
<p>[PS4 controller.]{.mark}</p>
<p><strong>[At the SBC]{.mark}</strong></p>
<p>sudo bluetoothctl</p>
<p>agent on</p>
<p>default-agent</p>
<p>scan on</p>
<p>[If your controller is off, Press Share/PlayStation]{.mark}</p>
<p>[example of a PS4 controller mac address]{.mark}</p>
<blockquote>
<p>[Device A6:15:66:D1:46:1B Alias: Wireless Controller]{.mark}</p>
</blockquote>
<p>connect YOUR_MAC_ADDRESS</p>
<p>trust YOUR_MAC_ADDRESS</p>
<p>quit</p>
<p>[If your ps4 turns off, turn it on again by pressing PS]{.mark}</p>
<p>[If you want to check controller was connected]{.mark}</p>
<p>ls /dev/input</p>
<p>[and see if js0 is listed.]{.mark}</p>
<p>[Lets test the joystick in a linux machine]{.mark}</p>
<p>sudo apt-get update</p>
<p>sudo apt-get install -y jstest-gtk</p>
<p>jstest /dev/input/js0</p>
<p>[Turn off your PS4 controller by pressing and holding PS]{.mark}</p>
<p>To remove a device, let\'s say another JoyStick that you don't use
anymore</p>
<p>bluetoothctl</p>
<p>paired-devices</p>
<p>remove THE_CONTROLLER_MAC_ADDRESS</p>
<p>I had to try the method above twice, I rebooted the SBC in between</p>
<h2 id="_3"></h2>
<h2 id="ld06-lidar">LD06 Lidar</h2>
<p>Datasheet for LD06 lidar:
<a href="https://drive.google.com/file/d/1t0CkEEd9fYG_eIq_DL8eqy4NywQQDrjg/view?usp=sharing">[datasheet]{.underline}</a></p>
<h3 id="laser-map">Laser Map</h3>
<p><img alt="" src="../10images/media/image9.png" />{width="3.4479166666666665in"
height="2.9040201224846895in"}</p>
<h3 id="mechanical-drawing">Mechanical drawing</h3>
<p><img alt="" src="../10images/media/image6.png" />{width="4.705857392825897in"
height="3.932292213473316in"}</p>
<h1 id="previous-versions-of-hardware">Previous Versions of Hardware</h1>
<p>Skip the PWM Controller and EMO starting in 2022 Summer II. Left here
for people using these as low cost alternative robot.</p>
<h2 id="pwm-controller">PWM Controller</h2>
<blockquote>
<p><em>If you are using a VESC, we don't use the PWM board, you can skip
this part.</em></p>
<p><em>These PWM controllers are used only if one has a regular ESC (not the
cool VESC).</em></p>
<p><em>[We are following the standard from RC CAR world, Channel 1 for
Steering, Channel 2 for Throttle]{.mark}</em></p>
<p><em>Note:The default DonkeyCar build uses Channels 0 and 1</em></p>
<p><em>The UCSDRoboCar has at least two actuators. A steering servo and the
DC motor connected to an Electronics Speed Controller (ESC). These are
controlled by PWM (Pulse Width Modulation).</em></p>
<p><em>We use PWM Controller to generate the PWM signals, a device similar
to the one in the picture below</em></p>
<p><img alt="" src="../10images/media/image5.png" />{width="3.0268350831146105in"
height="2.1093755468066493in"}</p>
<p>Shutdown the JTN if it is on by typing this command</p>
<p>sudo shutdown -h now</p>
<p>Connect the Steering Servo to the Channel 1</p>
<p>Connect the Throttle (Electronic Speed Controller ESC) to Channel 2</p>
<p>Observe the orientation of the 3 wires connector coming from the
Steering Servo and ESC. Look for a the black or brown wire, that is
the GND (-).</p>
<p>Lets install the PWM Controller</p>
<p><a href="https://www.jetsonhacks.com/nvidia-jetson-nano-j41-header-pinout/">[https://www.jetsonhacks.com/nvidia-jetson-nano-j41-header-pinout/]{.underline}</a></p>
</blockquote>
<h3 id="wiring-adafruit-board">Wiring adafruit board</h3>
<blockquote>
<p><em>You need to connect the following pins between the JTN and the PWM
board:</em></p>
<p><em>Disconnect the power to the JTN</em></p>
<p><em>+3.3v, the two I2C pins (SDA and SCL) and ground</em></p>
<p><strong><em>The 3.3V from the JTN goes to the VCC at the PWM board</em></strong></p>
<p><em>Note: for the ground connection you need to skip one pin (skip pin
7)</em></p>
</blockquote>
<ul>
<li>
<p><em>3.3V - pin 1</em></p>
</li>
<li>
<p><em>SDA - pin 3</em></p>
</li>
<li>
<p><em>SCL- pin 5</em></p>
</li>
<li>
<p><em>No_Connect (Skip) - pin 7</em></p>
</li>
<li>
<p><em>Ground - pin 9</em></p>
</li>
</ul>
<blockquote>
<p><img alt="" src="../10images/media/image40.jpg" />{width="2.115685695538058in"
height="2.8177088801399823in"}<img alt="" src="../10images/media/image42.jpg" />{width="2.021828521434821in"
height="2.6927088801399823in"}<img alt="" src="../10images/media/image41.jpg" />{width="2.2616382327209097in"
height="3.0364588801399823in"}</p>
</blockquote>
<h3 id="_4"></h3>
<h3 id="detecting-the-pwm-controller">Detecting the PWM controller</h3>
<blockquote>
<p>Lets detect the PWM controller</p>
<p>sudo i2cdetect -r -y 1</p>
<p><em>0 1 2 3 4 5 6 7 8 9 a b c d e f</em></p>
<p><em>00: -- -- -- -- -- -- -- -- -- -- -- -- --</em></p>
<p><em>10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</em></p>
<p><em>20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</em></p>
<p><em>30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</em></p>
<p><em>40: [40]{.mark} -- -- -- -- -- -- -- -- -- -- -- -- --
-- --</em></p>
<p><em>50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</em></p>
<p><em>60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</em></p>
<p><em>70: [70]{.mark} -- -- -- -- -- -- --</em></p>
<p>If you see the 40 and 70 above, the JTN is communicating with the PWM
board</p>
<p>If you want to save some typing everytime you log into the JTN, add it
to the end of .bashrc</p>
<p>nano \~/.bashrc</p>
<p>source /projects/envs/donkey/bin/activate</p>
<p>cd \~/projects/d3</p>
<p>Test is with a reboot</p>
<p>sudo reboot now</p>
</blockquote>
<h2 id="_5"></h2>
<h2 id="emergency-stop-relay">Emergency stop - Relay</h2>
<blockquote>
<p>If using an ESC vs. VESC follow this, with VESC skip</p>
<p>Connecting the Emergency Stop Circuit and Batteries into the Robot</p>
<p>For this part of your robot, you will have to do some hacking. That is
part of the class.</p>
<p>The instructor will discuss the principle of the circuit and how to
implement it with the component in your robot kit.</p>
<p>Long story short, the PWM Controller we use has a disable pin. If the
correct logic is applied to it for example Logic 1 or 0 (3.3V or 0V)
it will disable the PWM signals and the UCSDRoboCar will stop.</p>
<p>Think why one needs a separate EMO circuit vs. relying on the
software, operating system and computer communicating with the PWM
controller that then send PWM pulses to the actuators (steering servo
and driving DC motor by the electronics speed controller)</p>
<p>First search on the web and read the datasheet of the emergency stop
switch (EMO) components provided with the robot kit and discuss with
your teammates how the EMO will work. You got two main components to
build the WireLess EMO switch:</p>
</blockquote>
<p>a)  A Wireless Relay with wireless remote controls</p>
<p>b)  A Red/Blue high power LED. This is to help the user know if the car
    is disable (Red) or Enabled (Blue).</p>
<blockquote>
<p>Do some discussion with your Team and pay attention to the Lecture
explanations:</p>
</blockquote>
<ul>
<li>
<p>What is the disable pin of the PWM controller?</p>
</li>
<li>
<p>Does it disable logic 1 or 0?</p>
</li>
<li>
<p>How to wire the wireless relay to provide the logic you need to
    disable the PWM controller? (1 or 0)</p>
</li>
<li>
<p>What pin at the Single Board Computer (SBC) can provide the logic
    you need to disable the PWM controller</p>
</li>
<li>
<p>How to connect the LEDs (Blue and Red) to indicate (BLUE - enabled),
    (RED - power is on / robot is disabled).</p>
</li>
<li>
<p>What is the fail safe situation, using the normally closed or
    normally open pins of the wireless relay to disable the PWM
    controller?</p>
</li>
<li>
<p>Note: The power to the PWM controller, that powers the LEDs, comes
    from ESC (Electronics Speed Controller). Therefore, you have to
    connect the robot batteries to the ESC and the ESC to the PWM
    controller. <strong>We are using channel 2 for the ESC</strong>. <strong>Channel 1 for
    the Steering. Then you need to power the ESC for the circuit to work
    ...</strong></p>
</li>
</ul>
<blockquote>
<p>After you see that the EMO is working, i.e. wireless remote control
disables the PWM, and the LEDs light up as planned, then you need to
document your work. Please use a schematic software such as Fritzing
(<a href="http://fritzing.org/home/">[http://fritzing.org/home/]{.underline}</a>)
to document your electrical schematic.</p>
<p>It may seem we did the opposite way; document after your build. In our
case, you learned by looking at components information, thinking about
the logic, and experimenting. Since you are an engineer you need to
document even if it was a hack / test try first...</p>
<p>Working in a company you may do fast prototyping first then document
your work when the risk is low. On larger projects you design, make
schematics, diagrams, drawings, work instructions, then build it. Keep
that in mind!</p>
<p>Now you can go drive your robot to collect data. Make sure to keep the</p>
<p>EMO handy and used when needed!</p>
<p>Also keep in mind that \&lt;X&gt; on your controller is like an emergency
break.</p>
<p>When you run Donkey it will display the functions associated with the
joystick buttons.</p>
<p>Read and remember them</p>
</blockquote>
<h1 id="_6"></h1>
<h1 id="_7"></h1>
<h1 id="install-opencv-from-source">Install OpenCV from Source</h1>
<blockquote>
<p># Installing an Open Source Computer Vision (OpenCV) package with
CUDA Support</p>
<p># As of Jan 2020, NVIDIA is not providing OpenCV optimized to use
CUDA (GPU acceleration).</p>
<p># Search the web if you are curious why.</p>
<p>#
<a href="https://forums.developer.nvidia.com/t/opencv-cuda-python-with-jetson-nano/72902">[https://forums.developer.nvidia.com/t/opencv-cuda-python-with-jetson-nano/72902]{.underline}</a></p>
</blockquote>
<p>#<a href="https://docs.google.com/document/d/1HX2zmjbVsyLnliEQ8wp97Y453g5qNAYHWtFQiKQ0elA/edit?usp=sharing">[Here are the instructions to build OpenCV from
source]{.underline}</a></p>
<p>It will take an approximate 4 hrs to install opencv</p>
<p>1. jtop</p>
<p>2. 4</p>
<p>3. Add 6 GB of swap space and enable</p>
<p>4. cd \~</p>
<p>5. nano install_opencv.sh</p>
<p>6. Copy the entire contents of the attached
<a href="https://drive.google.com/file/d/1jhsqzTNFHFMHxnDKIspjKlVHD1rL0izi/view?usp=share_link">[file]{.underline}</a></p>
<p>7. bash install_opencv.sh</p>
<h1 id="donkeycar-ai-framework">DonkeyCar AI Framework</h1>
<h2 id="setting-up-the-donkeycar-ai-framework">Setting up the DonkeyCar AI Framework</h2>
<p>Reference
<a href="http://docs.donkeycar.com">[http://docs.donkeycar.com]{.underline}</a></p>
<p>Make sure that the OpenCV you want to use supporting CUDA is already
available as a system</p>
<p>wide package.</p>
<p>Remember that when you are compiling and building software from source,
it may take a few</p>
<p>hours ...</p>
<p>SSH into the Single Board Computer (SBC) e.g., RPI, JTN, JNX, etc.</p>
<p># Install some packaged, some may be already installed</p>
<p>sudo apt update -y</p>
<p>sudo apt upgrade -y</p>
<p>sudo usermod -aG dialout jetson</p>
<blockquote>
<h1 id="if-packages-are-being-held-back">If packages are being held back</h1>
<p>sudo apt-get --with-new-pkgs upgrade</p>
</blockquote>
<p>sudo apt-get install -y build-essential python3 python3-dev python3-pip
libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev
liblapack-dev libblas-dev gfortran libxslt1-dev libxml2-dev libffi-dev
libcurl4-openssl-dev libssl-dev libpng-dev libopenblas-dev openmpi-doc
openmpi-bin libopenmpi-dev libopenblas-dev git nano</p>
<p>Install RPi.GPIO clone for Jetson Nano</p>
<p><a href="https://github.com/NVIDIA/jetson-gpio">[https://github.com/NVIDIA/jetson-gpio]{.underline}</a></p>
<p>pip3 install Jetson.GPIO</p>
<blockquote>
<p>If the pip install complains about ownership of the directory*</p>
<p>then execute the following command</p>
<p>sudo chown -R jetson:jetson /home/jetson/.cache/pip</p>
<p>ex:</p>
<p>WARNING: The directory \'/home/jetson/.cache/pip/http\' or its parent
directory is not owned by the current user and the cache has been
disabled. Please check the permissions and owner of that directory. If
executing pip with sudo, you may want sudo\'s -H flag.</p>
<p>WARNING: The directory \'/home/jetson/.cache/pip\' or its parent
directory is not owned by the current user and caching wheels has been
disabled. check the permissions and owner of</p>
<p>that directory. If executing pip with sudo, you may want sudo\'s -H
flag.</p>
<p>If pip breaks for some reason, you can reinstall it with the following
lines</p>
<p>python3 -m pip uninstall pip</p>
<p>sudo apt install python3-pip --reinstall</p>
<p>If the install request elevated privileges, execute the following
command</p>
<p>sudo pip3 install Jetson.GPIO</p>
<p>if pip has a new version</p>
<p>pip3 install --upgrade pip</p>
</blockquote>
<p>Let's make sure the user jetson can use gpio</p>
<p>sudo groupadd -f -r gpio</p>
<p>sudo usermod -a -G gpio jetson</p>
<blockquote>
<p>sudo cp /opt/nvidia/jetson-gpio/etc/99-gpio.rules /etc/udev/rules.d/</p>
<p>28Jan20 - did not work with JetPack3.4</p>
<p>15May21- did not work with JetPack4.5</p>
<p>19Oct21 - did not work with JetPack4.6</p>
<p>18Sep22 - did not work with JetPack4.6.2</p>
<p>Will get back to it later if the jetson user can not access GPIO</p>
</blockquote>
<p>sudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger</p>
<p>We want to have control over the versions of each software library to
minimize the framework from</p>
<p>breaking after system-wide upgrades. Therefore, lets create a virtual
environment for the</p>
<p>DonkeyCar.</p>
<h3 id="create-a-virtual-environment-for-the-donkeycar-ai-framework">Create a virtual environment for the DonkeyCar AI Framework.</h3>
<p>If you want the virtual environment to be under the user's home
directory, make sure to be on the</p>
<p>home directory for user jetson</p>
<p>If you have not done so, lets create a directory to store our projects
and one subdirectory</p>
<p>to store virtual environments</p>
<p>cd \~</p>
<p>mkdir projects</p>
<p>cd projects</p>
<p>mkdir envs</p>
<p>cd envs</p>
<p>pip3 install virtualenv</p>
<blockquote>
<p>if complains about user permission</p>
<p>pip3 install virtualenv --user</p>
</blockquote>
<p>We will create a virtual environment called donkey since our AI
framework is</p>
<p>based on the Donkey Car project</p>
<p>Since your SBC will be initially dedicated to the class AI framework
(Donkey), at least until</p>
<p>your custom project starts, let's activate the donkey virtual env
automatically every time the user</p>
<p>Jetson logs into the SBC. We can remove this settings later if needed
when using ROS2</p>
<p>echo \"source \~/projects/envs/donkey/bin/activate\" &gt;&gt; \~/.bashrc</p>
<p>source \~/.bashrc</p>
<p>When a virtual environment is active, you should see
(name_of_virtual_enviroment) in front of the terminal prompt.</p>
<p>ex:</p>
<p>[(donkey)]{.mark} jetson@ucsdrobocar-xxx-yy:\~\$</p>
<p>At this point, using pip and pip3 should be the same as using pip3 by
default in this virtual environment.</p>
<p>https://docs.donkeycar.com/guide/robot_sbc/setup_jetson_nano/ 46</p>
<blockquote>
<h1 id="it-is-necessary-to-create-a-link-to-it">it is necessary to create a link to it</h1>
<p># Go to the folder where OpenCV\'s native library is built</p>
<h1 id="cd-usrlocallibpython36site-packagescv2python-36">cd /usr/local/lib/python3.6/site-packages/cv2/python-3.6</h1>
<p># Rename</p>
<h1 id="mv-cv2cpython-36m-xxx-linux-gnuso-cv2so">mv cv2.cpython-36m-xxx-linux-gnu.so cv2.so</h1>
<p># Go to your virtual environments site-packages folder if previously
set</p>
<h1 id="cd-envlibpython36site-packages">cd \~/env/lib/python3.6/site-packages/</h1>
<p># Or just go to your home folder if not set a venv site-packages
folder</p>
<h1 id="cd">cd \~</h1>
<p># Symlink the native library</p>
<h1 id="ln-s-usrlocallibpython36site-packagescv2python-36cv2so">ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so</h1>
<p>cv2.so</p>
<h1 id="note-that-it-is-almost-mandatory-to-create-a-virtual-environment-in">NOTE that it is almost mandatory to create a virtual environment in</h1>
<p>order to properly install</p>
<p># tensorflow, scipy and keras, and always a best practice.</p>
</blockquote>
<p>cd \~/projects/envs/donkey/lib/python3.6/site-packages/</p>
<p>ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so
cv2.so</p>
<h3 id="_8"></h3>
<h3 id="confirm-that-opencv-built-from-previous-steps-is-working-on-the-virtual-environment-donkey">Confirm that OpenCV built from previous steps is working on the virtual environment Donkey</h3>
<p># Testing to see if OpenCV is installed in the virtual env.</p>
<p>python3 -c \'import cv2 as cv; print(cv.__version__)\'</p>
<blockquote>
<p>(donkey) <strong>jetson@ucsdrobocar-xxx-yy</strong>:<strong>\~/projects/envs/donkey</strong>\$
python3 -c \'import cv2 as cv; print(cv.__version__)\'</p>
<p>4.6.0</p>
</blockquote>
<p># We won\'t use Python2, but just in case one will need it for some
reason</p>
<p>python2 -c \'import cv2 as cv; print(cv.__version__)\'</p>
<p>We are not done installing software yet. We need to install more
dependencies..</p>
<p><strong>Make sure you have the donkey virtual environment activated</strong></p>
<p>Remember some of these installs may take a while. It does not mean that
the SBC is frozen, you</p>
<p>can see that the CPU is busy with top, htop, or jtop</p>
<p>source \~/projects/envs/donkey/bin/activate</p>
<p>pip3 install -U pip testresources setuptools</p>
<p>pip3 install -U futures==3.1.1 protobuf==3.12.2 pybind11==2.5.0</p>
<p>pip3 install -U cython==0.29.21 pyserial</p>
<p>pip3 install -U future==0.18.2 mock==4.0.2 h5py==2.10.0
keras_preprocessing==1.1.2 keras_applications==1.0.8 gast==0.3.3</p>
<p>pip3 install -U absl-py==0.9.0 py-cpuinfo==7.0.0 psutil==5.7.2
portpicker==1.3.1 six requests==2.24.0 astor==0.8.1 termcolor==1.1.0
wrapt==1.12.1 google-pasta==0.2.0</p>
<p>pip3 install -U gdown</p>
<h3 id="tensorflow">Tensorflow</h3>
<p>Now let\'s install
<a href="https://www.tensorflow.org/">[Tensorflow]{.underline}</a> (Artificial
Neural Network software).</p>
<blockquote>
<p>"TensorFlow is an end-to-end open source platform for machine
learning. It has a comprehensive, flexible ecosystem of tools,
libraries and community resources that lets researchers push the
state-of-the-art in ML and developers easily build and deploy ML
powered applications."</p>
</blockquote>
<p>Lets install Tensorflow enabled for GPU acceleration</p>
<p>Another chance for you to study while software is being installed...</p>
<p>Background information here</p>
<p><a href="https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html">[https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html]{.underline}</a></p>
<p>Remember you are using a low power SBC, depending on the size of the
software it takes a while</p>
<p>We are installing Tensorflow outside the virtual environment so it is
available for other uses</p>
<p>Here is another chance for you to study while software is being
installed...</p>
<p><a href="https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html">[Background information
here]{.underline}</a></p>
<p>We are using JetPack 4.5 because the new DonkeyCar release was breaking
the install with JetPack4.6.2. It requires Python 7 and newer.</p>
<p>Let\'s stick with JetPack4.5 for now</p>
<p><a href="https://developer.download.nvidia.com/compute/redist/jp/v45/tensorflow/">[https://developer.download.nvidia.com/compute/redist/jp/v45/tensorflow/]{.underline}</a></p>
<p>As of 18Sep22</p>
<blockquote>
<p>tensorflow-1.15.4+nv20.12-cp36-cp36m-linux_aarch64.whl 218MB
2020-12-18 14:54:04</p>
<p>tensorflow-2.3.1+nv20.12-cp36-cp36m-linux_aarch64.whl 264MB 2020-12-18
14:54:06</p>
<p>tensorflow-1.15.5+nv21.2-cp36-cp36m-linux_aarch64.whl 218MB 2021-02-26
16:10:00</p>
<p>tensorflow-2.4.0+nv21.2-cp36-cp36m-linux_aarch64.whl 273MB 2021-02-26
16:10:14</p>
<p>tensorflow-1.15.5+nv21.3-cp36-cp36m-linux_aarch64.whl 218MB 2021-03-25
18:14:17</p>
<p>tensorflow-2.4.0+nv21.3-cp36-cp36m-linux_aarch64.whl 273MB 2021-03-25
18:14:48</p>
<p>tensorflow-1.15.5+nv21.4-cp36-cp36m-linux_aarch64.whl 218MB 2021-04-26
20:36:59</p>
<p>tensorflow-2.4.0+nv21.4-cp36-cp36m-linux_aarch64.whl 273MB 2021-04-26
20:38:13</p>
<p>tensorflow-1.15.5+nv21.5-cp36-cp36m-linux_aarch64.whl 218MB 2021-05-20
20:19:08</p>
<p>tensorflow-2.4.0+nv21.5-cp36-cp36m-linux_aarch64.whl 274MB 2021-05-20
20:19:20</p>
<p>tensorflow-1.15.5+nv21.6-cp36-cp36m-linux_aarch64.whl 220MB 2021-06-29
18:18:14</p>
<p><strong>tensorflow-2.5.0</strong>+nv21.6-cp36-cp36m-linux_aarch64.whl 293MB
2021-06-29 18:18:43</p>
</blockquote>
<p># This will install the latest tensorflow compatible with the Jet Pack
as a system package</p>
<p>Alternatively if you want to chose a particular version:</p>
<h1 id="pip3-install-pre-extra-index-url">pip3 install --pre --extra-index-url</h1>
<p>https://developer.download.nvidia.com/compute/redist/jp/v45
tensorflow==<strong>2.3.1</strong></p>
<blockquote>
<p>Previous versions install</p>
<p>pip3 install --pre --extra-index-url
https://developer.download.nvidia.com/compute/redist/jp/v45
tensorflow==2.3.1</p>
<p>pip3 install --pre --extra-index-url
https://developer.download.nvidia.com/compute/redist/jp/v45
tensorflow==2.4.0</p>
</blockquote>
<p>Remember you are using a low power SBC, depending on the size of the
software it takes a while</p>
<p>Lets verify that Tensorflow installed correctly</p>
<p>python3</p>
<p>import tensorflow</p>
<p>exit()</p>
<blockquote>
<p>No errors should be reported</p>
<p><strong>If you get errors importing Tensorflow 2.5.0, try these</strong></p>
<p><strong>pip install numpy==1.19.2</strong></p>
</blockquote>
<p>ex:</p>
<p>If you see info with <strong>libcuda</strong> it means, Tensorflow will be
accelerated using the CUDA</p>
<p>cores of the SBC's GPU</p>
<p>(donkey) jetson@ucsdrobocar-xxx-yy:\~/projects\$ python3</p>
<p>Python 3.6.9 (default, Jun 29 2022, 11:45:57)</p>
<p>[GCC 8.4.0] on linux</p>
<p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more
information.</p>
<p>&gt;&gt;&gt; import tensorflow</p>
<p>2022-08-09 12:19:25.285765: I
tensorflow/stream_executor/platform/default/dso_loader.cc:53]
Successfully opened dynamic library [libcudart.]{.mark}so.10.2</p>
<p>&gt;&gt;&gt; exit()</p>
<p>Verifying that TensorRT was installed</p>
<p>sudo apt-get update</p>
<p>(donkey) jetson@ucsdrobocar-xxx-yy:\~\$ sudo apt-get install tensorrt</p>
<p>Reading package lists... Done</p>
<p>Building dependency tree</p>
<p>Reading state information... Done</p>
<p>tensorrt is already the newest version (7.1.3.0-1+cuda10.2).</p>
<p>0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.</p>
<p>dpkg -l | grep TensorRT</p>
<blockquote>
<p>arm64 Meta package of TensorRT</p>
<p>ii uff-converter-tf 7.1.3-1+cuda10.2 arm64 UFF converter for TensorRT
package</p>
</blockquote>
<p># Periodically the versions of tensorflow, cuDNN / CUDA give us
conflict. Here is a list of compatibility</p>
<p><a href="https://www.tensorflow.org/install/source#gpu">[https://www.tensorflow.org/install/sourcegpu]{.underline}</a></p>
<p>Installing pycuda - it will take a while again...</p>
<p>pip3 install pycuda</p>
<p>If you are having errors installing pycuda use the following command:</p>
<p>pip3 install pycuda==2020.1</p>
<h3 id="pytorch">PyTorch</h3>
<p>Lets install <a href="https://pytorch.org/">[PyTorch]{.underline}</a> too</p>
<p>"An open source machine learning framework that accelerates the path
from research prototyping to production deployment"</p>
<p>Again, these steps will take some time. Use your time wisely</p>
<p>cd \~/projects</p>
<p>wget
https://nvidia.box.com/shared/static/p57jwntv436lfrd78inwl7iml6p13fzh.whl</p>
<p>cp p57jwntv436lfrd78inwl7iml6p13fzh.whl
torch-1.8.0-cp36-cp36m-linux_aarch64.whl</p>
<p>pip3 install torch-1.8.0-cp36-cp36m-linux_aarch64.whl</p>
<p>sudo apt-get install libjpeg-dev zlib1g-dev libpython3-dev
libavcodec-dev libavformat-dev libswscale-dev</p>
<p>git clone -b v0.9.0 https://github.com/pytorch/vision torchvision</p>
<p>cd torchvision</p>
<p>python setup.py install</p>
<p>cd ../</p>
<p># it will take a good while again. Keep studying other things...</p>
<p>Testing Pythorch install</p>
<p>python</p>
<blockquote>
<p>import torch</p>
<p>print(torch.__version__)</p>
</blockquote>
<p>exit()</p>
<p>ex:</p>
<blockquote>
<p>(donkey) jetson@ucsdrobocar-xxx-yy:\~/projects\$ python</p>
<p>Python 3.6.9 (default, Jan 26 2021, 15:33:00)</p>
<p>[GCC 8.4.0] on linux</p>
<p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more
information.</p>
<p>&gt;&gt;&gt; import torch</p>
<p>&gt;&gt;&gt; print(torch.__version__)</p>
<p>1.8.0</p>
<p>&gt;&gt;&gt; exit()</p>
<p>(donkey) jetson@ucsdrobocar-xxx-yy:\~/projects\$</p>
</blockquote>
<p>One more test</p>
<p>pip3 show torch</p>
<p>ex:</p>
<blockquote>
<p>Name: torch</p>
<p>Version: 1.8.0</p>
<p>Summary: Tensors and Dynamic neural networks in Python with strong GPU
acceleration</p>
<p>Home-page: https://pytorch.org/</p>
<p>Author: PyTorch Team</p>
<p>Author-email: packages@pytorch.org</p>
<p>License: BSD-3</p>
<p>Location:
/home/jetson/projects/envs/donkey/lib/python3.6/site-packages</p>
<p>Requires: dataclasses, typing-extensions, numpy</p>
<p>Required-by: torchvision</p>
</blockquote>
<h3 id="_9"></h3>
<h2 id="as-of-summer-ii-2022-we-are-using-a-new-stereo-camera-from-luxonis">As of Summer II 2022, we are using a new Stereo Camera from Luxonis</h2>
<h2 id="configuring-oakd-lite">Configuring OAKD Lite</h2>
<p>Open a terminal window and run the following commands:</p>
<p>sudo apt update &amp;&amp; sudo apt upgrade</p>
<p># after upgrades</p>
<p>sudo reboot now</p>
<p>If you have not added the extra swap space while building OpenCV, please
add it</p>
<p>You can use jtop to add more swap space using the left and right keys
and clicking the plus button</p>
<p>jtop</p>
<p>4</p>
<p><img alt="" src="../10images/media/image33.png" />{width="4.005208880139983in"
height="1.776890857392826in"}</p>
<p>Add 4G of swap and press \&lt;S&gt; to enable it.</p>
<p><img alt="" src="../10images/media/image25.png" />{width="3.0156255468066493in"
height="2.161738845144357in"}</p>
<blockquote>
<p>Alternatively you can use the command line</p>
<p># Disable ZRAM:</p>
<p>sudo systemctl disable nvzramconfig</p>
<p># Create 4GB swap file</p>
<p>sudo fallocate -l 4G /mnt/4GB.swap</p>
<p>sudo chmod 600 /mnt/4GB.swap</p>
<p>sudo mkswap /mnt/4GB.swap</p>
<p>If you have an issue with the final command, you can try the
following:</p>
<p>sudo nano /etc/fstab</p>
<p># Add this line at the bottom of the file</p>
<p>/mnt/4GB.swap swap swap defaults 0 0</p>
<p># Reboot</p>
<p>sudo reboot now</p>
</blockquote>
<h1 id="installing-dependencies">Installing dependencies</h1>
<p>Navigate to the directory where you will be installing the luxonis
libraries using cd \~/projects</p>
<p>sudo nano install_dependencies.sh</p>
<p>Copy the entire contents of the file:
<a href="https://drive.google.com/file/d/1dA_MAeJbzDbkNlWzFMTlBraCIS2v965S/view?usp=share_link">[install_dependencies.sh]{.underline}</a></p>
<p>bash install_dependencies.sh</p>
<p>echo \"export OPENBLAS_CORETYPE=ARMV8\" &gt;&gt; \~/.bashrc</p>
<p>echo \'SUBSYSTEM==\"usb\", ATTRS{idVendor}==\"03e7\", MODE=\"0666\"\' |
sudo tee /etc/udev/rules.d/80-movidius.rules</p>
<p>sudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger</p>
<h1 id="navigate-using-cd-to-the-folder-where-you-would-like-to-install-the">Navigate using cd to the folder where you would like to install the</h1>
<p>camera example files and requirements</p>
<p>cd \~/projects</p>
<p>git clone
<a href="https://github.com/luxonis/depthai-python.git">[https://github.com/luxonis/depthai-python.git]{.underline}</a></p>
<p>cd depthai-python/examples</p>
<p>python3 install_requirements.py</p>
<h1 id="if-you-want-to-test-the-camera-and-you-have-remote-desktop">If you want to test the camera and you have remote desktop</h1>
<p><a href="https://www.nomachine.com/download">[NoMachine]{.underline}</a> already
installed and the OAKD Lite is connected to JTN , run the following in
the terminal on a NoMachine session</p>
<h1 id="navigate-to-the-examples-folder-in-depthai-python-first-and-then">Navigate to the examples folder in depthai-python first and then</h1>
<p>cd ColorCamera</p>
<p>python3 rgb_preview.py</p>
<p>You should be able to see preview video on the No machine desktop</p>
<h3 id="_10"></h3>
<h3 id="installing-donkeycar-ai-framework">Installing Donkeycar AI Framework</h3>
<p>Lets Install the Donkeycar AI Framework</p>
<p>If you are upgrading from Donkey3 then save the values from your
calibration that</p>
<p>you had on</p>
<p>myconfig.py</p>
<p>Then let\'s remove the old donkeycar and d3 directories</p>
<blockquote>
<p>cd \~/projects</p>
<p>rm -rf donkeycar</p>
<p>rm -rf d3</p>
<p>If the projects directory was not created yet, mkdir projects</p>
<h1 id="cd-projects">cd \~/projects</h1>
<p>Get donkeycar from Github</p>
<p>git clone https://github.com/autorope/donkeycar</p>
<p>cd donkeycar</p>
<p>cd \~/projects</p>
<p>git clone https://github.com/autorope/donkeycar</p>
<p>cd donkeycar</p>
<p>git fetch --all --tags -f</p>
<p>git checkout 4.5.1</p>
<p>pip install -e .[[nano]{.mark}]</p>
</blockquote>
<p>Install more dependencies</p>
<p>sudo apt-get install python3-dev python3-numpy python-dev libsdl-dev
libsdl1.2-dev libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev
libsdl1.2-dev libsmpeg-dev python-numpy subversion libportmidi-dev
ffmpeg libswscale-dev libavformat-dev libavcodec-dev libfreetype6-dev
libswscale-dev libjpeg-dev libfreetype6-dev</p>
<p>pip install pygame</p>
<p>Lets enable the use of synchronization of files with remote computers
using rsync</p>
<p>sudo apt-get install rsync</p>
<p>This part will take a bit of time. Be patient, please keep in mind that
you are using a low power</p>
<p>single board computer (SBC).</p>
<p>If you are curious if your SBC is really working, you can open another
tab in the terminal window</p>
<p>or a complete new terminal window, ssh to the JTN then execute the
command top or htop</p>
<p>look at the CPU utilization...</p>
<blockquote>
<p>Note I had problems installing Donkey with the latest version of pip
(20.0.2). I had to revert</p>
<p>to an earlier version of pip. See versions of pip here
<a href="https://pip.pypa.io/en/stable/news/">[https://pip.pypa.io/en/stable/news/]{.underline}</a></p>
<p>On 28 May20, it worked. Keeping the line below for reference in case
the problem happens again</p>
<p># pip install --upgrade pip==18.1</p>
</blockquote>
<p>Install Donkey with</p>
<h1 id="pip3-install-e-nano">pip3 install -e .[nano]</h1>
<p>Proceed to <a href="#create-a-car">[Create a Car]{.underline}</a></p>
<blockquote>
<p># 1/30/24</p>
<p># <a href="mailto:mlopezme@ucsd.edu">[mlopezme@ucsd.edu]{.underline}</a>
[]{.mark}</p>
<p># Moises, did you change the instructions to have nano45? It was
giving problems in WI24. Does it install Donkey 4.5.1? That is what we
need</p>
<p># For ECE MAE 148 when we are ready for the latest version of Donkey,
let\'s say using a docker container</p>
<p>cd \~/projects</p>
<p>git clone https://github.com/autorope/donkeycar</p>
<p>cd donkeycar</p>
<p>git fetch --all --tags -f</p>
<p>latestTag=\$(git describe --tags `git rev-list --tags
--max-count=1`)</p>
<p>git checkout \$latestTag</p>
<p>pip install -e .[[nano45]{.mark}]</p>
</blockquote>
<p>It may take a while. You may not see progress on the terminal. You can
ssh to the SBC</p>
<p>and run the command top or htop or jtop from another terminal/tab</p>
<p>Grab a coffee, go study something ...</p>
<h3 id="_11"></h3>
<h3 id="create-a-car">Create a Car</h3>
<p>Let's create a car on the path \~/project/d4</p>
<p>cd \~/projects/donkeycar</p>
<p>donkey createcar --path \~/projects/d4</p>
<p>If complains about old version of numpy and the install fails</p>
<p>pip install numpy --upgrade</p>
<p>________ ______ _________</p>
<p>___ __ \_______________ /___________
__ __ ____/_____ ________</p>
<p>__ / / / __ \_ __ \_ //_/ _ \_ / / / _ / _ __ `/_
___/</p>
<p>_ /_/ // /_/ / / / / ,\&lt; / __/ /_/ / / /___ / /_/ /_ /</p>
<p>/_____/ \____//_/ /_//_/|_| \___/_\__, /
\____/ \__,_/ /_/</p>
<p>/____/</p>
<p>using donkey v4.3.22 ...</p>
<p>Creating car folder: /home/jetson/projects/d4</p>
<p>making dir /home/jetson/projects/d4</p>
<p>The version of the Donkey car may be newer than the one above...</p>
<p># For Winter 2024</p>
<p>Make sure the DonkeyCar is version 4.5.1. The latest version of the
DonkeyCar (5.x) does not work at the Jetson Nano yet.</p>
<p>You spent several hours on this configuration right?! Please make a
backup of your uSD card - "<a href="https://docs.google.com/document/d/1TF1uGAFeDARNwbPkgg9xg4NvWPJvrsHW2I_IN9dWQH0/edit#heading=h.mwv009fkh5ax">[Backup of the uSD
Card]{.underline}</a>"</p>
<h1 id="_12"></h1>
<h1 id="if-you-are-using-a-pwm-board-with-a-esc-vs-a-vesc">If you are using a PWM board with a ESC vs. a VESC</h1>
<h3 id="starting-on-fall22-we-use-a-vesc-for-controlling-the-bldc-motor-skip-setting-the-pwm-board-we-left-here-for-people-that-may-want-to-use-it-on-their-own-robot"><strong>Starting on FALL'22, we use a VESC for controlling the BLDC motor. Skip setting the PWM board. We left here for people that may want to use it on their own robot</strong></h3>
<h3 id="again-if-you-using-an-esc-skip-the-pwm-board-setup">Again, if you using an ESC skip the PWM board setup</h3>
<h3 id="modifying-pwm-board-configuration">#Modifying PWM board configuration</h3>
<blockquote>
<h1 id="now-we-need-to-edit-the-myconfigpy-to-change-the-default-bus-number">Now we need to edit the myconfig.py to change the default bus number</h1>
<p>for the PWM board</p>
<h1 id="pca9685">(PCA9685)</h1>
<h1 id="nano-myconfigpy">nano myconfig.py</h1>
<h1 id="jetson-nano-set-pca9685_i2c_busnum-1">Jetson Nano: set PCA9685_I2C_BUSNUM = 1</h1>
<h1 id="remove-the-comment-from-the-line-and-add-1-to-the-busnum">Remove the comment from the line; "" and add "1" to the BUSNUM</h1>
<h1 id="pca9685_i2c_busnum-1-none">PCA9685_I2C_BUSNUM = 1 None ...</h1>
</blockquote>
<h3 id="modifying-camera">Modifying Camera</h3>
<p>Change the camera type to MOCK to enable us to test the system without a
camera</p>
<p>nano myconfig.py</p>
<h1 id="camera">CAMERA</h1>
<p>CAMERA_TYPE = \"MOCK\" (PICAM|WEBCAM|CVCAM|CSIC|V4L|MOCK)</p>
<p># if you have USB camera connected to the JTN , use WEBCAM</p>
<p># And change this line so the Donkey can run using the web interface</p>
<p>USE_JOYSTICK_AS_DEFAULT = False</p>
<p>In summary you change these 3 lines in the myconfig.py to be able to
test your Donkey installation</p>
<p># if using the PWM board the PWM board and ESC vs. VESC</p>
<blockquote>
<p>PCA9685_I2C_BUSNUM = 1</p>
</blockquote>
<p>CAMERA_TYPE = \"MOCK\"</p>
<p>USE_JOYSTICK_AS_DEFAULT = False</p>
<h3 id="_13"></h3>
<h3 id="quick-test">Quick Test</h3>
<p>Lets test the Donkey AI framework install</p>
<p>python manage.py drive</p>
<blockquote>
<p>Adding part PWMSteering.</p>
<p>Adding part PWMThrottle.</p>
<p>Tub does NOT exist. Creating a new tub...</p>
<p>New tub created at: /home/jetson/projects/d3/data/tub_1_19-08-05</p>
<p>Adding part TubWriter.</p>
<p>You can now go to \&lt;your pi ip address&gt;:8887 to drive your car.</p>
<p>Starting vehicle...</p>
<p>8887</p>
</blockquote>
<p>Lets connect to the JTN by using a web browser from your PC</p>
<p><a href="http://ucsdrobocar01.local:8887">[http://ucsdrobocar-xxx-yy:8887]{.underline}</a></p>
<p>You should see a screen like this</p>
<p><img alt="" src="../10images/media/image43.png" />{width="4.6631517935258096in"
height="3.119792213473316in"}</p>
<h1 id="we-stop-the-donkey-with-ctrl-c">We stop the Donkey with Ctrl-C</h1>
<p>Ctrl-C</p>
<h3 id="modifying-joystick">Modifying Joystick</h3>
<p>Now let\'s change the type of joystick we use with Donkey</p>
<p>nano myconfig.py</p>
<blockquote>
<p>JOYSTICK</p>
<p>[USE_JOYSTICK_AS_DEFAULT = True]{.mark} when starting the manage.py,
when True, wil\$</p>
<p>JOYSTICK_MAX_THROTTLE = 0.5 this scalar is multiplied with the -1 to\$</p>
<p>JOYSTICK_STEERING_SCALE = 1.0 some people want a steering that is
less\$</p>
<p>AUTO_RECORD_ON_THROTTLE = True if true, we will record whenever
throttle\$</p>
<p>[CONTROLLER_TYPE=\'F710\']{.mark} (ps3|ps4|xbox|nimbus|wiiu|F710)</p>
</blockquote>
<p>python manage.py drive</p>
<p>ex</p>
<blockquote>
<p>Starting vehicle...</p>
<p>Opening /dev/input/js0...</p>
<p>Device name: Logitech Gamepad F710</p>
<p>recorded 10 records</p>
<p>recorded 20 records</p>
<p>recorded 30 records</p>
<p>recorded 40 records</p>
<p>erased last 100 records.</p>
<p>E-Stop!!!</p>
<p>recorded 10 records</p>
<p>recorded 20 records</p>
<p>recorded 30 records</p>
<p>recorded 40 records</p>
<p>recorded 50 records</p>
</blockquote>
<p>The Right Joystick is the Throttle, the Left Joystick is the Steering</p>
<p>The Y Button deletes 5s of driving at the default configuration =100
records at 20 Hz</p>
<p>The A Button is the emergency break</p>
<p>Joystick Controls:</p>
<blockquote>
<p>+------------------+---------------------------+</p>
<p>| control | action |</p>
<p>+------------------+---------------------------+</p>
<p>| start | toggle_mode |</p>
<p>| B | toggle_manual_recording |</p>
<p>| Y | erase_last_N_records |</p>
<p>| A | emergency_stop |</p>
<p>| back | toggle_constant_throttle |</p>
<p>| R1 | chaos_monkey_on_right |</p>
<p>| L1 | chaos_monkey_on_left |</p>
<p>| circle | show_record_acount_status |</p>
<p>| R2 | enable_ai_launch |</p>
<p>| left_stick_horz | set_steering |</p>
<p>| right_stick_vert | set_throttle |</p>
<p>+------------------+---------------------------+</p>
</blockquote>
<p>If your joystick is not returning to neutral</p>
<p>You can add a deadzone value</p>
<p>on myconfig.py</p>
<p>ex:</p>
<blockquote>
<p>NETWORK_JS_SERVER_IP = \"192.168.0.1\"when listening for network
joystick cont\$</p>
<p>JOYSTICK_DEADZONE = 0.01 when non zero, this is the smallest throt\$</p>
<p>JOYSTICK_THROTTLE_DIR = -1.0 use -1.0 to flip forward/backward, use \$</p>
</blockquote>
<p>Lets Integrate the JTN and PWM Controller into the RC Chassis</p>
<p>Charge your LiPo Battery</p>
<p>After you charge your Lithium Polymer (LiPo) battery(ries) - <a href="https://rogershobbycenter.com/lipoguide/">[some info
here]{.underline}</a></p>
<p>Connect the battery(ries) <strong>and batteries monitor/alarm</strong></p>
<p>Please do not use the batteries without the batteries monitor/alarms</p>
<p><strong>If you discharge a LiPO batteries below a threshold lets say 3.0</strong></p>
<h1 id="_14"></h1>
<h2 id="calibration-of-the-throttle-and-steering">Calibration of the Throttle and Steering</h2>
<p>Before you develop code or you can use someone\'s code to control a
robot, we need to calibrate the actuator/mechanism to find out its range
of motion compared to the control / command a computer will send to the
controller of the actuator/mechanism.</p>
<p>The calibration is car specific. If you use the same platform, the
numbers should be really close. Otherwise, you will need to calibrate
your car.</p>
<p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p>
<p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p>
<p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p>
<p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p>
<p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p>
<p>Follow the safety guidelines provided in person in the class.</p>
<p>If something does not sound right, don't do it. Safety first.</p>
<p>Power the JTN</p>
<p>Power the Electronic Speed Controller (ESC)</p>
<p>Lets run a Python command to calibrate Steering and Throttle</p>
<p>The donkey commands need to be run from the directory you created for
your car, i.e., \~/projects/[d4]{.mark}</p>
<p>If you have not done so, SSH into the JTN</p>
<p>If needed, change directory to d4</p>
<p>cd \~/projects/d4</p>
<h2 id="_15"></h2>
<h3 id="begin-calibration">Begin Calibration</h3>
<p>donkey calibrate [--channel 1]{.mark} --bus 1</p>
<p><strong>Please make sure that when you are trying the steering values you do
not keep the steering</strong></p>
<p><strong>servo max out</strong>.</p>
<p>Please reduce go back 10 or 15 values when you notice the servo motor is
not moving or making a</p>
<p>constant noise. If you leave the servo motor max-out you will most
likely burn it.</p>
<p><strong>If by any chance your software locks-up, please turn off the ESC
immediately.</strong></p>
<p>Then once you run the calibrate again, issue the center value before
turning in the ESC again</p>
<p>Note that after 10 values or so the calibration may time out. Just run
it again.</p>
<p>Try some values around 390 to center the steering</p>
<blockquote>
<p>Enter a PWM setting to test(100-600)370</p>
<p>Enter a PWM setting to test(100-600)380</p>
<p>Enter a PWM setting to test(100-600)390</p>
<p>Enter a PWM setting to test(100-600)400</p>
</blockquote>
<p>[In my case]{.mark}, 390 seems to center the steering</p>
<p>Take note of the left max and right max value. We will be able to adjust
these after we try driving the</p>
<p>car so it goes straight.</p>
<p>ex:</p>
<p>390 - Center</p>
<p>[290]{.mark} - Steering left max</p>
<p>[490]{.mark} - Steering right max</p>
<p>Note: If the donkey calibration times-out, just run it again.</p>
<p>You can end the calibration by typing CTRL-C</p>
<p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p>
<p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p>
<p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p>
<p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p>
<p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p>
<p>Now to calibrate the Throttle Electronic Speed Controller ESC (THROTTLE)</p>
<p>Following the standard for R/C cars. Throttle goes on channel [2]{.mark}</p>
<p>donkey calibrate [--channel 2 --bus 1]{.mark}</p>
<blockquote>
<p>Enter a PWM setting to test(100-600)370</p>
<p>Enter a PWM setting to test(100-600)[380]{.mark} (neutral)</p>
<p>Enter a PWM setting to test(100-600)390</p>
</blockquote>
<p>On my case, 380 when I power up the ESC seems to be the middle point
(neutral),</p>
<p>I will use 380. Your case may vaires. 370 seems a common value</p>
<p>Neutral when power the ESC - [380]{.mark}</p>
<p>[Make sure the car is balanced over the car stand]{.mark}</p>
<p>Then go in increments of 10 until you can no longer hear an increase on
the speed</p>
<p>of the car. Don't worry much about the max speed since we won't drive
that fast autonomously</p>
<p>and during training the speed will be limited.</p>
<p>Max speed forward - [490]{.mark}</p>
<p>Reverse on RC cars is a little tricky because the ESC needs to receive</p>
<p>a reverse pulse, zero pulse, and again reverse pulse to start to go
backwards.</p>
<p>Use the same technique as above set the PWM setting to your zero
throttle</p>
<p>(lets say 380 or 370).</p>
<blockquote>
<p>[Enter the reverse value]{.mark}, then the [zero throttle (e.g.,
370)]{.mark} value, then a [reverse value again]{.mark}.</p>
<p>Enter values +/- 10 of the reverse value to find a reasonable reverse
speed. Remember this reverse PWM value.</p>
<p>(dk)<strong>pi@jackrpi02</strong>:<strong>\~/projects/d3 \$</strong></p>
<p>donkey calibrate [--channel 2 --bus 1]{.mark}</p>
<p>Enter a PWM setting to test(100-600)360</p>
<p>Enter a PWM setting to test(100-600)[370]{.mark}</p>
<p>Enter a PWM setting to test(100-600)360</p>
<p>Enter a PWM setting to test(100-600)350</p>
<p>Enter a PWM setting to test(100-600)340</p>
<p>Enter a PWM setting to test(100-600)330</p>
<p>Enter a PWM setting to test(100-600)320</p>
</blockquote>
<p>I got for Throttle</p>
<p>[490]{.mark} - Max speed forward</p>
<p>[380]{.mark} - Neutral</p>
<p>[300]{.mark} - Max speed backwards</p>
<p>---</p>
<p>For my robocar I have:</p>
<p>Steering</p>
<p>[290]{.mark} - Steering left max</p>
<p>[490]{.mark} - Steering right max</p>
<p>Throttle</p>
<p>[490]{.mark} - Max speed forward</p>
<p>[380]{.mark} - Neutral</p>
<p>[300]{.mark} - Max speed backwards</p>
<h3 id="saving-car-configuration-file">Saving car configuration file</h3>
<p>Now let\'s write these values into the car configuration file</p>
<p>Edit the file my config.py</p>
<p>nano myconfig.py</p>
<p>Change these values according to YOUR [calibration values]{.mark} and
[where you have the Steering Servo and ESC]{.mark} connected</p>
<p>...</p>
<p>STEERING</p>
<p>STEERING_CHANNEL = 1 channel on the 9685 pwm board 0-15</p>
<p>STEERING_LEFT_PWM = 290 pwm value for full left steering</p>
<p>STEERING_RIGHT_PWM = 490 pwm value for full right steering</p>
<p>THROTTLE</p>
<p>THROTTLE_CHANNEL = 2 channel on the 9685 pwm board 0-15</p>
<p>THROTTLE_FORWARD_PWM = 490 pwm value for max forward throttle</p>
<p>THROTTLE_STOPPED_PWM = 380 pwm value for no movement</p>
<p>THROTTLE_REVERSE_PWM = 300 pwm value for max reverse throttle</p>
<p>Also, change these</p>
<p>CAMERA</p>
<p>CAMERA_TYPE = \"WEBCAM\" (PICAM|WEBCAM|CVCAM|CSIC|V4L|MOCK)</p>
<p>9865, overrides only if needed, ie. TX2..</p>
<p>PCA9685_I2C_ADDR = 0x40 I2C address, use i2cdetect to validate this
numb\$</p>
<p>PCA9685_I2C_BUSNUM = 1 None will auto detect, which is fine on the pi.
\$</p>
<p>JOYSTICK</p>
<p>USE_JOYSTICK_AS_DEFAULT = True when starting the manage.py, when True,
wil\$</p>
<p>and if needed to zero the joystick</p>
<p>JOYSTICK_DEADZONE = 0.01 when non zero, this is the smallest thro\$</p>
<p>Note: When driving the robot, if you robot has too much power or not
enough power</p>
<p>you can adjust the max_throttle</p>
<p>JOYSTICK_MAX_THROTTLE = 0.5</p>
<p>This also will be your starting power setting when using the constant
throttle autopilot.</p>
<p>You can test driving your robot by issuing</p>
<p>python manage.py drive</p>
<h1 id="if-you-are-using-the-vesc-and-oakd-camera-on-the-physical-car">If you are using the VESC and OAKD camera on the physical car</h1>
<h2 id="vesc_1">VESC</h2>
<p>Ensure that you have already configured the VESC device using the VESC
Tool Software</p>
<p>Edit the myconfig.py to have these values</p>
<p>DRIVE_TRAIN_TYPE = \"VESC\"</p>
<p>VESC_MAX_SPEED_PERCENT =.2 ## Max speed as a percent of actual max
speed</p>
<p>VESC_SERIAL_PORT= \"/dev/ttyACM0\" ## check this val with ls /dev/tty*</p>
<p>VESC_HAS_SENSOR= True</p>
<p>VESC_START_HEARTBEAT= True</p>
<p>VESC_BAUDRATE= 115200</p>
<p>VESC_TIMEOUT= 0.05</p>
<p>VESC_STEERING_SCALE = .5</p>
<p>VESC_STEERING_OFFSET = .5</p>
<p>DONKEY_GYM = False</p>
<p>(we will leave the CAMERA_TYPE = "MOCK" for now to make sure we can
drive the car with the VESC)</p>
<p>Download the following files</p>
<p><a href="https://drive.google.com/drive/folders/1SBzChXK2ebzPHgZBP_AIhVXJOekVc0r3?usp=sharing">~~[https://drive.google.com/drive/folders/1SBzChXK2ebzPHgZBP_AIhVXJOekVc0r3?usp=sharing]{.underline}~~</a></p>
<p><a href="https://drive.google.com/drive/folders/19TS3VyNXQPBSr41yiPaxpI1lnxClI2c8?usp=sharing">[https://drive.google.com/drive/folders/19TS3VyNXQPBSr41yiPaxpI1lnxClI2c8?usp=sharing]{.underline}</a></p>
<p>And replace them on the Jetson in the locations shown in the images
below. Note - to get the files on the jetson you can use SFTP (secure
file transfer protocol):</p>
<p>Examples on how to use SFTP:\
sftp jetson@ucsdrobocar-148-xx.local</p>
<p>cd To the directory you want to go to</p>
<p>put /File/Path/On/Your/Computer</p>
<p>alternatively</p>
<p>get filename /File/Location/You/Want/Them/Go/On/Your/Computer</p>
<p>To get a directory</p>
<p>get -rf filename /File/Location/You/Want/Them/Go/On/Your/Computer</p>
<p>type \"exit\" to disconnect</p>
<h2 id="width4351609798775153in-height3276042213473316inwidth4307292213473316in-height3131517935258093in"><img alt="" src="../10images/media/image18.png" />{width="4.351609798775153in" height="3.276042213473316in"}<img alt="" src="../10images/media/image24.png" />{width="4.307292213473316in" height="3.131517935258093in"}</h2>
<p>Once these have been replaced, you should run</p>
<p>python manage.py drive</p>
<p>It should first throw a pyvesc import error. Follow the description in
the terminal to install the needed libraries</p>
<p>Then run it again. It should throw a permissions error. Follow the
advice on how to fix the error with chmod</p>
<p>Int(10) error bug fix (Credit Saimai Lau and Albert Chang):\
When running python manage.py drive, the intermittent \"invalid literal
for int() with base 10: \'None\' error is from the VESC package checking
whether the version of the VESC is below 3.0, so we can comment out that
part since we\'re using 6.0 just do</p>
<p>nano
/home/jetson/projects/envs/donkey/lib/python3.6/site-packages/pyvesc/VESC/VESC.py</p>
<p>and put # at the beginning of lines 38-40 Then \^S to save and \^X to
exit</p>
<p><img alt="" src="../10images/media/image35.png" />{width="7.5in"
height="2.0833333333333335in"}</p>
<p>Error explanation: The self.get_firmware_version() get thes version by
requesting it from the VESC and reading the replied bytes, but sometimes
the data is lost or incomplete so the function returns \'None\' as the
version. We already know the version is 6.0 so we don't need this
function.</p>
<h2 id="oakd">OAKD</h2>
<p>Once you have the VESC driving the car, you will need to make sure you
have set up the document by following the instructions labeled
Configuring OAKD Lite found <a href="#configuring-oakd-lite">[earlier in this
document]{.underline}</a></p>
<p>Edit the myconfig.py camera type to OAKD</p>
<p>CAMERA_TYPE = \"OAKD\"</p>
<p>Make sure the camera is working by checking the images that are being
saved to the /data/images directory.</p>
<p>The easiest way to do this is to go to</p>
<p><a href="http://localhost:8887">[http://localhost:8887]{.underline}</a> while
running donkeysim, and you should be able to see a livestream from the
camera. Note - if several people are running donkeysim at the same time
on the same wifi this interface may get buggy</p>
<p>You can also do this by either:</p>
<p>transferring the files to you laptop or virtual machine</p>
<p>with scp, rsync, winscp (windows) or filezilla (mac)</p>
<p>Or</p>
<p>Using NoMachine by following the instructions found
<a href="#remote-desktop-installation">[here]{.underline}</a> in this document</p>
<h2 id="gnss-configuration">GNSS Configuration:</h2>
<h2 id="how-to-plug-pointonenav-to-donkeycar">How to Plug PointOneNav to Donkeycar</h2>
<ol>
<li>
<p>Make sure your user is added to the dialout group. If not</p>
<p>a.  sudo adduser jetson dialout</p>
<p>b.  sudo reboot now</p>
</li>
<li>
<p>Download
    <a href="https://drive.google.com/file/d/1BK_UjH-He9d_D4eObWMHzpHHCqmtq75h/view?usp=share_link">[https://drive.google.com/file/d/1BK_UjH-He9d_D4eObWMHzpHHCqmtq75h/view?usp=share_link]{.underline}</a>
    (Note that this zip file cannot be shared outside of the class. It
    is still proprietary as of now)</p>
</li>
<li>
<p>Unzip.</p>
</li>
<li>
<p>Run</p>
<p>a.  cd quectel-lg69t-am.0.15.0/p1_runner</p>
<p>b.  deactivate (This should get you out of the current environment)</p>
<p>c.  wget
    https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-pypy3-Linux-aarch64.sh
    .</p>
<p>d.  bash Mambaforge-pypy3-Linux-aarch64.sh</p>
<p>e.  Reboot the jetson</p>
<p>f.  mamba create --name py37 -c conda-forge python=3.7 pip</p>
<p>g.  mamba activate py37</p>
<p>h.  [pip3 install -e .]{.mark}</p>
<pre><code>i.  %If this fails, you can try just going to the p1_runner
    directory and running the python3 bin/config_tool.py
    command, and then doing "pip install \_\_\_\_" for all the
    missing things, you may just need

    1.  pip install pyserial

    2.  pip install fusion_engine_client

    3.  pip install pynmea

    4.  pip install ntripstreams

    5.  pip install websockets
</code></pre>
<p>i.  python3 bin/config_tool.py reset factory</p>
<p>j.  python3 bin/config_tool.py apply uart2_message_rate nmea gga on</p>
<p>k.  python3 bin/config_tool.py save</p>
<p>l.  python3 bin/runner.py --device-id \&lt;polaris_username&gt;
    --polaris \&lt;polaris_password&gt; --device-port /dev/ttyUSB1</p>
</li>
</ol>
<blockquote>
<p>(if not getting any data including Nans try USB0)</p>
</blockquote>
<p>Note: The GPS corrections will only happen when you are actively running
runner.py. I recommend making a bashrc command that you can run to start
up the runner.py program easily in a 2nd terminal while using the GPS
for anything.</p>
<ol>
<li>
<p>Create a project with the DonkeyCar path follow template</p>
<p>a.  Open a new terminal window</p>
<p>b.  Make sure that the donkey car environment is running</p>
<pre><code>i.  source \~/projects/envs/donkey/bin/activate
</code></pre>
<p>c.  cd \~/projects</p>
<p>d.  donkey createcar --path ./mycar --template path_follow</p>
</li>
<li>
<p>Set the following in the myconfig.py</p>
<p>a.  GPS_SERIAL = "/dev/ttyUSB2" (USB1 if USB0 used above)</p>
<p>b.  GPS_SERIAL_BAUDRATE = 460800</p>
<p>c.  GPS_DEBUG = True</p>
<p>d.  HAVE_GPS = True</p>
<p>e.  GPS_NMEA_PATH = None</p>
</li>
<li>
<p>Also set things like the VESC parameters in myconfig.py. You can
    copy these over from the donkeycar you created earlier.</p>
</li>
<li>
<p>Run</p>
<p>a.  python3 manage.py drive</p>
</li>
<li>
<p>You should see GPS positions being outputted after you run
    Donkeycar. If you don't want to output set GPS_DEBUG to False</p>
</li>
<li>
<p>Configure button actions</p>
<p>a.  SAVE_PATH_BTN is the button to save the in-memory path to a
    file.</p>
<p>b.  LOAD_PATH_BTN is the button to (re)load path from the csv file
    into memory.</p>
<p>c.  RESET_ORIGIN_BTN is the button to set the current position as
    the origin.</p>
<p>d.  ERASE_PATH_BTN is the button to erase path from memory and reset
    the origin.</p>
<p>e.  TOGGLE_RECORDING_BTN is the button to toggle recording mode on
    or off. Note that there is a pre-assigned button in the web ui,
    so there is not need to assign this button to one of the web/w*
    buttons if you are using the web ui.</p>
<p>f.  INC_PID_D_BTN is the button to change PID \'D\' constant by
    PID_D_DELTA.</p>
<p>g.  DEC_PID_D_BTN is the button to change PID \'D\' constant by
    -PID_D_DELTA</p>
<p>h.  INC_PID_P_BTN is the button to change PID \'P\' constant by
    PID_P_DELTA</p>
<p>i.  DEC_PID_P_BTN is the button to change PID \'P\' constant by
    -PID_P_DELTA</p>
</li>
</ol>
<blockquote>
<p>The logitech buttons are named stuff like "X" or "R1" See the example
config below.\
SAVE_PATH_BTN = \"R1\" # button to save path</p>
<p>LOAD_PATH_BTN = \"X\" # button (re)load path</p>
<p>RESET_ORIGIN_BTN = \"B\" # button to press to move car back to origin</p>
<p>ERASE_PATH_BTN = \"Y\" # button to erase path</p>
<p>TOGGLE_RECORDING_BTN = \"L1\" # button to toggle recording mode</p>
<p>INC_PID_D_BTN = None # button to change PID \'D\' constant by
PID_D_DELTA</p>
<p>DEC_PID_D_BTN = None # button to change PID \'D\' constant by
-PID_D_DELTA</p>
<p>INC_PID_P_BTN = \"None\" # button to change PID \'P\' constant by
PID_P_DELTA</p>
<p>DEC_PID_P_BTN = \"None\" # button to change PID \'P\' constant by
-PID_P_DELTA</p>
<p>#</p>
</blockquote>
<ol>
<li>
<p>Recording a path</p>
<p>a.  The algorithm assumes we will be driving in a continuous
    connected path such that the start and end are the same. You can
    adjust the space between recorded waypoints by editing the
    PATH_MIN_DIST value in myconfig.py You can change the name and
    location of the saved file by editing the PATH_FILENAME value.</p>
<p>b.  Enter User driving mode using either the web controller or a
    game controller.</p>
<p>c.  Move the car to the desired starting point</p>
<p>d.  Erase the path in memory (which will also reset the origin).</p>
<pre><code>i.  Make sure to reset the origin!!! If you didn't need to erase
    the path in memory you can just
</code></pre>
<p>e.  Toggle recording on.</p>
<p>f.  Drive the car manually around the track until you reach the
    desired starting point again.</p>
<p>g.  Toggle recording off.</p>
<p>h.  If desired, save the path.</p>
</li>
<li>
<p>Following a path</p>
<p>a.  Enter User driving mode using either the web controller or a
    game controller.</p>
<p>b.  Move the car to the desired starting point - make sure it's the
    same one from when you recorded the path</p>
<p>c.  Reset the origin (be careful; don\'t erase the path, just reset
    the origin).</p>
<p>d.  Load the path</p>
<p>e.  Enter Autosteering or Autopilot driving mode. This is normally
    done by pressing the start button either once or twice If you
    are in Autosteering mode you will need to manually provide
    throttle for the car to move. If you are in Autopilot mode the
    car should drive itself completely.</p>
</li>
<li>
<p>Configuring Path Follow Parameters</p>
<p>a.  So the algorithm uses the cross-track error between a desired
    line and the vehicle\'s measured position to decide how much and
    which way to steer. But the path we recorded is not a simple
    line; it is a lot of points that is typically some kind of
    circuit. As described above, we use the vehicle\'s current
    position to choose a short segment of the path that we use as
    our desired track. That short segment is recalculated every time
    we get a new measured car position. There are a few
    configuration parameters that determine exactly which two points
    on the path that we use to calculate the desired track line.</p>
<pre><code>i.  PATH_SEARCH_LENGTH = None \# number of points to search for
    closest point, None to search entire path

ii. PATH_LOOK_AHEAD = 1 \# number of points ahead of the closest
    point to include in cte track

iii. PATH_LOOK_BEHIND = 1 \# number of points behind the closest
     point to include in cte track
</code></pre>
<p>b.  Generally, if you are driving very fast you might want the look
    ahead to be larger than if driving slowly so that your steering
    can anticipate upcoming curves. Increasing the length of the
    resulting track line, by increasing the look behind and/or look
    ahead, also acts as a noise filter; it smooths out the track.
    This reduces the amount of jitter in the controller. However,
    this must be balanced with the true curves in the path; longer
    track segments effectively \'flatten\' curves and so can result
    in understeer; not steering enough when on a curve.</p>
</li>
<li>
<p>Determining PID Coefficients</p>
<p>a.  The PID coefficients are the most important (and time consuming)
    parameters to configure. If they are not correct for your car
    then it will not follow the path. The coefficients can be
    changed by editing their values in the myconfig.py file.</p>
<p>b.  PID_P is the proportional coefficient; it is multiplied with the
    cross-track error. This is the most important parameter; it
    contributes the most to the output steering value and in some
    cases may be all that is needed to follow the line. If this is
    too small then car will not turn enough when it reaches a curve.
    If this to too large then it will over-react to small changes in
    the path and may start turning in circles; especially when it
    gets to a curve.</p>
<p>c.  PID_D is the differential coefficient; it is multiplied with the
    change in the cross-track error. This parameter can be useful in
    reducing oscillations and overshoot.</p>
<p>d.  PID_I is the integral coefficient; it is multiplied with the
    total accumulated cross-track error. This may be useful in
    reducing offsets caused by accumulated error; such as if one
    wheel is slightly smaller in diameter than another.</p>
<p>e.  Determining PID Coefficients can be difficult. One approach is:</p>
<pre><code>i.  First determine the P coefficient.

ii. zero out the D and the I coefficients.

iii. Use a kind of \'binary\' search to find a value where the
     vehicle will roughly follow a recorded straight line;
     probably oscillating around it. It will be weaving

iv. Next find a D coefficient that reduces the weaving
    (oscillations) on a straight line. Then record a path with a
    tight turn. Find a D coefficient that reduces the overshoot
    when turning.

v.  You may not even need the I value. If the car becomes
    unstable after driving for a while then you may want to
    start to set this value. It will likely be much smaller than
    the other values.

vi. Be patient. Start with a reasonably slow speed. Change one
    thing at a time and test the change; don\'t make many
    changes at once. Write down what is working.

vii. Once you have a stable PID controller, then you can figure
     out just how fast you can go with it before autopilot
     becomes unstable. If you want to go faster then set the
     desired speed and start tweaking the values again using the
     method suggested above.
</code></pre>
</li>
</ol>
<h1 id="_16"></h1>
<h1 id="_17"></h1>
<h1 id="_18"></h1>
<h1 id="_19"></h1>
<h1 id="_20"></h1>
<h1 id="_21"></h1>
<h1 id="_22"></h1>
<h1 id="_23"></h1>
<h1 id="driving-the-robot-to-collect-data">Driving the Robot to Collect data</h1>
<p>Remember you are driving at max of <strong>x</strong> (0.<strong>x</strong>) Throttle power based
on the</p>
<p>myconfig.py that you edited.</p>
<p>The robot is not controlling speed but power given the motor using PWM
values</p>
<p>Transmitted from the Single Board Computer (SBC) like a Jetson Nano
(JTN) to the</p>
<p>Electronic Speed Controller (ESC).</p>
<p>To reverse you may have to reverse, stop, reverse. This is a feature of
some</p>
<p>ESCs used in RC cars to prevent damaging gears when changing from
forward to reverse.</p>
<p><strong>On the JTN</strong></p>
<p>If you are not changing directory automatically when the user logs in</p>
<p>cd \~/projects/d3</p>
<p>(env) jetson@ucsdrobocar00:\~/projects/d3 \$</p>
<p>python manage.py drive</p>
<p>Note: CTRL-C stop the manage.py drive</p>
<p>---</p>
<p>If you get an error on the joystick or Donkey stops loading the JoyStick</p>
<p>it is because game controller is off</p>
<p>or not connected/paired with the JTN</p>
<p>ls</p>
<p>config.py data logs manage.py models</p>
<blockquote>
<p>ls data</p>
<p>tub_1_17-10-13</p>
<p>If you want to wipe clean the data collected</p>
<p>Remove the content of the \~/projects/d3/data directory. It should be
tub_......</p>
<p>You can delete the entire directory then create it again.</p>
<p>\~/projects/d3 \$</p>
<p>rm -rf data</p>
<p>mkdir data</p>
</blockquote>
<p>Follow the Donkey Docs to install the Donkey AI framework into your PC
<a href="http://docs.donkeycar.com">[http://docs.donkeycar.com]{.underline}</a></p>
<p><strong>On the PC</strong></p>
<p>Activate the virtual environment. I am assuming you have your virtual
environment under</p>
<p>\~/projects/envs/donkey</p>
<p>source \~/projects/envs/donkey/bin/activate</p>
<p>cd projects</p>
<p>cd d3</p>
<p><strong>SSH into the JTN</strong></p>
<p>ssh jetson@ucsdrobocar00.local</p>
<p><strong>On the JTN</strong></p>
<p>If you are not changing directory automatically when the user logs in</p>
<p>cd \~/projects/d3</p>
<p>(env) jetson@ucsdrobocar00:\~/projects/d3 \$</p>
<p>python manage.py drive</p>
<p>Note: CTRL-C stop the manage.py drive</p>
<p>Drive the robot to collect data</p>
<p><strong>On the PC</strong></p>
<p>Get data from JTN</p>
<p>Transfer all data from the JTN to the PC and delete data that was
deleted from the JTN</p>
<p>rsync -a --progress --delete jetson@ucsdrobocar00.local:\~/d3/data
\~/projects/d3</p>
<p>ls data</p>
<blockquote>
<p>tub_1_17-10-12</p>
</blockquote>
<p>Train model on all data (Tubes)</p>
<p>python train.py --model=models/date_name.h5</p>
<p>To train using a particular tube</p>
<p>python train.py --tub \~/projects/d3/data/tub_1_18-01-07
--model=models/model_name.h5</p>
<p>To make an incremental training using a previous model</p>
<p>python train.py --tub \~/projects/d3/data/NAME_OF_NEW_TUBE
--transfer=models/NAME_OF_PREVIOUS_MODEL.h5
--model=models/NAME_OF_NEW_MODEL.h5</p>
<p><strong>On your personal PC (Not required, only if you installed on your
personal computer)</strong></p>
<p>clean-up tubs removing possible bad data</p>
<p>\~/projects/d3</p>
<p>donkey tubclean data</p>
<blockquote>
<p>using donkey ...</p>
<p>Listening on 8886...</p>
</blockquote>
<p>Open a browser and type</p>
<p><a href="http://localhost:8886">[http://localhost:8886]{.underline}</a></p>
<p><img alt="" src="../10images/media/image15.png" />{width="4.807292213473316in"
height="2.717165354330709in"}</p>
<p>You can clean-up your tub directories. Please make a backup of your data
before you start</p>
<p>to clean it up.</p>
<p>On the mac if the training complains</p>
<blockquote>
<p>rm \~/projects/d2t/data/.DS_Store</p>
</blockquote>
<p>If it complains about docopt, install it again. And I did not change
anything</p>
<p>from the previous day. Go figure...</p>
<blockquote>
<p>(env) jack@lnxmbp01:\~/projects/d2\$ pip list</p>
<p>(env) jack@lnxmbp01:\~/projects/d2\$ pip install docopt</p>
</blockquote>
<p>See the models here</p>
<p>\~/projects/d3</p>
<p>ls models</p>
<blockquote>
<p>[ucsd_12oct17.h5]{.mark}</p>
</blockquote>
<p>Place Autopilot into RPI</p>
<p>rsync -a --progress \~/projects/d3/models/
jetson@ucsdrobocar00:\~/projects/d3/models/</p>
<p>At JTN</p>
<blockquote>
<p>ls models</p>
<p>Ucsd_12oct17.h5</p>
</blockquote>
<p>On the JTN</p>
<p>Run AutoPilot at the RPI</p>
<p>python manage.py drive --model=./models/ucsd_12oct17.h5</p>
<blockquote>
<p>...</p>
<p>Using TensorFlow backend.</p>
<p>loading config file: /home/pi/d2/config.py</p>
<p>config loaded</p>
<p>PiCamera loaded.. .warming camera</p>
<p>Starting vehicle...</p>
<p>/home/pi/env/lib/python3.4/site-packages/picamera/encoders.py:544:
PiCameraResolutionRounded: frame size rounded up from 160x120 to
160x128</p>
<p>width, height, fwidth, fheight)))</p>
</blockquote>
<p><strong>END of DonkeyCar AI Framework Instructions</strong></p>
<h1 id="remote-desktop-installation">Remote Desktop Installation</h1>
<p>Remote Access to the SBC Graphical User Interface (GUI)</p>
<p>Lets install a remote desktop server on the SBC and a client on your
computer</p>
<p>We will be using <a href="https://www.nomachine.com/">[NoMachine]{.underline}</a></p>
<p><a href="https://docs.google.com/document/d/1WR4my5hZGzdHrmYuORuqSjyniT_PZXgufSlsZdT_atI/edit?usp=sharing">[Here is a link to the
instructions]{.underline}</a>
to install NOMACHINE on a Single Board Computers base</p>
<h1 id="backup-of-the-usd-card">Backup of the uSD Card</h1>
<p>Once you finished the configuration of your SBC, why not making another
backup of the uSD card</p>
<p>(or first one if you have not done it yet)</p>
<p>It can save you lots of time during a recovery process. In case of a
crash, you will only need to</p>
<p>restore the image vs. install all over again.</p>
<p>Backup the uSD card following these steps in a Linux machine</p>
<p>Eject the uSD card from your SBC, plug it into a linux PC using a uSD
adapter</p>
<p>sudo fdisk -l</p>
<blockquote>
<p>Disk /dev/sda: 59.6 GiB, 64021856256 bytes, 125042688 sectors</p>
<p>Units: sectors of 1 * 512 = 512 bytes</p>
<p>Sector size (logical/physical): 512 bytes / 512 bytes</p>
<p>I/O size (minimum/optimal): 512 bytes / 512 bytes</p>
<p>Disklabel type: gpt</p>
<p>Disk identifier: D048AD43-24FD-4DED-B06E-7BB8ED98158C</p>
<p>Device Start End Sectors Size Type</p>
<p>/dev/sda1 24576 125042654 125018079 59.6G Linux filesystem</p>
<p>/dev/sda2 2048 2303 256 128K Linux filesystem</p>
<p>/dev/sda3 4096 4991 896 448K Linux filesystem</p>
<p>/dev/sda4 6144 7295 1152 576K Linux filesystem</p>
<p>/dev/sda5 8192 8319 128 64K Linux filesystem</p>
<p>/dev/sda6 10240 10623 384 192K Linux filesystem</p>
<p>/dev/sda7 12288 13439 1152 576K Linux filesystem</p>
<p>/dev/sda8 14336 14463 128 64K Linux filesystem</p>
<p>/dev/sda9 16384 17663 1280 640K Linux filesystem</p>
<p>/dev/sda10 18432 19327 896 448K Linux filesystem</p>
<p>/dev/sda11 20480 20735 256 128K Linux filesystem</p>
<p>/dev/sda12 22528 22687 160 80K Linux filesystem</p>
</blockquote>
<p>On my case, the 64G uSD was mounted on /dev/sda</p>
<p>example on the command line for making an image of the uSD card mounted</p>
<p>on a Linux machines as /dev/sda</p>
<p>sudo dd bs=4M if=/dev/sda of=ucsd_robocar_image_25sep19.img
status=progress</p>
<p>Lets compress the image using Zip</p>
<p>zip ucsd_robocar_image_25sep19.zip ucsd_robocar_image_25sep19.img</p>
<p>Example using MacOS</p>
<p>diskutil list</p>
<blockquote>
<p>/dev/disk6 (external, physical):</p>
<p>: TYPE NAME SIZE IDENTIFIER</p>
<p>0: GUID_partition_scheme *128.0 GB disk6</p>
<p>1: Linux Filesystem 127.6 GB disk6s1</p>
<p>2: Linux Filesystem 67.1 MB disk6s2</p>
<p>3: Linux Filesystem 67.1 MB disk6s3</p>
<p>4: Linux Filesystem 458.8 KB disk6s4</p>
<p>5: Linux Filesystem 458.8 KB disk6s5</p>
<p>6: Linux Filesystem 66.1 MB disk6s6</p>
<p>7: Linux Filesystem 524.3 KB disk6s7</p>
<p>8: Linux Filesystem 262.1 KB disk6s8</p>
<p>9: Linux Filesystem 262.1 KB disk6s9</p>
<p>10: Linux Filesystem 104.9 MB disk6s10</p>
<p>11: Linux Filesystem 134.2 MB disk6s11</p>
</blockquote>
<p>sudo dd if=/dev/disk6 of=ucsdrobocar-xxx-yy-v1.0.img</p>
<p>Alternatively you can use MS Windows, use a software called Win32.
<a href="https://thepihut.com/blogs/raspberry-pi-tutorials/17789160-backing-up-and-restoring-your-raspberry-pis-sd-card#:~:text=Using%20Windows&amp;text=Once%20you%20open%20Win32%20Disk,backed%20up%20to%20your%20PC.">[Search the
web]{.underline}</a></p>
<p><a href="https://thepihut.com/blogs/raspberry-pi-tutorials/17789160-backing-up-and-restoring-your-raspberry-pis-sd-card#:~:text=Using%20Windows&amp;text=Once%20you%20open%20Win32%20Disk,backed%20up%20to%20your%20PC.">[for instructions on using
Win32]{.underline}</a></p>
<blockquote>
<p><strong>"</strong></p>
<p><strong>Using</strong> Windows</p>
<p>Once you open <strong>Win32</strong> Disk Imager, <strong>use</strong> the blue folder icon to
choose the location and the name of the <strong>backup</strong> you want to take,
and then choose the drive letter for your <strong>SD card</strong>. Click on the
Read button. The <strong>card</strong> will then be backed up to your PC.Mar 18,
2015</p>
<p><a href="https://thepihut.com/blogs/raspberry-pi-tutorials/17789160-backing-up-and-restoring-your-raspberry-pis-sd-card#:~:text=Using%20Windows&amp;text=Once%20you%20open%20Win32%20Disk,backed%20up%20to%20your%20PC.">[Backing up and Restoring your Raspberry Pi\'s SD Card-- The
...]{.underline}</a></p>
</blockquote>
<p>"</p>
<h2 id="if-needed-we-have-an-jtn-usd-card-image-ready-for-plug-and-play">If needed, we have an JTN uSD Card Image Ready for Plug and Play</h2>
<p>If you run out of time trying to make the compilation and build OpenCV,
and install the Donkey AI</p>
<p>framework, here is a link for a 64G Bytes uSDcard that is ready to go.
<a href="https://drive.google.com/file/d/1J8_rFLjNtM5CBymwTAi_3IYsQ7VI9Rd6/view?usp=sharing">[Get recovery]{.underline} [image
here]{.underline}</a>.
[]{.mark}</p>
<p>The recovery image already has all the software installed.</p>
<p>Connect a uUSB cable between the PC and the JTN, or connect the JTN to
the access point</p>
<p>using a network cable.</p>
<p>Boot the JTN, wait 1\~2 minutes for the software to finish loading</p>
<p>You can ssh to the JTN</p>
<p>ssh jetson@192.168.55.1</p>
<p>or</p>
<p>ssh jetson@ucsdrobocar-xxx-yy.local</p>
<p>enter password</p>
<p>jetsonucsd</p>
<p>You need to change the host name and change the default password</p>
<p>Connect the JTN to a WiFi network</p>
<p>see steps earlier in this document</p>
<p>ex:</p>
<p>sudo nmcli device wifi connect UCSDRoboCar5GHz password UCSDrobocars2018</p>
<p>shutdown the JTN</p>
<p>sudo shutdown now</p>
<p>If you are using the uUSB cable, remove it from the JTN</p>
<p>Power on the JTN with the provided 5V power supply</p>
<p>Wait 1\~2 minutes for the JTN to complete the boot process</p>
<p>Then use SSH to connect to the JTN</p>
<h1 id="_24"></h1>
<h1 id="ros-with-docker">ROS with Docker</h1>
<p><a href="https://docs.google.com/document/u/0/d/1YS5YGbo8evIo9Mlb0J-w2r3bZfju37Zl4UmdaN2CD2A/edit">[Here is the link for getting your robot set up with ROS using our
docker
images]{.underline}</a></p>
<h1 id="supporting-material">Supporting material</h1>
<p>How to show what Linux we have installed</p>
<p>dmesg | head -1</p>
<p>How to show the distribution we are running</p>
<p>lsb_release -a</p>
<p>This session is in-process, just a placeholder for now</p>
<p>Jetson Nano ROS based on NVIDIA</p>
<p><a href="https://github.com/dusty-nv/jetbot_ros">[https://github.com/dusty-nv/jetbot_ros]{.underline}</a></p>
<p>Here is what I changed on myconfig.py</p>
<p>CAMERA_TYPE = \"WEBCAM\" (PICAM|WEBCAM|CVCAM|CSIC|V4L|MOCK)</p>
<p>PCA9685_I2C_BUSNUM = 1 None will auto detect, which is fine on the pi.
But other platforms should specify the bus number.</p>
<p>STEERING_CHANNEL = 1 channel on the 9685 pwm board 0-15</p>
<p>STEERING_LEFT_PWM = 290 pwm value for full left steering</p>
<p>STEERING_RIGHT_PWM = 490 pwm value for full right steering</p>
<p>THROTTLE_CHANNEL = 2 channel on the 9685 pwm board 0-15</p>
<p>THROTTLE_FORWARD_PWM = 490 pwm value for max forward throttle</p>
<p>THROTTLE_STOPPED_PWM = 380 pwm value for no movement</p>
<p>THROTTLE_REVERSE_PWM = 300 pwm value for max reverse throttle</p>
<p>USE_JOYSTICK_AS_DEFAULT = True when starting the manage.py, when True,
will not require a --js option to use the joystick</p>
<p>CONTROLLER_TYPE=\'F710\' (ps3|ps4|xbox|nimbus|wiiu|F710)</p>
<p>JOYSTICK_DEADZONE = 0.01 when non zero, this is the smallest throttle
before recording triggered.</p>
<p>--</p>
<p>--</p>
<p>To use TensorflowRT on the JetsonNano</p>
<p>http://docs.donkeycar.com/guide/robot_sbc/tensorrt_jetson_nano/</p>
<p>Updating the Donkey AI framework and or using the master release fork</p>
<blockquote>
<p>"</p>
<p>Tawn 2:47 PM</p>
<p>3.1.0 released now. TensorRT support on Nano! TFlite support (w TF
2.0+). Better support for cropping across more tools. Please update
like:</p>
<p>cd projects/donkeycar</p>
<p>git checkout master</p>
<p>git pull</p>
<p>pip install -e .[pi] or .[nano] or .[pc] depending where you are
installing it</p>
<p>cd \~/mycar</p>
<p>donkey update</p>
<p>Important Note: Your old models will not work with the new code.
We\'ve changed how cropping and normalization work to support
TensorRT/TFLite. So please, RE-TRAIN your models after updating to\
\
docs to help get you started w TensorRT:
<a href="https://slack-redir.net/link?url=https%3A%2F%2Fdocs.donkeycar.com%2Fguide%2Frobot_sbc%2Ftensorrt_jetson_nano%2F">https://docs.donkeycar.com/guide/robot_sbc/tensorrt_jetson_nano/</a></p>
</blockquote>
<p>Installing TensorRT on Ubuntu18.04</p>
<p>After installing Tensorflow on your virtual environment</p>
<p>ex: conda install tensorflow-gpu==1.13.1</p>
<p>Testing the Tensorflow Install</p>
<p>python</p>
<p>Enter the following txt, you can cut and paste</p>
<p>Python</p>
<p>import tensorflow as tf</p>
<p>hello = tf.constant(\'Hello, TensorFlow!\')</p>
<p>sess = tf.Session()</p>
<p>print(sess.run(hello))</p>
<p><a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.html#installing-tar">[https://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.htmlinstalling-tar]{.underline}</a></p>
<p><a href="https://developer.nvidia.com/nvidia-tensorrt-5x-download">[https://developer.nvidia.com/nvidia-tensorrt-5x-download]{.underline}</a></p>
<p><img alt="" src="../10images/media/image31.png" />{width="1.9798939195100613in"
height="1.828125546806649in"}</p>
<p><img alt="" src="../10images/media/image14.png" />{width="3.5781255468066493in"
height="0.36722878390201225in"}</p>
<p>Open a terminal window and navigate to the directory where you saved the
file</p>
<p>Now let's expand the file</p>
<p><img alt="" src="../10images/media/image16.png" />{width="6.875in"
height="0.4444444444444444in"}</p>
<p>Note that the file above expects to work with cudnn7.5</p>
<p>The Cuda install will need to match that version, cudnn7.5</p>
<p>e.g. \~/projects/TensorRT-5.1.5.0/</p>
<p>cd \~/projects/TensorRT-5.1.5.0</p>
<p>tar xzvf
TensorRT-5.1.5.0.Ubuntu-18.04.2.x86_64-gnu.cuda-10.1.[cudnn7.5]{.mark}.tar.gz</p>
<p>export
LD_LIBRARY_PATH=\$LD_LIBRARY_PATH:\~/projects/TensorRT-5.1.5.0/lib</p>
<p>Activate the virtual environment you want TensorflowRT installed</p>
<p>e.g. source \~/projects/env/bin/activate</p>
<p>cd python</p>
<p>If using python 3.6</p>
<p>pip install tensorrt-5.1.5.0-cp36-none-linux_x86_64.whl</p>
<p>or if using python3.7</p>
<p>pip install tensorrt-5.1.5.0-cp37-none-linux_x86_64.whl</p>
<p>cd ..</p>
<p>cd uff</p>
<p>pip install uff-0.6.3-py2.py3-none-any.whl</p>
<p>which convert-to-uff</p>
<blockquote>
<p>/home/jack/projects/env/bin/convert-to-uff</p>
</blockquote>
<p>or</p>
<p>which convert-to-uff</p>
<blockquote>
<p>/home/jack/miniconda3/envs/donkey/bin/convert-to-uff</p>
</blockquote>
<p>cd ..</p>
<p>cd graphsurgeon</p>
<p>pip install graphsurgeon-0.4.1-py2.py3-none-any.whl</p>
<p>cd..</p>
<p>apt-get install tree</p>
<p>tree -d</p>
<p>dpkg -l | grep nvinfer</p>
<p>Testing TensorRT</p>
<p>python</p>
<p>import tensorrt as trt</p>
<p>exit()</p>
<p>Example after training a model</p>
<p>You end up with a Linear.h5 in the models folder</p>
<p>python manage.py train --model=./models/Linear.h5
--tub=./data/tub_1_19-06-29,...</p>
<p>Freeze model using freeze_model.py in donkeycar/scripts</p>
<p>The frozen model is stored as protocol buffers.</p>
<p>This command also exports some metadata about the model which is saved
in ./models/Linear.metadata</p>
<p>python freeze_model.py --model=./models/Linear.h5
--output=./models/Linear.pb</p>
<p>Convert the frozen model to UFF. The command below creates a file
./models/Linear.uff</p>
<p>convert-to-uff ./models/Linear.pb</p>
<blockquote>
<p>cd \~/projects/d3_trax_rally</p>
<p>python \~/projects/donkeycar/scripts/freeze_model.py
--model=./models/24aug19_ucsd_booker_linear_1_320_240.h5
--output=./models/24aug19_ucsd_booker_linear_1_320_240.pb</p>
<p>convert-to-uff ./models/24aug19_ucsd_booker_linear_1_320_240.pb</p>
</blockquote>
<p>For the RPI</p>
<p>If you would like to try tflite support, you will need a newer version
of Tensorflow on the pi. You can download and install this version:</p>
<p>wget
<a href="https://slack-redir.net/link?url=https%3A%2F%2Ftawn-train.s3.amazonaws.com%2Ftf%2Ftensorflow-2.0.0a0-cp35-cp35m-linux_armv7l.whl">https://tawn-train.s3.amazonaws.com/tf/tensorflow-2.0.0a0-cp35-cp35m-linux_armv7l.whl</a></p>
<p>pip install tensorflow-2.0.0a0-cp35-cp35m-linux_armv7l.whl</p>
<p>Your host PC can stay at TF 1.13.1</p>
<p>to train a TensorRT model, during training add the arg</p>
<p>--type=tensorrt_linear</p>
<p>And for TFlite support add the arg</p>
<p>--type=tflite_linear</p>
<p>Also use this flag when running your model on the car. (edited)</p>
<p>"</p>
<p>PS3 Controller Modes</p>
<p>This is similar for the Logitech wireless controllers</p>
<p>The default mode will be that [User]{.mark} is in Control. That is, the
user controls Steering and Throttle.</p>
<p>To switch to <strong>[Local Angle]{.mark}</strong> (software controls the Steering
and uses the Throttle), you need to press the <strong>\&lt;Select&gt;</strong> button in
the Joystick.</p>
<p>If you give Throttle the Robocar should drive around semi-autonomously.</p>
<p>After few laps that you see that your model is good,</p>
<p>[Please hold your robot with the wheels out of the floor]{.mark}</p>
<p>you can press the <strong>\&lt;Start&gt;</strong> button [and immediately press <strong>the
\&lt;left_DOWN_arrow&gt;</strong> button a few times to decrease the Throttle as
needed.]{.mark} This is important so you slow down the Robocar for a
constant Throttle.</p>
<p>Press the \&lt;left_UP_arrow&gt; to give it more Throttle as needed.</p>
<p>Pressing \&lt;[X&gt;]{.mark} will stop the robocar and go back to User mode
(user is in control)</p>
<p>You can change the driving modes by pressing the <strong>\&lt;Select&gt;</strong> button.
You should be able to see a message on your computer terminal that is
SSH connected to the RoboCar RPI.</p>
<p>The Local &amp; Angle mode (fully autonomous) is to be used after you see
that you can do few laps with local angle</p>
<blockquote>
<p>Hit the Select button to toggle between three modes - User, Local
Angle, and Local Throttle &amp; Angle.</p>
</blockquote>
<ul>
<li>
<p>User - User controls both steering and throttle with joystick</p>
</li>
<li>
<p>Local Angle - Ai controls steering. User controls the throttle.</p>
</li>
<li>
<p>Local Throttle &amp; Angle - Ai controls both steering and throttle</p>
</li>
</ul>
<blockquote>
<p>When the car is in Local Angle mode, the NN will steer. You must
provide throttle...</p>
</blockquote>
<p>Ideally you will have \~ 60 laps</p>
<p>If you don't have a good working Auto-Pilot, get more data in 10 laps
increments.</p>
<p>In summary, you may want to start with 60 laps and then do 10\~20 laps
more to see if the model gets better.</p>
<p>I would not worry much about a few bad spots when collecting data. Drive
the car back to the track,</p>
<p>then press Green_Triangle to delete the last 5s of data.</p>
<p>Keep driving, you will develop good skills, you will get good data and
better models. If you leave the track, just drive the RoboCar back to
track. It may even learn how to get back to track.</p>
<p>If you keep the data from the same track (ex: UCSD Track) in the
d2t/data directory, as you add more files to it (e.g.,
tub_[5]{.mark}_17-10-13) it will help your model. At the same time it
will take more time to train since your model will read all the data
sets in the directory. You can use the transfer model to add new data to
a current model.</p>
<p>Incremental training using a previous model</p>
<blockquote>
<p>python train.py --tub \~/projects/d2t/data/NAME_OF_NEW_TUBE_DATA
--transfer=models/NAME_OF_PREVIOUS_MODEL.h5
--model=models/NAME_OF_NEW_MODEL.h5</p>
</blockquote>
<p>Some Advanced Tools</p>
<p>The visualization tool is to be used on your PC. Please even if you can,
please do not use the GPU Cluster Resources for this.</p>
<p>https://docs.google.com/presentation/d/1oOF9qHh6qPwF-ocOwGRzmLIRXcIcguDFwa7x9EicCo8/editslide=id.g629a9e24fa_0_1149</p>
<p><img alt="" src="../10images/media/image44.png" />{width="6.875in"
height="4.083333333333333in"}</p>
<p>Visualizing the model driving the car vs. human driver</p>
<p>Install OpenCV</p>
<p>sudo apt-get install python-opencv</p>
<p>pip3 install opencv-python</p>
<p>donkey makemovie --tub=data\tub_file --model=models\model_name.h5
--limit=100 --salient --scale=2</p>
<p>example</p>
<p>donkey makemovie --tub=data/tub_9_19-01-19
--model=models/19jan19_oakland_5.h5 --start 1 --end 1000 --salient
--scale=2</p>
<p>tub_3_20-03-07</p>
<p>07mar20_circuit_320x180_3.h5</p>
<p>donkey makemovie --tub=data/tub_3_20-03-07
--model=models/07mar20_circuit_320x180_3.h5 --type=linear --start 1
--end 1000 --salient --scale=2</p>
<p><a href="https://devtalk.nvidia.com/default/topic/1051265/jetson-nano/remote-access-to-jetson-nano/">[https://devtalk.nvidia.com/default/topic/1051265/jetson-nano/remote-access-to-jetson-nano/]{.underline}</a></p>
<blockquote>
<p>If you\'re on the same wired local network, ssh -X works, but you
should be aware that the graphics are being rendered on your local X
server in such a case, so if you launch an OpenGL game for example,
it\'ll use your local virtual graphics hardware, which means your
local *CPU* in most cases. Cuda, however, will be done on the nano.</p>
<p>To launch a program remotely from a linux computer:</p>
<p>ssh -X some_user@test-jetson -C gedit</p>
<p>ssh -X ubuntu@tegra-ubuntu nautilus</p>
<p>Where `gedit` is the program you wish to run. You can omit -C get
straight to a ssh prompt with X support. Any graphical applications
you launch will pop up on your screen automatically.</p>
<p>Please note that X does not need to be running on the Nano, so if you
want to save a whole bunch of memory while working remotely you can
run `sudo systemctl isolate multi-user.target` to temporarily shut
down the graphical environment on the Nano itself.</p>
<p>To remotely access from windows, here are instructions on how to set
up an X server on windows and connect it to Putty, but please note
those instructions have an old download link. A new one is here.</p>
</blockquote>
<p>Raspberry PI (RPI or PI) Configuration</p>
<p>04Nov19 - Added a link to a plug and play image</p>
<p>03Nov19 - Updated instruction for Tensorflow 2.0</p>
<p>28Aug19 - updated the instructions to include Raspberry PI 4B</p>
<p>08Aug19 - Since we are using the Nvidia Jetson Nano, I am no longer
maintaining the RPI</p>
<p>instructions. If you got to this point and is using Raspberry PIs,
please check the</p>
<p>Donkey Docs for the latest updates...
<a href="http://docs.donkeycar.com">[http://docs.donkeycar.com]{.underline}</a></p>
<p>In general if you are using a Linux distribution like Ubuntu in this
course it will make</p>
<p>your life is much easier. Initially, you will need access to a Linux to
modify some files from</p>
<p>a uSD card. You can ask a colleague, TA, or course instructor for help.</p>
<p>Moreover, if you use Linux it will be another entry into your resume.
Give it a try. You can</p>
<p>make a dual boot in your computer or have a virtual machine,
<a href="https://www.virtualbox.org/">[VirtualBox is free]{.underline}</a>.</p>
<p><em>-------------</em></p>
<p>We will start with an Operating System Image called Raspbian (Raspberry
Debian ...)</p>
<p>You can use your favorite disk image writer to have the disk image
written to the uSD.</p>
<p>The uSD card to be used on the RPI. Note, this is not a regular file
copy operation.</p>
<p>You can use Etcher
<a href="https://etcher.io/">[https://etcher.io/]{.underline}</a></p>
<p>From your PC let\'s prepare the Raspberry PI (RPI) uSD card</p>
<p>Make sure your computer can access the Internet</p>
<p>If you are using one of our WiFi Access Points in one of the labs or at
one of the tracks, the first PC</p>
<p>that connects to the WiFi Access point will need to accept the UCSD
Wireless Visitor Agreement,</p>
<p>just like when connecting directly to UCSD's Visitor WiFi.</p>
<p>Get the Raspbian Lite uSD image
<a href="https://www.raspberrypi.org/downloads/raspbian/">[here.]{.underline}</a></p>
<p>Etcher can use Zipped files, you don't need to Unzip the image file. If
you are using Linux command</p>
<p>lines to write the disk image to a uSD card you may need to extract the
file first.</p>
<p>This OS (disk) image is based on the Raspbian headless (no GUI). We will
use command line on</p>
<p>the RPI, to get to the RPI we will be using SSH (secure shell). Don't
worry, these are just command</p>
<p>line names. Mastering these will be good skills to have.</p>
<p>If you don't know about SSH and the command line in Linux, you will
learn enough in this course.</p>
<p>Let\'s write the disk image into the uSD Card</p>
<p>Connect the provided uSD adapter into your PC</p>
<p>Insert the provided uSD card (64 Gbytes) into the uSD adapter</p>
<p>Install and run Etcher
<a href="https://etcher.io/">[https://etcher.io/]{.underline}</a></p>
<p>Start Etcher, chose the Zipped file with the Disk Image you downloaded,</p>
<p>pay attention when choosing the drive with the uSD card on it (e.g., 64
Gbytes)</p>
<p>write the image to uSD card.</p>
<p>If you are using Linux, after you finish writing the disk image to the
uSD card</p>
<p>you should see two partitions in your computer file system</p>
<p>[boot and rootfs]{.mark}</p>
<p>[If you are not seeing these partitions in your computer, try removing
the uSD card from]{.mark}</p>
<p>[your computer then insert it again. If that does not work, try using
a]{.mark}</p>
<p>[computer running Linux.]{.mark}</p>
<p>Prepare the network configuration file.</p>
<p>We will create a text file with the WiFi configurations.</p>
<p>You can use the nano or gedit on Linux (e.g.,Ubuntu).</p>
<p>On other OSes make sure the file you are creating is a plain text file.</p>
<p>Don't use MS Word or other Apps that may save the file with different
file formats</p>
<p>and hidden text format characters.</p>
<p>Note: If you are using a MAC with the SD to uSD adapter, MAC OS may not
be able to</p>
<p>write the Donkey Image disk "boot". Use the USB uSD card adapter
provider for each</p>
<p>Team.</p>
<p>At the root partition of the uSD card, will name the file as
wpa_supplicant.conf</p>
<p>Using Linux or a Mac terminal, the command line you will use is</p>
<p>sudo nano wpa_supplicant.conf</p>
<p>Navigate to the drive that you created the RPI image.</p>
<p>Look for a boot partition</p>
<p>Edit and save the file with this name wpa_supplicant.conf</p>
<p>Open a terminal ex:</p>
<p>cd /media/UserID/boot</p>
<p>ex:</p>
<p>cd /media/jack/boot</p>
<p>sudo nano wpa_supplicant.conf</p>
<p>or</p>
<p>sudo nano /media/UserID/boot/wpa_supplicant.conf</p>
<p>You will need to enter your password</p>
<p>ex: nano /media/jack/boot/wpa_supplicant.conf</p>
<p>on a mac</p>
<p>cd /Volumes/boot</p>
<p>nano wpa_supplicant.conf</p>
<p>Here is the content of wpa_supplicant.conf</p>
<p>You can copy and paste it</p>
<p>ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev</p>
<p>update_config=1</p>
<p>country=US</p>
<p>network={</p>
<p>ssid=\"SD-DIYRoboCar5GHz\"</p>
<p>key_mgmt=WPA-PSK</p>
<p>psk=\"SDrobocars2017\"</p>
<p>priority=100</p>
<p>id_str=\"SD-DIYRoboCars5GHz\"</p>
<p>}</p>
<p>network={</p>
<p>ssid=\"UCSDRoboCar5GHz\"</p>
<p>key_mgmt=WPA-PSK</p>
<p>psk=\"UCSDrobocars2018\"</p>
<p>priority=90</p>
<p>id_str=\"ucsdrobocar5G\"</p>
<p>}</p>
<p>network={</p>
<p>ssid=\"SD-DIYRoboCar\"</p>
<p>key_mgmt=WPA-PSK</p>
<p>psk=\"SDrobocars2017\"</p>
<p>priority=80</p>
<p>id_str=\"SD-DIYRoboCars2.4GHz\"</p>
<p>}</p>
<p>network={</p>
<p>ssid=\"UCSDRoboCar\"</p>
<p>key_mgmt=WPA-PSK</p>
<p>psk=\"UCSDrobocars2018\"</p>
<p>priority=70</p>
<p>id_str=\"ucsdrobocar2.4GHz\"</p>
<p>}</p>
<p>[If you are using a PC running Linux to edit the file in a later
time,]{.mark}</p>
<p>[place the uSD card into the PC]{.mark}</p>
<p>[sudo nano
/media/user_id/rootfs/etc/wpa_supplicant/wpa_supplicant.conf]{.mark}</p>
<p>On first boot, this file will be moved to the path below where it may be
edited later</p>
<p>as needed.</p>
<p>To edit current WiFi connections or to add WiFi Connections</p>
<p>You will need to ssh to to RPI and edit the following file</p>
<p>/etc/wpa_supplicant/wpa_supplicant.conf</p>
<p>From a terminal where you SSH to the RPI</p>
<p>sudo nano /etc/wpa_supplicant/wpa_supplicant.conf</p>
<p><strong>[Please change the hostname and the password of your RPI\
so other teams don\'t connect to your RPI by mistake.]{.mark}</strong></p>
<p>[Change the Hostname (HostID)]{.mark}</p>
<p>[Using Linux you will be able to edit two files that are in the
following directory]{.mark}</p>
<p>[When using MacOS you may not see ext3/ext4 partitions that Linux
(Raspbian) uses]{.mark}</p>
<p>[hostname]{.mark}</p>
<p>[and]{.mark}</p>
<p>[hosts]{.mark}</p>
<p>[Replace raspberrypi with ucsdrobocar-xxx-yy]{.mark}</p>
<p>[The hostnames should follow ucsdrobocar-xxx-yy]{.mark}</p>
<p>[xx= Team number]{.mark}</p>
<p>[For team 07, replace raspberrypi by ucsdrobocar07]{.mark}</p>
<p>[For COMOS]{.mark}</p>
<p>[For team 07, replace raspberrypi by ucsdcosmos11t07]{.mark}</p>
<p>[sudo nano /media/UserID/rootfs/etc/hostname\
sudo nano /media/UserID/rootfs/etc/hosts\
]{.mark}</p>
<p>[ex:]{.mark}</p>
<p>[sudo nano /media/jack/rootfs/etc/hostname]{.mark}</p>
<p>[sudo nano /media/jack/rootfs/etc/hosts\
]{.mark}</p>
<p>[When editing from the RPI]{.mark}</p>
<p>[sudo nano /etc/hostname]{.mark}</p>
<p>[sudo nano /etc/hosts]{.mark}</p>
<p>[If you have not changed the RPI host name, you need to make sure that
only your RPI is]{.mark}</p>
<p>[connected to the WiFi router until you can change its hostname and
password.]{.mark}</p>
<p>[Your computer and the RPI need to be on the same network so you can
connect to it.]{.mark}</p>
<p>[Said so, in our case we have 5GHz WiFi too, you should connect your
computer to]{.mark}</p>
<p>[the 5GHz Access Point. There is a bridge between]{.mark}</p>
<p>[the 5GHz and 2.4GHz networks in the Access Points we use.]{.mark}</p>
<p>[Therefore you computer will be able to connect to the]{.mark}</p>
<p>[RPI.]{.mark}</p>
<p>[Enable SSH on boot\
Create a text file and name it ssh in the boot partition]{.mark}</p>
<p>[On Linux]{.mark}</p>
<p>[sudo nano /media/UserID/boot/ssh]{.mark}</p>
<p>[ex: sudo nano /media/jack/boot/ssh]{.mark}</p>
<p>[On MacOS]{.mark}</p>
<p>[/Volumes/boot]{.mark}</p>
<p>[You can add some characters into the file to force nano to write
it.]{.mark}</p>
<p>[Type any short text such as test]{.mark}</p>
<p>[ex: 123]{.mark}</p>
<p>[Close any terminal that may be using the uSD card]{.mark}</p>
<p>[Use the eject equivalent function in your computer to eject the boot
and roofs uSD]{.mark}</p>
<p>[partitions]{.mark}</p>
<p>[Remove the uSD card from your computer, carefully insert it into your
RPI]{.mark}</p>
<p>[Power-up your RPI. You can use a uUSB power adapter or plug it into a
PC USB port.]{.mark}</p>
<p>[Connecting to your RPI using SSH]{.mark}</p>
<p>[SSH is installed by default on Linux, MacOS, and Windows 10.]{.mark}</p>
<p>[On MS Windows 7 or older you will need to get software/app that
supports SSH.]{.mark}</p>
<p>[During the first boot it may take a few minutes for the RPI to be
ready.]{.mark}</p>
<p>[Watch for the RPI green LED flashing, it indicates uSD access.]{.mark}</p>
<p>[During the first boot Raspbian extend the file system. It takes more
time than the next\
following OS boot.]{.mark}</p>
<p>[To connect to your RPI you will need to run a computer terminal and
issue a ssh command]{.mark}</p>
<p>ssh pi@rpiname.local</p>
<p>[ex: ssh pi@ucsdrobocar07.local]{.mark}</p>
<p>[password: raspberry]{.mark}</p>
<p>[If you have changed the hostname of your RPI or you can not see it in
the network,]{.mark}</p>
<p>[maybe because you have incorrect WiFi info, coordinate with other
teams]{.mark}</p>
<p>[that you are the only one doing the initial configuration while
connected to the WiFi or direct]{.mark}</p>
<p>[to the router using a network cable.]{.mark}</p>
<p>[Otherwise you will be connecting to another Team's RPI.]{.mark}</p>
<p>[Connect a network cable to your RPI and the WiFi access point. Boot
it...]{.mark}</p>
<p>[Alternatively, remove the uSD card and put it back in your PC. Read on
the net about]{.mark}</p>
<p>[Raspbian / Linux (Debian) on changing hostname at]{.mark}</p>
<p>[the OS level at the uSD card on your PC before placing it in
RPI.]{.mark}</p>
<p>[Hint]{.mark}</p>
<p>[sudo nano]{.mark}</p>
<p>[/etc/hostname\
/etc/hosts\
]{.mark}</p>
<p><strong>[Please change the hostname and the password so other]{.mark}</strong></p>
<p><strong>[teams don\'t work on your RPI.]{.mark}</strong></p>
<p>[Again]{.mark}</p>
<p>[If you are having difficulties connecting to you RPI using WiFi.
Connect the RPI to the]{.mark}</p>
<p>[router using a network cable (e.g., CAT5).]{.mark}</p>
<p>[We should have at least one network cable in lab.]{.mark}</p>
<p>SSH into the RPI</p>
<p>ssh pi@ucsdrobocar-xxx-yy.local</p>
<p>Lets update the repository content and upgrade the raspbian</p>
<p>(RPI Linux OS based on Debian)</p>
<p><strong>If you did not configure the RPI WiFI correctly it will not connect
to</strong></p>
<p><strong>your WiFi Access Point (AP)</strong></p>
<p><strong>If needed</strong></p>
<p>edit the wpa_supplicant.conf file</p>
<p>sudo nano /etc/wpa_supplicant/wpa_supplicant.conf</p>
<p>SSH into the RPI</p>
<p>ssh pi@ucsdrobocar-xxx-yy.local</p>
<p>Check that the date and time is correct at the RPI and instal ntp to
auto-update date and time</p>
<p>date</p>
<p>Hint. In the case the clock on the RPI is not updating automatically by
getting information</p>
<p>from the Internet, you need to set the date and time manually or updates
and install may fail</p>
<p>Here is how you can do it</p>
<p>ex: sudo date -s \'2018-10-04 09:30:00\'</p>
<p>Once you set the country/location, and have an active Internet access,</p>
<p>these should force a clock synchronization on boot/reboot</p>
<p>Initially it may be off because you have not set the time zone yet. e.g.
Pacific Time</p>
<p>sudo apt-get update</p>
<p>sudo apt-get install ntp</p>
<p>sudo /etc/init.d/ntp stop</p>
<p>sudo ntpd -q -g\
sudo /etc/init.d/ntp start</p>
<p>Assuming you RPI is connected to the Internet, you can always use</p>
<p>sudo ntpd -q -g</p>
<p>to get the time from a server in the Internet</p>
<p>Reboot to check if the date / times are updating correctly</p>
<p>sudo reboot now</p>
<p>SSH into the RPI</p>
<p>Check that the date and time are correct</p>
<p>date</p>
<p>Be patient, this may take a while depending on how many updates were
released</p>
<p>after the uSD card image that you are using</p>
<p>sudo apt-get update</p>
<p>sudo apt-get upgrade</p>
<p>[then reboot the RPI]{.mark}</p>
<p>[sudo reboot now]{.mark}</p>
<p>[Connect to the RPI\
As applicable, once you connect to your RPI run the following
command]{.mark}</p>
<p>[sudo raspi-config]{.mark}</p>
<p>[to change the host name and other things.]{.mark}</p>
<p>[On the RPI use this command]{.mark}</p>
<p>[ex: For team 07, use hostname ucsdrobocar07]{.mark}</p>
<p>[Change the password]{.mark}</p>
<p>[sudo raspi-config]{.mark}</p>
<p><img alt="" src="../10images/media/image27.png" />{width="6.875in"
height="4.097222222222222in"}</p>
<p>User Password, Hostname (if needed), <strong>enable camera, enable I2C</strong>,</p>
<p>locale-time zone to America/ US Pacific / Los Angeles</p>
<p>WiFi country US, then expand the file system just in case</p>
<p>it was not automatically done at boot.</p>
<p>If changed the host name, your RPI will have the new hostname on</p>
<p>the next boot</p>
<p>sudo reboot now</p>
<p>After you configure your RPI WiFi and Hostname correctly and reboot,</p>
<p>Your RPI should be accessible using SSH from Linux, Mac, Windows</p>
<p>Open a terminal</p>
<p>ssh pi@ucsdrobocar-xxx-yy.local</p>
<p>enter your password</p>
<p>try this just in case you can not connect to the RPI on another WiFi
Access point</p>
<p>such as your Phone acting as a hotspot</p>
<p>ssh pi@ucsdrobocar-xxx-yy (without .local)</p>
<p>If you have not done so, change the default password</p>
<p>on the RPI</p>
<p>passwd</p>
<p>To disable downloading translations, to save time,</p>
<p>create a file named 99translations</p>
<p>sudo nano /etc/apt/apt.conf.d/99translations</p>
<p>Place the following line in the 99translations</p>
<p>Acquire::Languages \"none\";</p>
<p>reboot the RPI</p>
<p>sudo reboot now</p>
<p>Lets install some dependencies, libraries, and utilities</p>
<p>sudo apt-get update</p>
<p>This step may take some time, be patient</p>
<p>sudo apt-get install build-essential python3 python3-dev python3-pip
python3-virtualenv python3-numpy python3-picamera python3-pandas
python3-rpi.gpio i2c-tools avahi-utils joystick libopenjp2-7-dev
libtiff5-dev gfortran libatlas-base-dev libopenblas-dev
libhdf5-serial-dev git ntp</p>
<p>And install these to have the dependencies for OpenCV</p>
<p>sudo apt-get install libilmbase-dev libopenexr-dev libgstreamer1.0-dev
libjasper-dev libwebp-dev libatlas-base-dev libavcodec-dev
libavformat-dev libswscale-dev libqtgui4 libqt4-test</p>
<p>Installing OpenCV</p>
<p>sudo apt install python3-opencv</p>
<p>reboot the RPI</p>
<p>sudo reboot now</p>
<blockquote>
<p>In case you want to use the (PCA9685) PWM controller from outside a
virtual environment</p>
<p>sudo apt-get install python3-pip</p>
<p>pip3 install Adafruit_PCA9685</p>
<p>and this if the Donkey framework complains about the RPI camera</p>
<p>pip install \"picamera[array]\"</p>
</blockquote>
<p>Lets set a virtual environment and name it donkey</p>
<p>We will create virtual environments to enable using different software
library configurations without\
having to install them at the root and user level (higher level)</p>
<p>If you have not done so, lets create a directory to store our projects
and one subdirectory</p>
<p>to store virtual environments</p>
<p>cd \~</p>
<p>mkdir projects</p>
<p>cd projects</p>
<p>mkdir envs</p>
<p>cd envs</p>
<p>pip3 install virtualenv</p>
<p>python3 -m virtualenv -p python3 \~/projects/envs/[donkey]{.mark}
--system-site-packages</p>
<p>this line will activate the virtual environment called donkey every time
the</p>
<p>user pi logs in a terminal</p>
<p>echo \"source \~/projects/envs/donkey/bin/activate\" &gt;&gt; \~/.bashrc</p>
<p>source \~/.bashrc</p>
<p>When a virtual environment is active, you should see
(name_of_virtual_enviroment)</p>
<p>in front of the terminal prompt</p>
<p>ex:</p>
<p>(donkey) pi@ucsdrobocar-xxx-yy:\~\$</p>
<p>Testing to see if OpenCV is installed in the virtual env</p>
<p>python</p>
<p>import cv2</p>
<p>cv2.__version__</p>
<p>exit ()</p>
<blockquote>
<p>[GCC 8.2.0] on linux</p>
<p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more
information.</p>
<p>&gt;&gt;&gt; import cv2</p>
<p>&gt;&gt;&gt; cv2.__version__</p>
<p>\'3.2.0\'</p>
<p>&gt;&gt;&gt; exit ()</p>
</blockquote>
<p>Lets install the Donkeycar AI framework</p>
<p>if not done, create a directory called projects</p>
<p>mkdir projects</p>
<p>cd projects</p>
<p>Get the latest donkeycar from Github.</p>
<p>git clone https://github.com/autorope/donkeycar</p>
<p>cd donkeycar</p>
<p>git checkout master</p>
<p>This step may take some time, be patient</p>
<p>pip install -e .[pi]</p>
<p>Lets install Tensorflow</p>
<p>single board computer (SBC) ...</p>
<p>cd \~/projects</p>
<p>Type the command below all in one line</p>
<p>wget
https://github.com/PINTO0309/Tensorflow-bin/raw/master/tensorflow-2.0.0-cp37-cp37m-linux_armv7l.whl</p>
<p>Lets install tensorflow. This step may take some time to install. You
are using a low power</p>
<p>pip install --upgrade tensorflow-2.0.0-cp37-cp37m-linux_armv7l.whl</p>
<p>Quick test of the Tensorflow install</p>
<p>python -c \"import tensorflow\"</p>
<p>If no errors, you should be good.</p>
<blockquote>
<p>If you want to know what is running in your RPI while it is busy,</p>
<p>open another terminal, or tab in the same terminal, SSH into the RPI</p>
<p>then run the command top</p>
<p>top</p>
<p>ex: you can see that the CPU(s) is/are busy</p>
<p>wait for the install to finish...</p>
<p><img alt="" src="../10images/media/image36.png" />{width="6.875in"
height="1.8611111111111112in"}</p>
</blockquote>
<p>Try the Tensorflow install again few times if it fails</p>
<p>If needed, upgrade pip</p>
<p>pip install --upgrade pip</p>
<p>Let's create a car on the path \~/projects/d3</p>
<p>donkey createcar --path \~/projects/d3</p>
<p>Your RPI directory should look like this</p>
<p>list the content of a directory with ls</p>
<p>d3 donkeycar envs tensorflow-2.0.0-cp37-cp37m-linux_armv7l.whl</p>
<p>cd \~/projects/d3</p>
<p>ls</p>
<p>The output should be something similar to</p>
<p>config.py data logs manage.py models myconfig.py train.py</p>
<p>Research what you have to do so when you log into your RPI you are
working from</p>
<p>the car directory like \~/projects/d3 vs. having to issue the command cd
/projects/d3</p>
<p>all the time after you log into the RPI.</p>
<p>What happens when the RPI boots in relation to the file .bashrc (
\~/.bashrc) ?</p>
<p>To enable the use of a WebCam (USB cameras)</p>
<p>sudo apt-get install python3-dev python3-numpy libsdl-dev
libsdl-image1.2-dev \</p>
<p>libsdl-mixer1.2-dev libsdl-ttf2.0-dev libsmpeg-dev libportmidi-dev \</p>
<p>libavformat-dev libswscale-dev libjpeg-dev libfreetype6-dev</p>
<p>pip install pygame</p>
<p><a href="https://drive.google.com/file/d/1P3ssbGEQ8H6_FHivowrTTnEfmLmv-KXJ/view?usp=sharing">[Here is a
link]{.underline}</a>
to a plug an play image with necessary software installed</p>
<p>If you are at UCSD connected to one of the classes WiFi access points
you can</p>
<p>connect to the RPI with</p>
<p>ssh pi@ucsdrobocar-xxx-yy.local</p>
<blockquote>
<p>raspberryucsd</p>
</blockquote>
<p>Then make sure to change the hostname and password. See instructions in
this document.</p>
<p>Installing the Pulse Width Modulation (PWM)</p>
<p>https://docs.google.com/document/d/11nu6_ReReoIxA1KVq-sCy7Tczbk6io0izcItucrw7hI/edit</p>
<p><img alt="" src="../10images/media/image39.png" />{width="5.15625in"
height="5.447916666666667in"}</p>
<p>For reference, below is the Raspberry Pi Pinout.</p>
<p>You will notice we connect to +3.3v, the two I2C pins (SDA and SCL) and
ground:</p>
<p><img alt="" src="../10images/media/image29.png" />{width="6.8125in"
height="5.947916666666667in"}</p>
<p>After connecting the I2C PWM Controller lets test the communication</p>
<p>Power the RPI, you can use the USB power adapter for this test</p>
<p>sudo i2cdetect -y 1</p>
<p>The output is something like</p>
<p>0 1 2 3 4 5 6 7 8 9 a b c d e f</p>
<p>00: -- -- -- -- -- -- -- -- -- -- -- -- --</p>
<p>10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p>
<p>20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p>
<p>30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p>
<p>40: [40]{.mark} -- -- -- -- -- -- -- -- -- -- -- -- -- --
--</p>
<p>50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p>
<p>60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p>
<p>70: [70]{.mark} -- -- -- -- -- -- --</p>
<p>(dk)pi@jackrpi02:\~ \$</p>
<p>Look for a similar result above. 40 ...70 in this case is what we are
looking for.</p>
<p>Now that the RPI3 is talking to the I2C PWM board we can calibrate the
Steering and Throttle.</p>
<p>Lets get the EMO circuit working too in case you need to use it.</p>
<p>For the UCSD RoboCars, an Emergency Stop Circuit is required.</p>
<p>COSMOS RoboCars are not required to have the EMO</p>
<p>After you charge your Lithium Polymer (LiPo) battery(ries) - <a href="https://rogershobbycenter.com/lipoguide/">[some info
here]{.underline}</a></p>
<p>Connect the battery(ries) <strong>and batteries monitor/alarm</strong></p>
<p><strong>[Please do not use the batteries without the batteries
monitor/alarms]{.mark}</strong></p>
<blockquote>
<p><strong>If you discharge a LiPO batteries below a threshold lets say 3.2
volts. It may not recover to be charged. That is why it is critical
that you use the LiPo batteries alarm all the time.</strong></p>
</blockquote>
<p>Connecting the Emergency Stop Circuit and Batteries into the Robot</p>
<p>For this part of your robot, you will have to do some hacking. That is
part of the class.</p>
<p>The instructor will discuss the principle of the circuit and how to
implement it with the component in your robot kit.</p>
<p>Long story short, the PWM Controller we use has a disable pin. It the
correct logic is applied to it for example Logic 1 or 0 (3.3V or 0V) it
will disable the PWM signals and the UCSDRoboCar will stop.</p>
<p>Think why one needs a separate EMO circuit vs. relying on the software,
operating system and computer communicating with the PWM controller that
then send PWM pulses to the actuators (steering servo and driving DC
motor by the electronics speed controller)</p>
<p>First search on the web and read the datasheet of the emergency stop
switch (EMO) components provided with the robot kit and discuss them
with your teammates how the EMO will work. You got two main components
to build the a WireLess EMO switch:</p>
<p>c)  A Wireless Relay with wireless remote controls</p>
<p>d)  A Red/Blue high power LED. This is to help the user know if the car
    is disable (Blue) or Enabled (Red).</p>
<ul>
<li>
<p>What is the disable pin the PWM controller?</p>
</li>
<li>
<p>Does it disable on logic 1 or 0?</p>
</li>
<li>
<p>How to wire the wireless relay to provide the logic you need to
    disable PWM controller? (1 or 0)</p>
</li>
<li>
<p>How to connect the LEDs (Blue and Red) to indicate (RED -
    hot/enabled), (BLUE - power is on / disabled).</p>
</li>
<li>
<p>Note: The power to the PWM controller, that powers the LEDs, comes
    from ESC (Electronics Speed Controller). Therefore, you have to
    connect the robot batteries to the ESC and the ESC to the PWM
    controller. <strong>We are using channel 2 for the ESC</strong>. <strong>Channel 1 for
    the Steering.</strong></p>
</li>
</ul>
<p>After you see that the EMO is working, i.e. wireless remote control
disable the PWM, and the LEDs light up as planned, then you need to
document your work. Please use a schematic software such as Fritzing
(<a href="http://fritzing.org/home/">[http://fritzing.org/home/]{.underline}</a>)
to document your electrical schematic.</p>
<p>It may seem we did the opposite way; document after your build. In our
case, you learned by looking at components information, thinking about
the logic, and experimenting. Since you are an engineer you need to
document even it if was a hack...</p>
<p>Working in a company you may do fast prototyping first then document. On
larger projects you make schematics, diagrams, drawings, work
instructions, then build it. Keep that in mind!</p>
<p>Calibration of the Throttle and Steering</p>
<p>Before you develop code or you can use a someone\'s code to control a
robot, we need to calibrate the actuator/mechanism to find out its range
of motion compared to the control / command a computer will send to the
controller of the actuator/mechanism.</p>
<p>The calibration is car specific. If you use the same platform, the
numbers should be really close. Otherwise, you will need to calibrate
your car.</p>
<p>[We are following the standard from RC CAR world, Channel 1 for
Steering, Channel 2 for Throttle]{.mark}</p>
<blockquote>
<p>Note:The default DonkeyCar build uses Channels 0 and 1</p>
</blockquote>
<p>The UCSDRoboCar has at least two actuators. A steering servo and the DC
motor connected to an Electronics Speed Controller (ESC). These are
controlled by PWM (Pulse Width Modulation).</p>
<p>We use PWM Controller to generate the PWM signals, a device similar to
the one in the picture below</p>
<p><img alt="" src="../10images/media/image5.png" />{width="4.015625546806649in"
height="2.804853455818023in"}</p>
<p>Shutdown the RPI if it is on type this command</p>
<blockquote>
<p>sudo shutdown -h now</p>
</blockquote>
<p>Connect the Steering Servo to the Channel 1</p>
<p>Connect the Throttle (ESC) to Channel 2</p>
<p>Observe the orientation of the 3 wires connector coming from the
Steering Servo and ESC. Look for a the black or brown wire, that is the
GND (-).</p>
<blockquote>
<p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p>
<p>Follow the safety guidelines provided in person in the class. If
something does not sound right, don't do it. Safety first.</p>
<p>Power on the RPI</p>
<p>Power the Electronic Speed Controller (ESC)</p>
<p>Enable the robot using your EMO wireless control - The safety LED
should be RED</p>
</blockquote>
<p>Lets run a Python command to calibrate Steering and Throttle</p>
<p>The donkey commands need to be run from the directory you created for
your car, i.e., [d2]{.mark}</p>
<p>If you have not done so, SSH into the RPI</p>
<p>If needed, change directory to d2t</p>
<p>cd d2t</p>
<p>The output</p>
<p>(env) pi@jackrpi10:\~/projects/d3 \$</p>
<p>donkey calibrate [--channel 1]{.mark}</p>
<p>Try some values around 400 to center the steering</p>
<blockquote>
<p>Enter a PWM setting to test(100-600)370</p>
<p>Enter a PWM setting to test(100-600)365</p>
<p>Enter a PWM setting to test(100-600)360</p>
</blockquote>
<p>[In my case]{.mark}, 365 seems to center the steering</p>
<p>Take note of the left max, right max,</p>
<p>ex:</p>
<p>365 - Center</p>
<p>[285]{.mark} - Steering left max</p>
<p>[440]{.mark} - Steering right max</p>
<p>Note: When donkey calibrate times-out after few tries, just run it again</p>
<p>donkey calibrate [--channel 1]{.mark}</p>
<p>You can interrupt the calibration by typing CTRL-C</p>
<blockquote>
<p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p>
</blockquote>
<p>Let me say this one more time</p>
<blockquote>
<p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p>
</blockquote>
<p>Now to calibrate the Throttle Electronic Speed Controller ESC (THROTTLE)</p>
<p>Following the standard for R/C cars. Throttle goes on channel [2]{.mark}</p>
<p>donkey calibrate [--channel 2]{.mark}</p>
<blockquote>
<p>Enter a PWM setting to test(100-600)[370 (neutral)]{.mark}</p>
<p>Enter a PWM setting to test(100-600)380</p>
<p>Enter a PWM setting to test(100-600)390</p>
</blockquote>
<p>370 when power up the ESC seems to be the middle point (neutral),</p>
<p>lets use 370 also to be compatible with Donkey.</p>
<p>Neutral when power the ESC - [370]{.mark}</p>
<p>Then go in increments of 10\~20 until you can no longer hear increase on
the speed</p>
<p>of the car. Don't worry much about the max speed since we won't drive
that fast autonomously</p>
<p>and during training the speed will be limited.</p>
<p>Max speed forward - [460]{.mark}</p>
<p>Reverse on RC cars is a little tricky because the ESC needs to receive</p>
<p>a reverse pulse, zero pulse, and again reverse pulse to start to go
backwards.</p>
<p>Use the same technique as above set the PWM [setting to your zero
throttle (lets say 370)]{.mark}.</p>
<blockquote>
<p>[Enter the reverse value]{.mark}, then the [zero throttle (e.g.,
370)]{.mark} value, then the [reverse value again]{.mark}.</p>
<p>Enter values +/- 10 of the reverse value to find a reasonable reverse
speed. Remember this reverse PWM value.</p>
<p>(dk)<strong>pi@jackrpi02</strong>:<strong>\~/projects/d3 \$</strong></p>
<p>donkey calibrate [--channel 2]{.mark}</p>
<p>...</p>
<p>Enter a PWM setting to test(100-600)360</p>
<p>Enter a PWM setting to test(100-600)[370]{.mark}</p>
<p>Enter a PWM setting to test(100-600)360</p>
<p>Enter a PWM setting to test(100-600)350</p>
<p>Enter a PWM setting to test(100-600)340</p>
<p>Enter a PWM setting to test(100-600)330</p>
<p>Enter a PWM setting to test(100-600)320</p>
<p>Enter a PWM setting to test(100-600)310</p>
<p>Enter a PWM setting to test(100-600)300</p>
<p>Enter a PWM setting to test(100-600)290</p>
</blockquote>
<p>---</p>
<p>Neutral when power the ESC - [370]{.mark}</p>
<p>Max speed forward - [460]{.mark}</p>
<p>Max speed backwards - [280]{.mark}</p>
<p>and from the steering calibration</p>
<p>Center - 365</p>
<p>Steering left max - [285]{.mark}</p>
<p>Steering right max - [440]{.mark}</p>
<p>---</p>
<p>Now let\'s write these values into the car configuration file</p>
<p>(dk)<strong>pi@jackrpi02</strong>:<strong>\~/projects/d3 \$</strong> ls</p>
<blockquote>
<p>config.py <strong>data</strong> <strong>logs</strong> manage.py <strong>models</strong></p>
</blockquote>
<p>Edit the file config.py</p>
<blockquote>
<p>(dk)<strong>pi@jackrpi02</strong>:<strong>\~/projects/d3 \$</strong></p>
<p>nano config.py</p>
</blockquote>
<p>Change these values according to YOUR [calibration values]{.mark} and
[where you have the Steering Servo and ESC]{.mark} connected</p>
<p>...</p>
<p><strong>STEERING</strong></p>
<p>STEERING_CHANNEL = 1</p>
<p>STEERING_LEFT_PWM = [285]{.mark}</p>
<p>STEERING_RIGHT_PWM = [440]{.mark}</p>
<p><strong>THROTTLE</strong></p>
<p>THROTTLE_CHANNEL = [2]{.mark}</p>
<p>THROTTLE_FORWARD_PWM = [460]{.mark}</p>
<p>THROTTLE_STOPPED_PWM = [370]{.mark}</p>
<p>THROTTLE_REVERSE_PWM = [280]{.mark}</p>
<p>Also, if needed change these</p>
<p><strong>JOYSTICK</strong></p>
<p>USE_JOYSTICK_AS_DEFAULT = True</p>
<p>JOYSTICK_MAX_THROTTLE = 0.4</p>
<p>JOYSTICK_STEERING_SCALE = 1.0</p>
<p>AUTO_RECORD_ON_THROTTLE = True</p>
<p>CONTROLLER_TYPE=<strong>\'ps3\'</strong> <strong>(ps3|ps4)</strong></p>
<p>USE_NETWORKED_JS = False</p>
<p>NETWORK_JS_SERVER_IP = <strong>\"192.168.0.1\"</strong></p>
<p><strong>LED</strong></p>
<p>HAVE_RGB_LED = True</p>
<p>LED_INVERT = False <strong>COMMON ANODE?</strong></p>
<p><strong>board pin number for pwm outputs</strong></p>
<p>LED_PIN_R = 12</p>
<p>LED_PIN_G = 10</p>
<p>LED_PIN_B = 16</p>
<p><strong>LED status color, 0-100</strong></p>
<p>LED_R = 0</p>
<p>LED_G = 0</p>
<p>LED_B = 10</p>
<p>Another example of configuration for the config.py, now with a joystick
dead zone for neutral</p>
<p>donkey calibrate --channel 1</p>
<p>donkey calibrate --channel 2</p>
<p>config.py settings</p>
<p><strong>STEERING</strong></p>
<p>STEERING_CHANNEL = 1</p>
<p>STEERING_LEFT_PWM = 275</p>
<p>STEERING_RIGHT_PWM = 440</p>
<p>Center \~ 360</p>
<p><strong>THROTTLE</strong></p>
<p>THROTTLE_CHANNEL = 2</p>
<p>THROTTLE_FORWARD_PWM = 460</p>
<p>THROTTLE_STOPPED_PWM = 370</p>
<p>THROTTLE_REVERSE_PWM = 220</p>
<p>JOYSTICK_MAX_THROTTLE = 0.8</p>
<p>At the UCSD Outdoor Track my car was turning too much. I scaled the
steering. May need to increase the number to have sharper turns.</p>
<p>JOYSTICK_STEERING_SCALE = 0.8</p>
<p>Because I have a particular joystick that the Throttle neutral was not
centered, I had to</p>
<p>use a deadzone value on config.py</p>
<p>Try first with deadzone= 0</p>
<p>JOYSTICK_DEADZONE = 0.02</p>
<p><strong>LED</strong></p>
<p>HAVE_RGB_LED = True</p>
<p><strong>board pin number for pwm outputs</strong></p>
<p>LED_PIN_R = 8</p>
<p>LED_PIN_G = 10</p>
<p>LED_PIN_B = 12</p>
<p><strong>LED status color, 0-100</strong></p>
<p>LED_R = 0</p>
<p>LED_G = 0</p>
<p>LED_B = 60</p>
<p>Note: If you robot has too much power or not enough power when you start
driving it, adjust the max_throttle</p>
<p>JOYSTICK_MAX_THROTTLE = 0.5</p>
<p>This also will be your starting power setting when using the constant
throttle autopilot.</p>
<p>Connecting the PS3 Keypad/Joystick</p>
<p><a href="http://docs.donkeycar.com/parts/controllers/">[http://docs.donkeycar.com/parts/controllers/]{.underline}</a></p>
<p>Bluetooth Setup</p>
<p><a href="https://pythonhosted.org/triangula/sixaxis.html">[https://pythonhosted.org/triangula/sixaxis.html]{.underline}</a></p>
<p>We need to install some bluetooth software in the RPI</p>
<p>ssh to the RPI</p>
<p>sudo apt-get update</p>
<p>sudo apt-get install bluetooth libbluetooth3 libusb-dev</p>
<p>sudo systemctl enable bluetooth.service</p>
<p>sudo usermod -G bluetooth -a pi</p>
<p>shutdown the RPI</p>
<p>sudo shutdown now</p>
<p>Remove the power from the RPI. Wait \~ 5 s.</p>
<p>Power on the RPI,</p>
<p>Connect your PS3 Controller to the RPI using a mini USB Cable.</p>
<p>SSH to the RPI</p>
<p>Lets download, compile, and install some bluetooth configuration
software</p>
<p>wget http://www.pabr.org/sixlinux/sixpair.c</p>
<p>gcc -o sixpair sixpair.c -lusb</p>
<p>sudo ./sixpair</p>
<p>Output should be something similar to this</p>
<blockquote>
<p>Current Bluetooth master: b8:27:eb:49:2d:8c</p>
<p>Setting master bd_addr to b8:27:eb:49:2d:8c</p>
</blockquote>
<p>Execute the 'bluetoothctl' command</p>
<p>Pay attention to your PS3 controller [mac address]{.mark}</p>
<p>bluetoothctl</p>
<p>the output should be something similar to this</p>
<p>(env) pi@jackrpi10:\~ \$ bluetoothctl</p>
<p>[NEW] Controller B8:27:EB:7A:12:18 jackrpi10 [default]</p>
<p>[NEW] Device [64:D4:BD:03:CD:C9]{.mark} PLAYSTATION(R)3 Controller</p>
<p>If you don't see the PS3 Controller, unplug and plug it in RPI again</p>
<p>Type agent on</p>
<p>[bluetooth]agent on</p>
<blockquote>
<p>Agent is already registered</p>
</blockquote>
<p>Type default-agent</p>
<p>default-agent</p>
<p>If you don't see the PS3 Controller, unplug and plug it in RPI again</p>
<p>default-agent64:D4:BD:03:CD:C9</p>
<blockquote>
<p>[bluetooth]default-agent</p>
<p>Default agent request successful</p>
<p>[NEW] Device 64:D4:BD:03:CD:C9 Sony PLAYSTATION(R)3 Controller</p>
<p>Authorize service</p>
<p>[agent] Authorize service 00001124-0000-1000-8000-00805f9b34fb
([yes]{.mark}/no):</p>
</blockquote>
<p>Type yes</p>
<p>Yes</p>
<blockquote>
<p>[CHG] Device 64:D4:BD:03:CD:C9 Trusted: yes</p>
<p>[CHG] Device 64:D4:BD:03:CD:C9 UUIDs:
00001124-0000-1000-8000-00805f9b34fb</p>
<p>[NEW] Device [64:D4:BD:03:CD:C9]{.mark} Sony PLAYSTATION(R)3
Controller</p>
<p>Authorize service</p>
</blockquote>
<p>Yes</p>
<p>Type trust and the MAC address for the PS3 controller</p>
<p>ex: [bluetooth]trust 64:D4:BD:03:CD:C9</p>
<p>trust THE_MAC_ADDRESS_PS3_CONTROLLER</p>
<p>Changing 64:D4:BD:03:CD:C9 trust succeeded</p>
<blockquote>
<p>[CHG] Device 64:D4:BD:03:CD:C9 Trusted: yes</p>
</blockquote>
<p>[bluetooth]quit</p>
<p>Agent unregistered</p>
<p>Lets check what input devices are available</p>
<p>ls /dev/input</p>
<p>Output</p>
<p>by-id by-path event0 event1 [js0]{.mark} mice</p>
<p>you should see a [js0]{.mark}</p>
<p>If connected by the USB cable, disconnect the PS3 controller from the
RPI.</p>
<p>Lets list the input devices again</p>
<p>ls /dev/input</p>
<p>Output</p>
<p>mice</p>
<p>Now press the \&lt;PS&gt; button at the PS3 controller,</p>
<p>the lights on the front of the controller should flash for a couple of
seconds then stop</p>
<p>The LED 1 should then stay one or flash then goes off. Since we are not
using a PS3 game</p>
<p>console, the LEDS on the controller showing the connections are not
reliable.</p>
<p>Again, we are not connecting the PS3 controller to a PS3 game
console...). As long as you see a</p>
<p>js0 listed on your input devices you are good to go. See below.</p>
<p>ls /dev/input</p>
<p>Output</p>
<p>event0 event1 [js0]{.mark} mice</p>
<p>Please keep your PS3 Controller Off when you are not using it. Press and
hold the \&lt;PS&gt;</p>
<p>button for \~10s. The LEDs at the controller will flash then go off.</p>
<p>If the PS3 controller connection is intermittent, try</p>
<p>sudo rpi-update</p>
<p>To remove a device, lets say another JoyStick that you don't use anymore</p>
<p>bluetoothctl</p>
<p>paired-devices</p>
<p>remove THE_MAC_ADDRESS</p>
<p>[NEW] Controller B8:27:EB:72:95:A6 rpimine01 [default]</p>
<p>[NEW] Device 00:1E:3D:D8:EA:15 PLAYSTATION(R)3 Controller</p>
<p>[NEW] Device 00:16:FE:74:12:B7 PLAYSTATION(R)3 Controller</p>
<p>[bluetooth]remove 00:1E:3D:D8:EA:15</p>
<p>[DEL] Device 00:1E:3D:D8:EA:15 PLAYSTATION(R)3 Controller</p>
<p>Device has been removed</p>
<p>[bluetooth]</p>
<p>If you are having problems pairing the PS3 controller, please reset it.</p>
<p>There is "small" reset button at the back. Look up at the Internet if
you can't find it.</p>
<p>This is another method that worked for me with the the RaspbianBuster
(Sep 2019)</p>
<p>[sudo apt-get install bluetooth libbluetooth3 libusb-dev]{.mark}</p>
<p>[sudo systemctl enable bluetooth.service]{.mark}</p>
<p>[sudo bluetoothctl]{.mark}</p>
<p>[agent on]{.mark}</p>
<p>[default-agent]{.mark}</p>
<p>[scan on]{.mark}</p>
<p>Plug the PS3 Controller in the RPI using a Mini USB Cable</p>
<p>Trust when asked</p>
<p>Disconnect the PS3 Controller</p>
<p>Press the PS Button on the Controller. See if it will pair</p>
<p>Now you can go drive you robot to collect data. Make sure to keep the</p>
<p>EMO handy and use when needed!</p>
<p>Also the \&lt;X&gt; on your controller is like an emergency break.</p>
<p>Driving the Robot to Collect data</p>
<p>Remember you are driving at max of <strong>x</strong> (0.<strong>x</strong>) Throttle power based
on the</p>
<p>config.py that you edited.</p>
<p>To reverse you may have to reverse, stop, reverse. This is a feature of
the</p>
<p>ESC using on RC cars to prevent damaging gears when changing from
forward to reverse.</p>
<p>On the RPI</p>
<p>cd d2t</p>
<p>(env) pi@jackrpi10:\~/projects/d3 \$</p>
<p>[python manage.py drive]{.mark}</p>
<p>Note: CTRL-C stop the manage.py drive</p>
<p>Output below</p>
<p>---</p>
<blockquote>
<p>(env) pi@jackrpi10:\~/projects/d3 \$ python manage.py drive</p>
<p>using donkey v2.5.0t ...</p>
<p>loading config file: /home/pi/d2t/config.py</p>
<p>config loaded</p>
<p>cfg.CAMERA_TYPE PICAM</p>
<p>PiCamera loaded.. .warming camera</p>
<p>Adding part PiCamera.</p>
<p>Adding part PS3JoystickController.</p>
<p>Adding part ThrottleFilter.</p>
<p>Adding part Lambda.</p>
<p>Adding part Lambda.</p>
<p>Adding part Lambda.</p>
<p>Init ESC</p>
<p>Adding part PWMSteering.</p>
<p>Adding part PWMThrottle.</p>
<p>Tub does NOT exist. Creating new tub...</p>
<p>New tub created at: /home/pi/d2t/data/tub_2_18-09-25</p>
<p>Adding part TubWriter.</p>
<p>You can now move your joystick to drive your car.</p>
<p>Starting vehicle...</p>
<p>Opening /dev/input/js0...</p>
<p>Device name: PLAYSTATION(R)3 Controller</p>
<p>/home/pi/env/lib/python3.5/site-packages/picamera/encoders.py:544:
PiCameraResolutionRounded: frame size rounded up from 160x120 to
160x128</p>
<p>width, height, fwidth, fheight)))</p>
</blockquote>
<p>---</p>
<p>If you get this error trying to drive it is because the PS3 game
controller is off</p>
<p>or not connected</p>
<p>Output</p>
<p>/dev/input/js0 is missing</p>
<p>ls</p>
<p>config.py data logs manage.py models sixpair sixpair.c</p>
<blockquote>
<p>ls data</p>
<p>tub_1_17-10-13</p>
<p>If you want to wipe clean the data collected</p>
<p>Remove the content of the \~./d2t/data directory. It should be
tub_......</p>
<p>You can delete the entire directory then create it again.</p>
<p>(dk)pi@jackrpi02:\~/projects/d3 \$</p>
<p>rm -rf data</p>
<p>mkdir data</p>
</blockquote>
<p>Donkey Install Using Linux - Ubuntu</p>
<p>It is highly recommended that you use Ubuntu in this class. Ideally you
can make your computer</p>
<p>dual boot to Ubuntu. Look for instructions at the Internet for that.
PLEASE backup your computer</p>
<p>first.</p>
<p>Alternatively, you can install a Virtual Machine management software.
e.g., VirtualBox.</p>
<p>Then install Ubuntu.</p>
<p>These first instructions wont have GPU support.</p>
<p>Nvidia GPU support documentation is provided later in this document.</p>
<p>Installation of GPU features won't be supported in the class.</p>
<p>You will have access to a GPU Cluster from the UC San Diego
Supercomputer Center.</p>
<p>Lets refresh the Ubuntu repositories and installed software</p>
<p>sudo apt-get update</p>
<p>sudo apt-get upgrade</p>
<p>First lets create a directory to store your projects</p>
<p>mkdir projects</p>
<p>cd projects</p>
<p>Install some necessary software and create a virtual environment</p>
<p>sudo apt-get install virtualenv build-essential python3-dev gfortran
libhdf5-dev libatlas-base-dev</p>
<p>virtualenv env -p python3</p>
<p>Activate the virtual environment</p>
<p>source env/bin/activate</p>
<p>Look for the [(env)]{.mark} in front of your command line. It is
indication that env is active</p>
<p>To deactivate an environment type deactivate from inside the environment</p>
<p>To activate, make sure you are at the projects directory then type
source env/bin/activate</p>
<p>To install specific versions of Keras and Tensorflow (non-GPU)</p>
<p>Install Keras</p>
<p>pip install keras==2.2.2</p>
<p>Install tensorflow</p>
<p>pip install tensorflow==1.10.0</p>
<p>As 03Feb19, the latest version of tensorflow is 1.12</p>
<p>Keep the same version of Tensorflow as close as the Tensorflow in the
robocar</p>
<p>(Raspberry) as feasible.</p>
<p>pip install tensorflow==1.12</p>
<p>if needed, install keras 2.2.4</p>
<p>pip install keras==2.2.4</p>
<blockquote>
<p>Run a short TensorFlow program to test it</p>
<p>Invoke python from your shell as follows:</p>
<p>\$ python</p>
<p>Enter the following short program inside the python interactive shell:</p>
<p>Python</p>
<p>import tensorflow as tf</p>
<p>hello = tf.constant(\'Hello, TensorFlow!\')</p>
<p>sess = tf.Session()</p>
<p>print(sess.run(hello))</p>
</blockquote>
<p>Ctr-D gets out of the Python Tensorflow test.</p>
<p>Install Donkey on your Ubuntu Linux Computer</p>
<p>cd projects</p>
<p>If you want to create another virtual environment or env was not created
yet</p>
<p>Install some necessary software and create a virtual environment</p>
<p>sudo apt-get install virtualenv build-essential python3-dev gfortran
libhdf5-dev libatlas-base-dev</p>
<p>virtualenv env -p python3</p>
<p>Activate the virtual environment</p>
<p>source env/bin/activate</p>
<p>Look for the [(env)]{.mark} in front of your command line. It is
indication that env is active</p>
<p>To deactivate an environment type deactivate from inside the environment</p>
<p>To activate, make sure you are at the projects directory then type
source env/bin/activate</p>
<p>This is like you did on RPI but I had my car under the \~/projects
directory</p>
<p>Lets get the latest Donkey Framework from Tawn Kramer</p>
<p>git clone https://github.com/tawnkramer/donkey\
pip install -e donkey[pc]</p>
<p>Create a car</p>
<p>donkey createcar --path \~/projects/d2t</p>
<p>from here you can transfer data from your RPI,and then train</p>
<p>create a model (autopilot), transfer the model to the RPI, test it.</p>
<p>Using the GPU Cluster to Train the Auto Pilots</p>
<p>27Aug18 - Under development - Please provide feedback</p>
<p>We are following the instruction provide by out IT Team</p>
<p><a href="https://docs.google.com/document/d/e/2PACX-1vTe9sehl7izNJJNypsDNABD4wg-F-AClAi0cYV3pIIRGpCknD7SEWQPEGqy_5DBRmFQtkulLkHkLxEm/pub">[https://docs.google.com/document/d/e/2PACX-1vTe9sehl7izNJJNypsDNABD4wg-F-AClAi0cYV3pIIRGpCknD7SEWQPEGqy_5DBRmFQtkulLkHkLxEm/pub]{.underline}</a></p>
<p>We will be using the UCSD Supercomputer GPU Cluster. There are few steps
that you need to follow. Pay attention to not leave machines running on
the cluster because it is a share resource.</p>
<p>You should be able to log with you student credentials.</p>
<p>Pay attention to not use a GPU machine to do work that does not require
GPU...</p>
<p>To begin, login to your account using an SSH client. Most students will
use their standard email username to sign on</p>
<p>Create Car Project</p>
<p>ssh
<a href="mailto:YOUR_USER_ID@ieng6.ucsd.edu">[YOUR_USER_ID@ieng6.ucsd.edu]{.underline}</a></p>
<p>prep me148f</p>
<p>We will launch a container (similar to a virtual machine) without GPU.
Tensorflow is not installed on it.</p>
<p>Lets launch a machine to enable us to create a robocar using the Donkey
Framework</p>
<p>[No GPU]{.mark} to save resources</p>
<p>launch-py3torch.sh</p>
<blockquote>
<p><em>Attempting to create job (\'pod\') with 4 CPU cores, 16 GB RAM, and
[0 GPU units]{.mark}.</em></p>
</blockquote>
<p>To keep the car name the same as in the RPI I created a d2t car</p>
<p>source activate /datasets/conda-envs/me148f-conda/</p>
<p>donkey createcar --path projects/[d2t]{.mark}/</p>
<p>The car was created under projects</p>
<p>cd \~/projects/d2t</p>
<p>Lets exit the container you used to create the Car</p>
<p>exit</p>
<p>Lets check that we don't have any container running under our user</p>
<p>kubectl get pods</p>
<blockquote>
<p><em>No resources found.</em></p>
</blockquote>
<p>If there is nothing still running, lets get out of the ieng6 machine</p>
<p>logout</p>
<p>If you see a container still running,</p>
<p>f "kubectl get pods" shows leftover containers still running, you may
kill them as follows:</p>
<blockquote>
<p>[ee148vzz@ieng6-201]:\~:505\$ [kubectl get pods]{.mark}</p>
<p>NAME READY STATUS RESTARTS AGE</p>
<p>ee148vzz-24313 1/1 Running 0 9m</p>
<p>[ee148vzz@ieng6-201]:\~:506\$ [kubectl delete pod agt-24313]{.mark}</p>
<p>pod \"ee148vzz-24313\" deleted</p>
<p>[ee148vzz@ieng6-201]:\~:507\$ [kubectl get pods]{.mark}</p>
<p>No resources found.</p>
<p>[ee148vzz@ieng6-201]:\~:508\$</p>
</blockquote>
<p>If there is no container running, lets get out of the ieng6 machine</p>
<p>logout</p>
<p>Now, let's bring some data in so we can train on it</p>
<p>Use rsync command below in one line</p>
<p>You can rsync direct from the RPI. SSH to the RPI the issue the rsync
command to rsync direct to the GPU cluster. Or you can rsync the data to
your computer then rsync to the GPU cluster</p>
<p>Example of rsync one particular Tub directory</p>
<p>rsync -avr -e ssh --rsync-path=cluster-rsync tub_1_17-11-18
[YOUR_USER_ID]{.mark}\@ieng6.ucsd.edu:projects/d2/data/</p>
<p>Example of rsync the entire data directory\
Sending data to the UCSD GPU Cluster direct from the RPI\
rsync -avr -e ssh --rsync-path=cluster-rsync \~/projects/d3/data/*
[YOUR_USER_ID]{.mark}\@ieng6.ucsd.edu:projects/[d2]{.mark}/data/\
\
or if you have the car created at d2t at the RPI and GPU Cluster\
\
rsync -avr -e ssh --rsync-path=cluster-rsync \~/projects/d3/data/*
YOUR_USER_ID@ieng6.ucsd.edu:projects/[d2t]{.mark}/data/</p>
<p>To enable constant rsync and train while you collect data (drive) we
need to exchange a key between the two computers we will rsync. e.g.,
GPU Cluster and RPI, your computer and RPI, your computer and the GPU
Cluster</p>
<p>To Enable Continuous Training - And to not have to type password for SSH
and RSYNC</p>
<p>At the PC or RPI</p>
<p><strong>Generate the Private and Public Keys</strong></p>
<p>cd \~</p>
<p>sudo apt-get install rsync</p>
<p>ssh-keygen -t rsa</p>
<p>You may see a warning if you already have a key in your system. Just
keep or replace it as you</p>
<p>wish. Keep in mind that if you replace the key your previous trust
connections will be lost.</p>
<p>Linux Ubuntu</p>
<p>cat .ssh/id_rsa.pub</p>
<p>macos</p>
<p>cat \~/.ssh/id_rsa.pub</p>
<p>Copy the output of the command into a buffer (e.g., Ctrl-C)</p>
<p>You can use two terminals to easy the copy and paste the information</p>
<p>Leave a terminal open with the key you generated</p>
<p>From your PC to the RPI</p>
<p>Try this first. If does not work try the steps below this line</p>
<p>cat \~/.ssh/id_rsa.pub | ssh pi@YOUR_PI_NAME.local \'cat &gt;&gt;
.ssh/authorized_keys\'</p>
<p>or</p>
<p>From the PC to the RPI or PC to the Cluster</p>
<p>At the RPI or GPU Cluster</p>
<p>cd \~</p>
<p>mkdir .ssh</p>
<p>If the directory exists, just ignore this error if you see it</p>
<blockquote>
<p>&gt; mkdir: cannot create directory '.ssh': File exists</p>
</blockquote>
<p>chmod 700 .ssh</p>
<p>chown pi:pi .ssh</p>
<p>nano .ssh/authorized_keys</p>
<p>then copy and paste the information from the host id_rsa.pub you
generated earlier at the PC or RPI</p>
<p>I was able to ssh to the RPI without asking for the password</p>
<p>ex: ssh pi@jackrpi05.local</p>
<p>Here is what I have done to ssh and use rsync from my computer to the
Cluster</p>
<p>without having to type the user password all the time</p>
<p>for user me148f</p>
<p>SSH to ieng6</p>
<p>ssh me148f@ieng6.ucsd.edu</p>
<p>[me148f@ieng6-201]:\~:44\$ cd \~</p>
<p>[me148f@ieng6-201]:\~:45\$ mkdir .ssh</p>
<p>[me148f@ieng6-201]:\~:46\$ chmod 700 .ssh</p>
<p>[me148f@ieng6-201]:\~:48\$ chown me148f:me148f .ssh</p>
<p>Just ignore the error below</p>
<blockquote>
<p>chown: invalid group: \'me148f:me148f\'</p>
</blockquote>
<p>[me148f@ieng6-201]:\~:49\$ nano .ssh/authorized_keys</p>
<p>Now copy and paste the Key you created on your computer.</p>
<p>You can use this same process to ssh / rsync from the RPI to the Cluster</p>
<p>without having to type the user password all the time.</p>
<p>Now you should be able to ssh and rsync to the RPI or Cluster without
typing the user password</p>
<p>ex: ssh
<a href="mailto:YOUR_USER_ID@ieng6.ucsd.edu">[YOUR_USER_ID@ieng6.ucsd.edu]{.underline}</a></p>
<p>If you can not ssh to the RPI or Cluster, fix the problem before
continue</p>
<p>Constant rsync - sync data while you drive the car</p>
<p>Lets make rsync run periodically</p>
<p>We will be using some Python Code</p>
<p>rsync data continuously to the data directory under where the code was
called from</p>
<p>ex: \~/projects/d2t</p>
<p>From the PC to the RPI</p>
<p>At the PC, create a file name continous_data_rsync.py</p>
<p>nano continous_data_rsync.py</p>
<blockquote>
<p>import os</p>
<p>import time</p>
<p>while True:</p>
<p>command = \"rsync -aW --progress %s@%s:%s/data/ ./data/ --delete\"
%\</p>
<p>(\'pi\', \'jackrpi04.local\', \'\~/projects/d3\')</p>
<p>os.system(command)</p>
<p>time.sleep(5)</p>
</blockquote>
<p>call the python code with</p>
<p>python continous_data_rsync.py</p>
<p>From the RPI to the GPU Cluster</p>
<p>Since the GPU cluster can not see your RPI on the network,</p>
<p>we need to RSYNC from the RPI to the GPU Cluster</p>
<p>At the RPI create a file name continous_data_rsync.py</p>
<p>nano continous_data_rsync.py</p>
<blockquote>
<p>import os</p>
<p>import time</p>
<p>while True:</p>
<p>command = \"rsync -aW --progress %s@%s:%s/data/ ./data/ --delete\"
%\</p>
<p>(\'pi\', \'jackrpi04.local\', \'\~/projects/d3\')</p>
<p>os.system(command)</p>
<p>time.sleep(5)</p>
</blockquote>
<p>call the python code with</p>
<p>python continous_data_rsync.py</p>
<p>Continuous Train\
This command fires off the keras training in a mode where it will
continuously look for new</p>
<p>data at the end of every epoch.\
\
Usage:\
donkey contrain [--tub=\&lt;data_path&gt;] [--model=\&lt;path to model&gt;]
[--transfer=\&lt;path to model&gt;]
[--type=\&lt;linear|categorical|rnn|imu|behavior|3d&gt;] [--aug]</p>
<p>example</p>
<p>training for a particular tub</p>
<p>donkey contrain --tub=\~/projects/d2t/data/tub_1_18-08-20
--model=20aug18_just_testing.h5</p>
<p>training for all tub files</p>
<p>donkey contrain --model=24aug18_just_testing.h5</p>
<p>Continuous Training from a PC</p>
<p>Youetter have a GPU for training, if not this will take a long time. It
wont work.</p>
<p>If you don't have a NVIDIA GPU on your PC with CUDA</p>
<p>Here what I have done for continuous training using a PC</p>
<p>For continuous training</p>
<p>Read instructions and do the security keys exchange first</p>
<p>After exchanging the security keys to not have to type the password on
ssh and RSYNC</p>
<p>Create a file name continous_data_rsync.py</p>
<p>nano continous_data_rsync.py</p>
<p>import os</p>
<p>import time</p>
<p>while True:</p>
<p>command = \"rsync -aW --progress %s@%s:%s/data/ ./data/ --delete\" %\</p>
<p>(\'pi\', \'[jackrpi01]{.mark}.local\', \'\~/projects/d3\')</p>
<p>os.system(command)</p>
<p>time.sleep(10)</p>
<p>To start the continuous RSYNC</p>
<p>python continous_data_rsync.py</p>
<p>Edit the config.py at your computer so the continuous training works</p>
<p>Continuous Training sends models to the PI when there is new model
created.</p>
<p>No need to have an RSYNC for models.</p>
<p>nano config.py</p>
<p>pi information</p>
<p>PI_USERNAME = \"pi\"</p>
<p>PI_PASSWD = \"[Your_RPI_PassWord_Here]{.mark}\"</p>
<p>PI_HOSTNAME = \"[jackrpi01.local]{.mark}\"</p>
<p>PI_DONKEY_ROOT = \"[d2t]{.mark}\"</p>
<p>SEND_BEST_MODEL_TO_PI = True</p>
<p>save the config.py</p>
<p>Continuous Training</p>
<p>training for all tub files</p>
<p>python train.py --continuous --model=models/date_modelName.h5</p>
<p>training for a particular tub</p>
<p>python train.py --continuous
--tub=\~/projects/d2t_pmm01/data/tub_1_18-08-20
--model=20aug18_just_testing.h5</p>
<p>If set on config.py continuous Training automatically sends the new
model to the RPI.</p>
<p>SEND_BEST_MODEL_TO_PI = True</p>
<p>When using the UCSD's GPU Cluster, you will need an rsync for the models</p>
<p>since the GPU Cluster can not see the RPI</p>
<p>When using the GPU Cluster, rsync works from the RPI to the GPU Cluster</p>
<p>rsync models to the RPI continuously from the models directory where the
code was called from</p>
<p>Create a file name continous_models_rsync.py</p>
<p>nano continous_models_rsync.py</p>
<p>import os</p>
<p>import time</p>
<p>while True:</p>
<p>command = \"rsync -aW --progress ./models/ %s@%s:%s/models/ --delete\"
%\</p>
<p>(\'pi\', \'[jackrpi01.local]{.mark}\', \'\~/projects/d3\')</p>
<p>os.system(command)</p>
<p>time.sleep(30)</p>
<p>To start the continuous RSYNC</p>
<p>python continous_models_rsync.py</p>
<p>Issue the drive command with the model name.</p>
<p>You need a new model name before issuing the command to drive with a
mode.</p>
<p>So lets create a blank model file. Use the same name for the model you
will create with</p>
<p>continuous training</p>
<p>touch \~/projects/d3/[24nov18_ucsd01.json]{.mark}</p>
<p>Load the model_name.json because it loads faster than model_name.h5</p>
<p>python manage.py drive --model=./models/[24nov18_ucsd01.json]{.mark}</p>
<p>Security Keys Exchange</p>
<p>It can save you some time by allowing you log to the RPI without asking
for the password,</p>
<p>same for RSYNC</p>
<p>Dont use this on mission critical computers</p>
<p>At the PC</p>
<p><strong>Generate the Private and Public Keys</strong></p>
<p><strong>Exchange security Keys</strong></p>
<p>cd \~</p>
<p>sudo apt-get install rsync</p>
<p>ssh-keygen -t rsa</p>
<p>ubuntu</p>
<p>cat .ssh/id_rsa.pub</p>
<p>mac</p>
<p>cat \~/.ssh/id_rsa.pub</p>
<p>Copy the output of the command into the buffer (Ctrl-C or Command-C on a
Mac)</p>
<p>At the RPI</p>
<p>cd \~</p>
<p>mkdir .ssh</p>
<blockquote>
<p>Ignore this message if you get it</p>
<p>&gt; mkdir: cannot create directory '.ssh': File exists</p>
</blockquote>
<p>chmod 700 .ssh</p>
<p>chown pi:pi .ssh</p>
<p>nano .ssh/authorized_keys</p>
<p>I was able to ssh to the RPI without asking for the password</p>
<p>ex: ssh pi@jackrpi04.local</p>
<p>Summary of commands after the robot built and software Installed</p>
<p><a href="http://docs.donkeycar.com/guide/get_driving/">[http://docs.donkeycar.com/guide/get_driving/]{.underline}</a></p>
<p>You will need and Access Point (AP) to connect to your RoboCar to be
able to SSH to it and give the commands:</p>
<p>For that we have few options:</p>
<ul>
<li>
<p>Move the WiFi AP to a place close to the track. You will need a
    power outlet there to power the AP</p>
</li>
<li>
<p>Use your phone as an AP. You will need to add an entry in the
    RoboCar WiFi configuration so it connects to your Phone AP. See
    instruction on RPI WiFi configuration. Please make sure it has a
    lower priority than the UCSDRoboCar AP. e.g., 30 and below.
    Moreover, you will need to connect your computer to the same phone
    AP so you can SSH to your RoboCar RPI.</p>
</li>
<li>
<p>You can create a direct connection between your computer and the
    RoboCar. Need to search the net for that. Please make sure the
    RoboCar still connects to the AP UCSDRoboCar after you are done
    using the direct connection with your phone.</p>
</li>
</ul>
<p>If you follow the instructions in this document, the PS3Keypad/Joystick
will be used by default to drive the RoboCar.</p>
<p>Summary of commands after the installs are complete</p>
<p>[Jack's RPI]{.mark}</p>
<p>On the PC</p>
<p>Activate the virtual environment</p>
<blockquote>
<p>cd projects</p>
<p>source \~/env/bin/activate</p>
<p>ssh pi@[jackrpi02.local]{.mark}</p>
</blockquote>
<p>On the RPI</p>
<p>(env)pi@jackrpi02:\~ \$</p>
<blockquote>
<p>cd d2t</p>
<p>python manage.py drive</p>
</blockquote>
<p>Drive the robot to collect data</p>
<p>On the PC</p>
<p>Get data from RPI</p>
<p>(env) jack@virtlnx01:\~/projects/d2t\$</p>
<blockquote>
<p>rsync -a --progress pi@[jackrpi02.local]{.mark}:\~/projects/d3/data
\~/projects/d2t</p>
<p>(env) jack@virtlnx01:\~/projects/d2t\$</p>
</blockquote>
<p>ls data</p>
<blockquote>
<p>tub_1_17-10-12</p>
</blockquote>
<p>Train a model using all tubes in the data directory</p>
<p>(env) jack@lnxmbp01:\~/projects/d2t\$</p>
<p>python train.py --model=models/[date_model_name]{.mark}.h5</p>
<p>To use on particular tube</p>
<blockquote>
<p>python train.py --tub \~/projects/d2t/data/tub_1_18-01-07
--model=models/07jan18_coleman_tube1Test.h5</p>
</blockquote>
<p>To make an incremental training using a previous model</p>
<blockquote>
<p>python train.py --tub \~/projects/d2t/data/NAME_OF_NEW_TUBE
--transfer=models/NAME_OF_PREVIOUS_MODEL.h5
--model=models/NAME_OF_NEW_MODEL.h5</p>
</blockquote>
<p>To clean-up tubs</p>
<p>(env) jack@LnxWS1:\~/projects/d2t\$</p>
<blockquote>
<p>donkey tubclean data</p>
<p>using donkey v2.2.0 ...</p>
<p>Listening on 8886...</p>
</blockquote>
<p>Open a browser and type</p>
<p><a href="http://localhost:8886">[http://localhost:8886]{.underline}</a></p>
<p><img alt="" src="../10images/media/image15.png" />{width="5.744792213473316in"
height="3.2466776027996502in"}</p>
<p>You can clean-up your tub directories. Please make a backup of your data
before you start to clean it up.</p>
<p>On the mac if the training complains</p>
<blockquote>
<p>rm \~/projects/d2t/data/.DS_Store</p>
</blockquote>
<p>If it complains about docopt, install it again. And I did not change
anything from the previous day. Go figure...</p>
<blockquote>
<p>(env) jack@lnxmbp01:\~/projects/d2\$ pip list</p>
<p>(env) jack@lnxmbp01:\~/projects/d2\$ pip install docopt</p>
<p>Collecting docopt</p>
<p>Installing collected packages: docopt</p>
<p>Successfully installed docopt-0.6.2</p>
</blockquote>
<p>(env) jack@virtlnx01:\~/projects/d2\$</p>
<p>python train.py --model=models/[12oct17_ucsd_day1.h5]{.mark}</p>
<p>See the models here</p>
<p>(env) jack@virtlnx01:\~/projects/d2\$</p>
<blockquote>
<p>ls models</p>
<p>[ucsd_12oct17.h5]{.mark}</p>
</blockquote>
<p>Place Autopilot into RPI</p>
<p>(env) jack@lnxmbp01:\~/projects/d2\$</p>
<blockquote>
<p>rsync -a --progress \~/projects/d2t/models/
pi@[jackrpi02.local:]{.mark}\~/projects/d3/models/</p>
</blockquote>
<p>(dk)pi@jackrpi02:\~/d2 \$</p>
<blockquote>
<p>ls models</p>
<p>[ucsd_12oct17.h5]{.mark}</p>
</blockquote>
<p>On the RPI</p>
<p>Run AutoPilot at the RPI</p>
<p>(dk)pi@jackrpi02:\~/d2 \$</p>
<p>python manage.py drive --model=./models/[ucsd_12oct17.h5]{.mark}</p>
<blockquote>
<p>...</p>
<p>Using TensorFlow backend.</p>
<p>loading config file: /home/pi/d2/config.py</p>
<p>config loaded</p>
<p>PiCamera loaded.. .warming camera</p>
<p>Starting vehicle...</p>
<p>/home/pi/env/lib/python3.4/site-packages/picamera/encoders.py:544:
PiCameraResolutionRounded: frame size rounded up from 160x120 to
160x128</p>
<p>width, height, fwidth, fheight)))</p>
<p>...</p>
</blockquote>
<p>Check Tub</p>
<p>This command allows you to see how many records are contained in any/all
tubs. It will also open each record and ensure that the data is readable
and intact. If not, it will allow you to remove corrupt records.</p>
<p>Usage:</p>
<p>[donkey tubcheck \&lt;tub_path&gt; [--fix]\
]{.mark}</p>
<ul>
<li>
<p>Run on the host computer or the robot</p>
</li>
<li>
<p>It will print summary of record count and channels recorded for each
    tub</p>
</li>
<li>
<p>It will print the records that throw an exception while reading</p>
</li>
<li>
<p>The optional [--fix]{.mark} will delete records that have problem</p>
</li>
</ul>
<p>PS3 Controller Modes</p>
<p>The default mode will be that [User]{.mark} is in Control. That is, the
user controls Steering and Throttle.</p>
<p>To switch to <strong>[Local Angle]{.mark}</strong> (software controls the Steering
and user the Throttle), you need to press the <strong>\&lt;Select&gt;</strong> button in
the Joystick.</p>
<p>If you give Throttle the Robocar should drive around semi-autonomously.</p>
<p>After few laps that you see that your model is good,</p>
<p>[Please hold your robot with the wheels out of the floor]{.mark}</p>
<p>you can press the <strong>\&lt;Start&gt;</strong> button [and immediately press <strong>the
\&lt;left_DOWN_arrow&gt;</strong> button few times to decrease the Throttle as
needed.]{.mark} This is important so you slow down the Robocar for a
constant Throttle.</p>
<p>Press the \&lt;left_UP_arrow&gt; to give it more Throttle as needed.</p>
<p>Pressing \&lt;[X&gt;]{.mark} will stop the robocar and go back to User mode
(user is in control)</p>
<p>You can change the driving modes by pressing the <strong>\&lt;Select&gt;</strong> button.
You should be able to see a message on your computer terminal that is
SSH connected to the RoboCar RPI.</p>
<p>The Local &amp; Angle mode (fully autonomous) is to be used after you see
that you can do few laps with local angle</p>
<blockquote>
<p>Hit Select button to toggle between three modes - User, Local Angle,
and Local Throttle &amp; Angle.</p>
</blockquote>
<ul>
<li>
<p>User - User controls both steering and throttle with joystick</p>
</li>
<li>
<p>Local Angle - Ai controls steering. User controls throttle.</p>
</li>
<li>
<p>Local Throttle &amp; Angle - Ai controls both steering and throttle</p>
</li>
</ul>
<blockquote>
<p>When the car is in Local Angle mode, the NN will steer. You must
provide throttle...</p>
</blockquote>
<p>Ideally you will have \~ 60 laps</p>
<p>If you don't have a good working Auto-Pilot, get more data in 10 laps
increments.</p>
<p>In summary, you may want to start with 60 laps and then do 10\~20 laps
more to see if the model gets better.</p>
<p>I would not worry much about few bad spots when collecting data. Drive
the car back to the track,</p>
<p>then press Green_Triangle to delete the last 5s of data.</p>
<p>Keep driving, you will develop good skills, you will get good data and
better models. If you leave the track, just drive the RoboCar back to
track. It may even learn how to get back to track.</p>
<p>If you keep the data from the same track (ex: UCSD Track) in the
d2t/data directory, as you add more files to it (e.g.,
tub_[5]{.mark}_17-10-13) it will help your model. At the same time it
will take more time to train since your model will read all the data
sets in the directory. You can use transfer model to add new data to a
current model.</p>
<p>Incremental training using a previous model</p>
<blockquote>
<p>python train.py --tub \~/projects/d2t/data/NAME_OF_NEW_TUBE_DATA
--transfer=models/NAME_OF_PREVIOUS_MODEL.h5
--model=models/NAME_OF_NEW_MODEL.h5</p>
</blockquote>
<p>Some Advanced Tools</p>
<p>The visualization tool is to be use on your PC. Please even if you can,
please do not use the GPU Cluster Resources for this.</p>
<p>Visualizing the model driving the car vs. human driver</p>
<p>Install OpenCV</p>
<p>sudo apt-get install python-opencv</p>
<p>pip3 install opencv-python</p>
<p>donkey makemovie --tub=data\tub_file --model=models\model_name.h5
--limit=100 --salient --scale=2</p>
<p>example</p>
<p>donkey makemovie --tub=data/tub_9_19-01-19
--model=models/19jan19_oakland_5.h5 --start 1 --end 1000 --salient
--scale=2</p>
<p>Here are installs with very limited or no support in this course.</p>
<p>There are too many computers and OS variations to support...</p>
<p>28Aug20</p>
<p>The installation on Linux(ubuntu) is much easier now using Conda</p>
<p>See docs.donkeycar.com</p>
<p><a href="https://docs.donkeycar.com/guide/host_pc/setup_ubuntu/">[https://docs.donkeycar.com/guide/host_pc/setup_ubuntu/]{.underline}</a></p>
<p>I will stop maintaining the installing of the donkeycar into the host
computers.</p>
<p>Ubuntu 18.04 - Tensorflow-GPU - CUDA Install - NVIDIA GPUs with CUDA
Cores</p>
<p>If your computer has a NVIDIA GPU with CUDA cores, you can take
advantage of the GPU to</p>
<p>accelerate the AI training. Based on the class experience, results
varies between</p>
<p>3\~10 times faster when comparing to CPU training</p>
<p>Installing CUDA related files and Tensorflow GPU</p>
<p>If you want to try the easier way but not the latest files, we have a
compressed Zip file</p>
<p>with the necessary files for Linux Ubuntu 64 bits</p>
<p>and Tensorflow 1.12 - GPU (latest version on Feb 2019) -
cudnn-10.0-linux-x64-v7.4.1.5,</p>
<p>libcudnn7_7.4.1.5-1+cuda10.0_amd64</p>
<p>[Tensorflow1.12 with GPU CUDA10 cuddnn7.4.2.24 Compute
Capabilties_3.0_5.2_6.1_7.0 for Linux]{.mark}</p>
<p>[The link below has the Tensorflow and CUDA supporting files for Linux
Ubuntu]{.mark}</p>
<p>/* <img alt="" src="../10images/media/image20.png" />{width="3.7864588801399823in"
height="0.39585739282589677in"}</p>
<p>*/</p>
<p><a href="https://drive.google.com/open?id=1xfbn_qy77SjXqpfWQWwti5JFWWtJVjIM">[https://drive.google.com/open?id=1xfbn_qy77SjXqpfWQWwti5JFWWtJVjIM]{.underline}</a></p>
<p>Alternatively, you can download the files from the NVIDIA site as long
as you use the same versions</p>
<p>for cudnn and others libraries</p>
<p>Or build Tensorflow from source with the version of the CUDA files you
have installed.</p>
<p>Building Tensorflow from source can take a few hours even on modern I7s
CPU with SSD disk and</p>
<p>lots of RAM...</p>
<p>Skip these downloads if you are using the files included in the Zipped
file listed above.</p>
<p>Otherwise you can get the files from the NVIDIA site</p>
<p>Download
<a href="http://developer.nvidia.com/cuda-downloads">[http://developer.nvidia.com/cuda-downloads]{.underline}</a></p>
<p>Download Installer for Linux Ubuntu 18.04 x86_64</p>
<p>The base installer is available for download below.</p>
<p>As of 17Apr20</p>
<p>https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1804&amp;target_type=deblocal</p>
<p>At the directory you download files to install</p>
<p>mkdir cuda</p>
<p>mkdir 10.2</p>
<p>wget
https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin</p>
<p>sudo mv cuda-ubuntu1804.pin
/etc/apt/preferences.d/cuda-repository-pin-600</p>
<p>wget
http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb</p>
<p>sudo dpkg -i
cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb</p>
<p>sudo apt-key add
/var/cuda-repo-10-2-local-10.2.89-440.33.01/7fa2af80.pub</p>
<p>sudo apt-get update</p>
<p>sudo apt-get -y install cuda</p>
<p>09Aug20</p>
<p>https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=2004&amp;target_type=deblocal</p>
<p>28Aug20</p>
<p>https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=2004&amp;target_type=deblocal</p>
<p>CUDA 11</p>
<p><img alt="" src="../10images/media/image37.png" />{width="2.604530839895013in"
height="3.0677088801399823in"}</p>
<blockquote>
<p>Base Installer</p>
<p>Installation Instructions:</p>
<p>cd projects</p>
<p>mkdir cuda</p>
<p>cd cuda</p>
<p>wget
https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin</p>
<p>sudo mv cuda-ubuntu2004.pin
/etc/apt/preferences.d/cuda-repository-pin-600</p>
<p>wget
https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda-repo-ubuntu2004-11-0-local_11.0.3-450.51.06-1_amd64.deb</p>
<p>sudo dpkg -i
cuda-repo-ubuntu2004-11-0-local_11.0.3-450.51.06-1_amd64.deb</p>
<p>sudo apt-key add /var/cuda-repo-ubuntu2004-11-0-local/7fa2af80.pub</p>
<p>sudo apt-get update</p>
<p>sudo apt-get -y install cuda</p>
<p>echo \'export PATH=/usr/local/cuda/bin\${PATH:+:\${PATH}}\' &gt;&gt;
\~/.bashrc</p>
</blockquote>
<p>reboot the machine</p>
<p>As 28aug20 nvidia driver 450.51.06 is the latest</p>
<p>After rebooted, nvcc-V and nvidia-smi worked</p>
<p>nvidia-smi</p>
<p><img alt="" src="../10images/media/image11.png" />{width="4.557292213473316in"
height="2.899010279965004in"}</p>
<p><img alt="" src="../10images/media/image7.png" />{width="3.494792213473316in"
height="1.6441896325459318in"}</p>
<p><a href="https://developer.nvidia.com/rdp/cudnn-download">[https://developer.nvidia.com/rdp/cudnn-download]{.underline}</a></p>
<p><img alt="" src="../10images/media/image38.png" />{width="2.2031255468066493in"
height="2.3837095363079617in"}</p>
<p>download both these files</p>
<p>cuDNN Library for Linux (x86_64)</p>
<p>Extract cudnn-11.0-linux-x64-v8.0.2.39.tgz</p>
<p>Make sure you are in the directory where you downloaded the files</p>
<p>tar -xf cudnn-11.0-linux-x64-v8.0.2.39.tgz</p>
<p>cd cudnn-11.0-linux-x64-v8.0.2.39</p>
<p>We have cuda 11.0 at</p>
<p>/usr/local/cuda-11.0</p>
<p>sudo cp cuda/include/cudnn.h /usr/local/cuda/include/</p>
<p>sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/</p>
<p>sudo chmod a+r /usr/local/cuda/include/cudnn.h
/usr/local/cuda/lib64/libcudnn*</p>
<p>sudo cp -R cuda/include/* /usr/local/cuda-11.0/include</p>
<p>sudo cp -R cuda/lib64/* /usr/local/cuda-11.0/lib64</p>
<p>Install libcudnn8_8.0.2.39-1+cuda11.0_amd64.deb</p>
<p>sudo dpkg -i libcudnn8_8.0.2.39-1+cuda11.0_amd64.deb</p>
<p>Install tensorflow with GPU support</p>
<p>For a ubuntu server lets make tensor available without virtual env.</p>
<p>For workstations and donkeycar, see below for virtualenv install</p>
<p>sudo apt install python3-pip</p>
<p>pip3 install --upgrade tensorflow-gpu</p>
<p>Lets test the tensorflow-gpu install</p>
<p>python3</p>
<blockquote>
<p>import tensorflow as tf</p>
<p>tf.config.list_physical_devices(\'GPU\')</p>
<p>exit()</p>
</blockquote>
<p>Optional</p>
<p>Let's install NCCL. NCCL is NVIDIA optimization for multi-GPU use.</p>
<p>Ex: Use GPU in the computer in one external such as eGPU.</p>
<p><a href="https://developer.nvidia.com/nccl/nccl-download">[https://developer.nvidia.com/nccl/nccl-download]{.underline}</a></p>
<p><img alt="" src="../10images/media/image2.png" />{width="3.7239588801399823in"
height="0.7391491688538933in"}</p>
<p>Download and install NCCL using Gdeb package install</p>
<p>O/S agnostic local installer</p>
<p>cd to the download directory</p>
<p>cd \~/Downloads/cuda/10.1</p>
<p>tar -xf nccl_2.4.8-1+cuda10.1_x86_64.txz</p>
<p>cd nccl_2.4.8-1+cuda10.1_x86_64</p>
<p>sudo cp -R * /usr/local/cuda-10.1/targets/x86_64-linux/</p>
<p>sudo ldconfig</p>
<p>Based on the NVIDIA video driver and CUDA that I installed</p>
<p>Pay attention to the version listed ex: CUDA 10.2</p>
<p>Adapt 10.1 to 10.2 or whatever version you installed</p>
<p>nvcc -V</p>
<blockquote>
<p>Command \'nvcc\' not found, but can be installed with:</p>
<p>sudo apt install nvidia-cuda-toolkit</p>
</blockquote>
<p>This line should make nvcc -V work</p>
<p>echo \'export PATH=/usr/local/cuda/bin\${PATH:+:\${PATH}}\' &gt;&gt;
\~/.bashrc</p>
<p>adminlnx@lnxsrv6:\~\$ nvcc -V</p>
<p>nvcc: NVIDIA (R) Cuda compiler driver</p>
<p>Copyright (c) 2005-2020 NVIDIA Corporation</p>
<p>Built on Wed_Jul_22_19:09:09_PDT_2020</p>
<p>Cuda compilation tools, release 11.0, V11.0.221</p>
<p>Build cuda_11.0_bu.TC445_37.28845127_0</p>
<p>adminlnx@lnxsrv6:\~\$</p>
<p>Double checking</p>
<blockquote>
<p>adminlnx@lnxsrv6:\~\$ cd Downloads</p>
<p>adminlnx@lnxsrv6:\~/Downloads\$ cd cuda</p>
<p>adminlnx@lnxsrv6:\~/Downloads/cuda\$ cd 11</p>
<p>adminlnx@lnxsrv6:\~/Downloads/cuda/11\$ sudo apt-get -y install cuda</p>
<p>[sudo] password for adminlnx:</p>
<p>Reading package lists... Done</p>
<p>Building dependency tree</p>
<p>Reading state information... Done</p>
<p>cuda is already the newest version (11.0.3-1).</p>
<p>0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.</p>
</blockquote>
<p>We have the latest version installed...</p>
<blockquote>
<p>We will install the nvidia-cuda-toolkit next</p>
<p>Easier way? But older cuDNN</p>
<p><a href="https://linuxconfig.org/how-to-install-cuda-on-ubuntu-20-04-focal-fossa-linux">[https://linuxconfig.org/how-to-install-cuda-on-ubuntu-20-04-focal-fossa-linux]{.underline}</a></p>
<p>Although you might not end up with the latest CUDA toolkit version,
the easiest way to install CUDA on Ubuntu 20.04 is to perform the
installation from Ubuntu\'s standard repositories.</p>
<p>To install CUDA execute the following commands:</p>
<p>\$ sudo apt update</p>
<p>\$ sudo apt install nvidia-cuda-toolkit</p>
<p><img alt="" src="../10images/media/image26.png" />{width="5.536458880139983in"
height="1.417668416447944in"}</p>
<p>All should be ready now. Check your CUDA version:</p>
<p>If not, try this</p>
<p>echo \'export PATH=/usr/local/cuda/bin\${PATH:+:\${PATH}}\' &gt;&gt;
\~/.bashrc</p>
<p>\$ nvcc --version</p>
<p>nvcc: NVIDIA (R) Cuda compiler driver</p>
<p>Copyright (c) 2005-2019 NVIDIA Corporation</p>
<p>Built on Sun_Jul_28_19:07:16_PDT_2019</p>
<p>Cuda compilation tools, release 10.1, V10.1.243</p>
<p>As 09Ag20, 10.1 is not the latest version of CUDA toolkit.</p>
<p>Let me install it manually</p>
</blockquote>
<p><a href="https://developer.nvidia.com/rdp/cudnn-download">[https://developer.nvidia.com/rdp/cudnn-download]{.underline}</a></p>
<p>As of 23Apr20</p>
<p>cuDNN7.6.5</p>
<p>Adjust instructions accordingly</p>
<p><img alt="" src="../10images/media/image19.png" />{width="6.875in"
height="0.5694444444444444in"}</p>
<p>Download cuDNN Runtime Library for Ubuntu18.04 (Deb)</p>
<p>libcudnn7_7.6.2.24-1+cuda10.1_amd64.deb</p>
<p>Download cuDNN v7.5.1 for CUDA 10.1</p>
<p>cuDNN Runtime Library for Ubuntu18.04 (Deb)</p>
<p>get Gdebi package and install libcudnn using Gdebi</p>
<p>install libcudnn7_7.6.2.24-1+cuda10.1_amd64.deb</p>
<p>Type this to add the CUDA 10.1 to the path</p>
<p>export PATH=/usr/local/cuda-10.1/bin\${PATH:+:\${PATH}}</p>
<p>export PATH=/usr/local/cuda-10.2/bin\${PATH:+:\${PATH}}</p>
<p>Then nvcc -V worked</p>
<p><img alt="" src="../10images/media/image8.png" />{width="5.307292213473316in"
height="0.860424321959755in"}</p>
<p>note release 10.1, adjust for 10.2 or other version you may be
installing</p>
<p>Now that it works, let\'s add the path to be persistent\
add at the end of the file \~./profile\
nano \~/.profile</p>
<p>Add the PATH to include cuda-10.1 ...</p>
<p>set PATH so it includes user\'s private bin if it exists</p>
<p>if [ -d \"\$HOME/.local/bin\" ] ; then</p>
<p>PATH=\"\$HOME/.local/bin:\$PATH\"</p>
<p>fi</p>
<p>PATH=/usr/local/cuda-10.2/bin\${PATH:+:\${PATH}}</p>
<p>reboot to test nvidia-smi and nvcc -V</p>
<p>when installing CUDA it seems it reverted my NVIDIA driver to 4.10</p>
<p>let me install version 430 (the latest as 11May19), I used the</p>
<p>Ubuntu software update / additional drivers - then I tried again
nvidia-smi and nvcc -V</p>
<p><img alt="" src="../10images/media/image1.png" />{width="3.9140627734033244in"
height="2.6093755468066493in"}</p>
<p><img alt="" src="../10images/media/image32.png" />{width="5.380208880139983in"
height="2.796078302712161in"}</p>
<p><a href="https://developer.nvidia.com/rdp/cudnn-download">[https://developer.nvidia.com/rdp/cudnn-download]{.underline}</a></p>
<p><img alt="" src="../10images/media/image28.png" />{width="5.609375546806649in"
height="1.1643700787401574in"}</p>
<p>cudnn-10.1-linux-x64-v7.6.2.24.tgz</p>
<p>cudnn-10.2-linux-x64-v7.6.5.32.tgz</p>
<p>Make sure you are in the directory where you downloaded the files</p>
<p>tar -xf cudnn-10.1-linux-x64-v7.6.2.24.tgz</p>
<p>for cudnn 10.2</p>
<p>tar -xf cudnn-10.2-linux-x64-v7.6.5.32.tgz</p>
<p>sudo cp -R cuda/include/* /usr/local/cuda-10.2/include\
sudo cp -R cuda/lib64/* /usr/local/cuda-10.2/lib64</p>
<p>Optional</p>
<p>Let's install NCCL. NCCL is NVIDIA optimization for multi-GPU use.</p>
<p>Ex: Use GPU in the computer in one external such as eGPU.</p>
<p><a href="https://developer.nvidia.com/nccl/nccl-download">[https://developer.nvidia.com/nccl/nccl-download]{.underline}</a></p>
<p><img alt="" src="../10images/media/image2.png" />{width="3.7239588801399823in"
height="0.7391491688538933in"}</p>
<p>Download and install NCCL using Gdeb package install</p>
<p>O/S agnostic local installer</p>
<p>cd to the download directory</p>
<p>cd \~/Downloads/cuda/10.1</p>
<p>tar -xf nccl_2.4.8-1+cuda10.1_x86_64.txz</p>
<p>cd nccl_2.4.8-1+cuda10.1_x86_64</p>
<p>sudo cp -R * /usr/local/cuda-10.1/targets/x86_64-linux/</p>
<p>sudo ldconfig</p>
<p>29Aug20</p>
<p>TensorFlow 2.3 on Ubuntu 20.04 LTS with CUDA 11.0 and CUDNN 8.0</p>
<p><a href="https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03">[https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03]{.underline}</a></p>
<p><a href="https://medium.com/@cwbernards/tensorflow-2-3-on-ubuntu-20-04-lts-with-cuda-11-0-and-cudnn-8-0-fb136a829e7f">[https://medium.com/@cwbernards/tensorflow-2-3-on-ubuntu-20-04-lts-with-cuda-11-0-and-cudnn-8-0-fb136a829e7f]{.underline}</a></p>
<p>After installing NVIDIA drivers and cuDNN or here in this same document
I have examples</p>
<p><a href="https://gist.github.com/kmhofmann/cee7c0053da8cc09d62d74a6a4c1c5e4">Please refer to my instructions
here</a>.</p>
<p>Now lets install Tensorflow</p>
<p>If you don't have the python3 virtual environment, create one</p>
<p>I am using \~/projects/envs/env1</p>
<p>sudo apt-get update</p>
<p>sudo apt-get install virtualenv</p>
<p>virtualenv --system-site-packages -p python3 \~/projects/envs/env1</p>
<p>Activate your virtual environment</p>
<p>source \~/projects/envs/env1/bin/activate</p>
<p>(env) jack@lnxmbp01:\~/projects\$</p>
<p>easy_install -U pip</p>
<p>To install the latest version of tensorflow with GPU support, you can
use</p>
<p>pip3 install --upgrade tensorflow-gpu</p>
<p>The version on the Zipped file is already old. Left available for
testing only</p>
<p>(if you have new CPUs like Intel I5 and I7), you can use the tensorflow
file included</p>
<p>in the downloaded Zip file earlier in these instructions.</p>
<p><img alt="" src="../10images/media/image20.png" />{width="4.359375546806649in"
height="0.45575240594925637in"}</p>
<p>tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl</p>
<p>[Or alternatively if you believe the released tensorflow use the same
CUDA install you have]{.mark}</p>
<p>[running in your computer, you can use]{.mark}</p>
<p>[pip3 install --upgrade tensorflow-gpu]{.mark}</p>
<p>[Note on using the latest CUDA version. The packages available at the
standard repository may]{.mark}</p>
<p>[not work with the latest CUDA such as CUDA 10]{.mark}</p>
<p>[You need to download Tensorflow that was built for the version of CUDA
and relate]{.mark}</p>
<p>[software you installed]{.mark}</p>
<p>[You can install a particular version of Tensorflow such as 1.4]{.mark}</p>
<p>[pip3 install --upgrade tensorflow-gpu==1.4]{.mark}</p>
<p>[or 1.3.1]{.mark}</p>
<p>[pip3 install --upgrade tensorflow-gpu==1.3.0]{.mark}</p>
<p>To install a particular Tensorflow such as the *.whl file included at
the Zipped</p>
<p>file you have downloaded</p>
<p>use the Tensorflow file name you downloaded</p>
<p>pip3 install --upgrade tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl</p>
<p>you need to be in the same directory where the file is or specify the
complete path.</p>
<p>I usually copy over the tensorflow .whl file to my projects directory
and install it from there.</p>
<p>After a successful install</p>
<p>Run a short Python program to test the Tensorflow install</p>
<p>(env) jack@lnxmbp01:\~/projects\$</p>
<p>python</p>
<p>Enter the following txt, you can cut and paste</p>
<blockquote>
<p>Python</p>
<p>import tensorflow as tf</p>
<p>hello = tf.constant(\'Hello, TensorFlow!\')</p>
<p>sess = tf.Session()</p>
<p>print(sess.run(hello))</p>
</blockquote>
<p>It worked.</p>
<blockquote>
<p>*2018-11-28 03:33:42.219251: I
tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created
TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with
913 MB memory) -&gt; physical GPU (device: 0, name: GeForce GT 750M, pci
bus id: 0000:01:00.0, compute capability: 3.0)\
&gt;&gt;&gt; print(sess.run(hello))\
b\'Hello, TensorFlow!\'\
&gt;&gt;&gt;\
*</p>
</blockquote>
<p>end of the Tesnforflow GPU install on Linux Ubuntu</p>
<p>Some previous Instructions and for older GPUs</p>
<p>How to install Tenforflow with GPU support</p>
<p>Note the instructions below are for an older NVIDIA GPUs. It will work
for modern NVIDIA GPUs</p>
<p>but it wont take advantage of the new features and all its performance</p>
<p>You can adapt the instructions to have the latest CUDA, cuDNN, and
latest Tensorflow with GPU</p>
<p>support.</p>
<p>On some Macbook Pro (\~2013) you may have NVIDIA dGPU (dedicated GPU).</p>
<p>It may require CUDA 8</p>
<p>Make sure you have the NVIDIA driver installed</p>
<p><img alt="" src="../10images/media/image4.png" />{width="6.5in"
height="1.4444444444444444in"}</p>
<p><img alt="" src="../10images/media/image13.png" />{width="6.5in"
height="4.291666666666667in"}</p>
<p>Download the CUDA files from here</p>
<p><a href="https://developer.nvidia.com/cuda-toolkit-archive">[https://developer.nvidia.com/cuda-toolkit-archive]{.underline}</a></p>
<p><img alt="" src="../10images/media/image23.png" />{width="6.5in"
height="2.7222222222222223in"}</p>
<p>Download cuDNN v6.0 for CUDA 8 from here</p>
<p><a href="https://developer.nvidia.com/rdp/cudnn-archive">[https://developer.nvidia.com/rdp/cudnn-archive]{.underline}</a></p>
<p><img alt="" src="../10images/media/image12.png" />{width="6.5in"
height="1.0277777777777777in"}</p>
<p><img alt="" src="../10images/media/image17.png" />{width="6.5in" height="0.625in"}</p>
<p><img alt="" src="../10images/media/image30.png" />{width="6.5in"
height="1.6666666666666667in"}</p>
<p>Open a terminal and navigate to where you saved the files</p>
<p>Then issue the following commands</p>
<p>sudo apt-get update</p>
<p>sudo apt-get install gdebi</p>
<p>sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb</p>
<p>sudo dpkg -i
cuda-repo-ubuntu1604-8-0-local-cublas-performance-update_8.0.61-1_amd64.deb</p>
<p>sudo dpkg -i libcudnn6_6.0.21-1+cuda8.0_amd64.deb</p>
<p>sudo apt-get update</p>
<p>sudo apt-get install cuda</p>
<p>Let it install. It may take a while.</p>
<p>...</p>
<blockquote>
<p>Setting up cuda-toolkit-8-0 (8.0.61-1) ...</p>
<p>Setting up cuda-drivers (375.26-1) ...</p>
<p>Setting up cuda-runtime-8-0 (8.0.61-1) ...</p>
<p>Setting up cuda-demo-suite-8-0 (8.0.61-1) ...</p>
<p>Setting up cuda-8-0 (8.0.61-1) ...</p>
<p>Setting up cuda (8.0.61-1) ...</p>
<p>Processing triggers for initramfs-tools (0.130ubuntu3.5) ...</p>
<p>update-initramfs: Generating /boot/initrd.img-4.15.0-36-generic</p>
<p>Processing triggers for libc-bin (2.27-3ubuntu1) ...</p>
</blockquote>
<p>....</p>
<p>Lets test the install</p>
<p>nvcc --version</p>
<p>Command \'nvcc\' not found, but can be installed with:</p>
<p>export PATH=/usr/local/cuda-8.0/bin\${PATH:+:\${PATH}}</p>
<p>nvcc --version</p>
<p>nvcc: NVIDIA (R) Cuda compiler driver</p>
<p>Copyright (c) 2005-2016 NVIDIA Corporation</p>
<p>Built on Tue_Jan_10_13:22:03_CST_2017</p>
<p>Cuda compilation tools, release 8.0, V8.0.61</p>
<p>nano \~/.profile</p>
<p>Add this line to the end of the file</p>
<p>export PATH=/usr/local/cuda-8.0/bin\${PATH:+:\${PATH}}</p>
<p>sudo reboot now</p>
<p>Open a terminal and run again</p>
<p>nvcc --version</p>
<p>nvidia-smi</p>
<p><img alt="" src="../10images/media/image22.png" />{width="6.5in"
height="3.736111111111111in"}</p>
<p>Now lets install Tensorflow-GPU and test it</p>
<p>sudo apt-get update</p>
<p>sudo apt-get install virtualenv build-essential python3-dev gfortran
libhdf5-dev libatlas-base-dev\
virtualenv env -p python3\
source env/bin/activate\
pip install keras==2.2.2</p>
<p>pip install tensorflow-gpu==1.3.0</p>
<p>Lets Test Tensorflow\
python</p>
<p>Enter the following lines, it is a short program, inside the python
interactive shell:</p>
<p>Python\
import tensorflow as tf\
hello = tf.constant(\'Hello, TensorFlow!\')\
sess = tf.Session()\
print(sess.run(hello))</p>
<p><img alt="" src="../10images/media/image34.png" />{width="6.5in"
height="5.736111111111111in"}</p>
<p>It worked. Note that Tensorflow can see the dGPU</p>
<p>Also try this when inquiring about the Tensorflow version you have
installed</p>
<p>pip3 show tensorflow</p>
<p>Install CUDA to enable GPU Support on Ubuntu 16.04 - Not supported in
this course</p>
<p><a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#post-installation-actions">[http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.htmlpost-installation-actions]{.underline}</a></p>
<p>Installing CUDA on Ubuntu 16.04 to enable using GPU computation such as
TensorFlow.</p>
<p>First I installed the latest NVIDIA Driver from the Ubuntu setup 3rd
party driver</p>
<p>System Settings/Software &amp; Updates/Additional Drivers</p>
<p>NVIDIA xxx Proprietary Tested</p>
<p><strong>CUDA Install on Ubuntu 16.04</strong></p>
<p>Note: Initially I installed the latest CUDA (9), I had to revert to CUDA
8 install because Tensorflow still looks for it.</p>
<p>In the future it may work with CUDA 9</p>
<p>Installing CUDA on Ubuntu 16.04 to enable using GPU computation such as
TensorFlow.</p>
<p>First I installed the latest NVIDIA Driver from the Ubuntu setup 3rd
party driver</p>
<p>System Settings/Software &amp; Updates/Additional Drivers</p>
<p>NVIDIA xxx Proprietary Tested</p>
<p>To remove other CUDA and NVIDIA installs</p>
<p><a href="https://devtalk.nvidia.com/default/topic/903867">[https://devtalk.nvidia.com/default/topic/903867]{.underline}</a></p>
<p>[sudo apt-get remove --purge nvidia-*]{.mark}</p>
<p>After this command, you need to enable the 3rd party video driver again</p>
<p>If needed, remove the repository that may be installing CUDA 9.</p>
<p><a href="https://askubuntu.com/questions/43345/how-to-remove-a-repository">[https://askubuntu.com/questions/43345/how-to-remove-a-repository]{.underline}</a></p>
<p>Download Installers for Linux Ubuntu 16.04 x86_64</p>
<p><a href="https://developer.nvidia.com/cuda-80-ga2-download-archive">[https://developer.nvidia.com/cuda-80-ga2-download-archive]{.underline}</a></p>
<p>Get the base installer and patch</p>
<p>Get CUDA 8 (because Tensorflow was/is not compatible with CUDA 9 yet)</p>
<p>jack@lnxmbp01:\~/Downloads/NVIDIA\$</p>
<p>sudo apt-get update</p>
<p>sudo apt-get install gdebi</p>
<p>sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb</p>
<p>sudo dpkg -i
cuda-repo-ubuntu1604-8-0-local-cublas-performance-update_8.0.61-1_amd64.deb</p>
<p>jack@lnxmbp01:\~/Downloads/NVIDIA\$ sudo apt-get update</p>
<p>jack@lnxmbp01:\~/Downloads/NVIDIA\$ sudo apt-get install cuda</p>
<p>Let it install. It may take a while.</p>
<p>jack@lnxmbp01:\~\$ [nvcc --version]{.mark}</p>
<blockquote>
<p>The program \'nvcc\' is currently not installed. You can install it by
typing:</p>
<p>sudo apt install nvidia-cuda-toolkit</p>
</blockquote>
<p>Cuda8</p>
<p>jack@lnxmbp01:\~\$ [export
PATH=/usr/local/cuda-8.0/bin\${PATH:+:\${PATH}}]{.mark}</p>
<p>jack@lnxmbp01:\~\$ nvcc -V</p>
<blockquote>
<p>nvcc: NVIDIA (R) Cuda compiler driver</p>
<p>Copyright (c) 2005-2016 NVIDIA Corporation</p>
<p>Built on Tue_Jan_10_13:22:03_CST_2017</p>
<p>Cuda compilation tools, release 8.0, V8.0.61</p>
</blockquote>
<p>If you see an error with libcudnn, check versions. CUDA and Tensorflow
need to play nice together with particular versions specially for older
GPUs.</p>
<blockquote>
<p>ImportError: [libcudnn.so.6]{.mark}: cannot open shared object file:
No such file or directory\
Fixed it! When I installed CUDA 8, I originally I had cudnn7 I had to
download cudnn6 from here
<a href="https://developer.nvidia.com/rdp/cudnn-download#a-collapse6-8">[https://developer.nvidia.com/rdp/cudnn-downloada-collapse6-8]{.underline}</a></p>
<p><a href="https://developer.nvidia.com/rdp/cudnn-download#a-collapse6-8">[Download cuDNN v6.0 (April 27, 2017), for CUDA
8.0]{.underline}</a>://developer.nvidia.com/rdp/cudnn-download\
Need to install cuDNN V6.</p>
</blockquote>
<p>Verify CUDA installation</p>
<p>Reboot, if not the nvidia-smi command may not work</p>
<p>jack@lnxmbp01:\~\$ nvcc --version</p>
<p>The program \'nvcc\' is currently not installed. You can install it by
typing:</p>
<p>sudo apt install nvidia-cuda-toolkit</p>
<p>jack@lnxmbp01:\~\$ export
PATH=/usr/local/cuda-9.0/bin\${PATH:+:\${PATH}}</p>
<p>jack@lnxmbp01:\~\$ [nvcc --version]{.mark}</p>
<p>nvcc: NVIDIA (R) Cuda compiler driver</p>
<p>Copyright (c) 2005-2017 NVIDIA Corporation</p>
<p>Built on Fri_Sep__1_21:08:03_CDT_2017</p>
<p>Cuda compilation tools, release 9.0, V9.0.176</p>
<p>jack@lnxmbp01:\~\$ [nvidia-smi]{.mark}</p>
<p>Sat Sep 30 13:22:52 2017</p>
<p>+-----------------------------------------------------------------------------+</p>
<p>| NVIDIA-SMI 384.81 Driver Version: 384.81 |</p>
<p>|-------------------------------+----------------------+----------------------+</p>
<p>| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |</p>
<p>| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |</p>
<p>|===============================+======================+======================|</p>
<p>| 0 GeForce GT 750M Off | 00000000:01:00.0 N/A | N/A |</p>
<p>| N/A 67C P0 N/A / N/A | 403MiB / 1998MiB | N/A Default |</p>
<p>+-------------------------------+----------------------+----------------------+</p>
<p>+-----------------------------------------------------------------------------+</p>
<p>| Processes: GPU Memory |</p>
<p>| GPU PID Type Process name Usage |</p>
<p>|=============================================================================|</p>
<p>| 0 Not Supported |</p>
<p>+-----------------------------------------------------------------------------+</p>
<p>For CUDA 9.2</p>
<p>jack@lnx01:\~\$ nvcc -V</p>
<p>Command \'nvcc\' not found, but can be installed with:</p>
<p>sudo apt install nvidia-cuda-toolkit</p>
<p>jack@lnx01:\~\$ export PATH=/usr/local/cuda-9.2/bin\${PATH:+:\${PATH}}</p>
<p>jack@lnx01:\~\$ nvcc -V</p>
<p>nvcc: NVIDIA (R) Cuda compiler driver</p>
<p>Copyright (c) 2005-2018 NVIDIA Corporation</p>
<p>Built on Tue_Jun_12_23:07:04_CDT_2018</p>
<p>Cuda compilation tools, release 9.2, V9.2.148</p>
<p>Now that ii works, lets add the path to be persistent</p>
<p>jack@lnxmbp01:\~\$ nano \~/.profile</p>
<p>..</p>
<p>set PATH so it includes user\'s private bin directories</p>
<p>PATH=\"\$HOME/bin:\$HOME/.local/bin:\$PATH\"</p>
<p>[PATH=\"/usr/local/cuda-8.0/bin\${PATH:+:\${PATH}}\"]{.mark}</p>
<p>PATH=/usr/local/cuda-9.2/bin\${PATH:+:\${PATH}}</p>
<p>Installing TensorFlow with GPU Support on Ubuntu 16.04- not supported in
this course</p>
<p>[CUDA needs to be installed and tested first]{.mark} - search for
Install CUDA in this document.</p>
<p><a href="https://www.tensorflow.org/install/install_linux#determine_which_tensorflow_to_install">[https://www.tensorflow.org/install/install_linuxdetermine_which_tensorflow_to_install]{.underline}</a></p>
<p><a href="https://www.tensorflow.org/install/install_linux#InstallingVirtualenv">[https://www.tensorflow.org/install/install_linuxInstallingVirtualenv]{.underline}</a></p>
<p><a href="https://github.com/mind/wheels/releases/">[https://github.com/mind/wheels/releases/]{.underline}</a></p>
<p>sudo apt-get update</p>
<p>sudo apt-get upgrade</p>
<p>sudo apt-get install virtualenv</p>
<p>mkdir projects</p>
<p>jack@lnxmbp01:\~/projects\$</p>
<p>virtualenv --system-site-packages -p python3 \~/projects/env</p>
<p>jack@lnxmbp01:\~/projects\$</p>
<p>source \~/projects/env/bin/activate</p>
<p>(env) jack@lnxmbp01:\~/projects\$</p>
<p>(env) jack@lnxmbp01:\~/projects\$</p>
<p>easy_install -U pip</p>
<p>(env) jack@lnxmbp01:\~/projects\$</p>
<p>pip3 install --upgrade tensorflow[-gpu]{.mark}</p>
<blockquote>
<p>Successfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.6.9
numpy-1.13.3 protobuf-3.4.0 six-1.11.0 tensorflow-gpu-1.3.0
tensorflow-tensorboard-0.1.8 werkzeug-0.12.2</p>
</blockquote>
<p>To install a particular version of tensorflow, example</p>
<p>[pip3 install --upgrade tensorflow-gpu==1.4]{.mark}</p>
<p>[(env) jack@lnxmbp01:\~/projects\$]{.mark}</p>
<p>[pip3 install --upgrade tensorflow-gpu==1.3.0]{.mark}</p>
<p>Run a short TensorFlow program to test it</p>
<blockquote>
<p>(env) jack@lnxmbp01:\~/projects\$</p>
<p>python</p>
<p>Python 3.5.2 (default, Sep 14 2017, 22:51:06)</p>
<p>[GCC 5.4.0 20160609] on linux</p>
<p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more
information.</p>
<p>&gt;&gt;&gt;</p>
</blockquote>
<p>Enter the following txt, you can cut and paste</p>
<blockquote>
<p>Python</p>
<p>import tensorflow as tf</p>
<p>hello = tf.constant(\'Hello, TensorFlow!\')</p>
<p>sess = tf.Session()</p>
<p>print(sess.run(hello))</p>
<p>The result should have this at the end.</p>
<p>...</p>
<p>&gt;&gt;&gt; print(sess.run(hello))</p>
<p>b\'Hello, TensorFlow!\'</p>
<p>&gt;&gt;&gt;</p>
</blockquote>
<p>Also try this when inquiring about the Tensorflow version you have
installed</p>
<p>pip3 show tensorflow</p>
<p>Install TensorFlow on MacOS - not supported in this course</p>
<p><a href="https://github.com/tawnkramer/donkey/blob/master/docs/guide/install_software.md#install-donkeycar-on-mac">[From Tawn Kramer
Instructions]{.underline}</a></p>
<p><a href="https://conda.io/miniconda.html">[Install miniconda Python 3.6 64
bit]{.underline}</a></p>
<p><a href="https://conda.io/docs/user-guide/tasks/manage-python.html">[https://conda.io/docs/user-guide/tasks/manage-python.html]{.underline}</a></p>
<p><a href="https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/20/conda/">[https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/20/conda/]{.underline}</a></p>
<p><a href="https://conda.io/docs/commands.html#conda-general-commands">[https://conda.io/docs/commands.htmlconda-general-commands]{.underline}</a></p>
<p><a href="https://www.atlassian.com/git/tutorials/install-git">[Install git 64
bit]{.underline}</a>\
\
Start Terminal\
\
cd \~\
mkdir projects\
cd projects</p>
<p>Download the latest version of miniconda for macos 64 bits from
<a href="https://conda.io/miniconda.html">[https://conda.io/miniconda.html]{.underline}</a></p>
<p>Save the file on your projects directory</p>
<p>bash Miniconda3-latest-MacOSX-x86_64.sh -u</p>
<p>As of 27Oct18, the default Python to be installed will be 3.7</p>
<p>To revert conda to use python 3.6</p>
<p>conda install python=3.6</p>
<p>If you need to create a virtual environment with Python 3.6, that is
required for many of the</p>
<p>TensorFlow for MacOS at the moment. The command below creates an
environment called py36</p>
<p>conda create -n py36 python=3.6 anaconda</p>
<p>Activate the environment</p>
<p>source activate py36</p>
<p>Deactivate the environment</p>
<p>source deactivate</p>
<p>Get the latest donkey from Github.\
git clone https://github.com/tawnkramer/donkey\
cd donkey</p>
<p>Create the Python anaconda environment\
conda env create -f envs/mac.yml</p>
<p>Activate the virtual environment</p>
<p>source activate donkey</p>
<p>To deactivate the virtual environment</p>
<p>source deactivate</p>
<p>To delete a virtual environment</p>
<p>conda remove -n your_env_name --all --verbose</p>
<p>Install Tensorflow\
pip install tensorflow</p>
<p>pip3 show tensorflow</p>
<p>python</p>
<p>Enter the following txt, you can cut and paste</p>
<p>Python\
import tensorflow as tf\
hello = tf.constant(\'Hello, TensorFlow!\')\
sess = tf.Session()\
print(sess.run(hello))\
\
The result should have this at the end.\
...\
&gt;&gt;&gt; print(sess.run(hello))\
\
b\'Hello, TensorFlow!\'\
&gt;&gt;&gt;</p>
<p>Install donkey source and create your local working dir\
pip install -e .[pc]\
donkey createcar --path \~/projects/d2t</p>
<p>Note: After closing the Terminal, when you open it again</p>
<p>you will need to type source activate donkey to re-enable the mappings
to</p>
<p>donkey specific Python libraries</p>
<p>2nd Method for installation on MacOS</p>
<p><a href="http://exponential.io/blog/2015/02/11/install-python-on-mac-os-x-for-development/">[http://exponential.io/blog/2015/02/11/install-python-on-mac-os-x-for-development/]{.underline}</a></p>
<p>cd\
curl -O
https://raw.githubusercontent.com/Homebrew/install/master/install\
ruby install\
rm install</p>
<p>\$ brew install python</p>
<p>\$ brew install python3</p>
<p>\$ brew unlink python &amp;&amp; brew link python</p>
<p>\$ sudo pip install --upgrade pip</p>
<p>\$ sudo pip install virtualenv\
\$ mkdir projects</p>
<p>\$ cd projects</p>
<p>\$ virtualenv --system-site-packages -p python3 \~/projects/env</p>
<p>\$ source \~/projects/env/bin/activate</p>
<p>[(env)]{.mark} Jacks-MBP:projects jack\$ easy_install -U pip</p>
<p>(env) Jacks-MBP:projects jack\$ pip3 install --upgrade tensorflow</p>
<p>...</p>
<p>Successfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.6.9
numpy-1.13.3 protobuf-3.4.0 six-1.11.0 tensorflow-1.3.0
tensorflow-tensorboard-0.1.8 werkzeug-0.12.2</p>
<p>...</p>
<p>Run a short TensorFlow program to test it</p>
<p>(env) Jacks-MBP:projects jack\$ python</p>
<p>Python 3.6.3 (default, Oct 4 2017, 06:09:38)</p>
<p>[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.37)] on darwin</p>
<p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more
information.</p>
<p>&gt;&gt;&gt;</p>
<p>Enter the following txt, you can cut and paste</p>
<blockquote>
<p>Python</p>
<p>import tensorflow as tf</p>
<p>hello = tf.constant(\'Hello, TensorFlow!\')</p>
<p>sess = tf.Session()</p>
<p>print(sess.run(hello))</p>
</blockquote>
<p>The output should be</p>
<blockquote>
<p>&gt;&gt;&gt; Python</p>
<p>... import tensorflow as tf</p>
<p>&gt;&gt;&gt; hello = tf.constant(\'Hello, TensorFlow!\')</p>
<p>&gt;&gt;&gt; sess = tf.Session()</p>
<p>2017-10-13 18:48:46.858423: W
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow
library wasn\'t compiled to use SSE4.2 instructions, but these are
available on your machine and could speed up CPU computations.</p>
<p>2017-10-13 18:48:46.858440: W
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow
library wasn\'t compiled to use AVX instructions, but these are
available on your machine and could speed up CPU computations.</p>
<p>2017-10-13 18:48:46.858455: W
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow
library wasn\'t compiled to use AVX2 instructions, but these are
available on your machine and could speed up CPU computations.</p>
<p>2017-10-13 18:48:46.858459: W
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow
library wasn\'t compiled to use FMA instructions, but these are
available on your machine and could speed up CPU computations.</p>
<p>&gt;&gt;&gt; print(sess.run(hello))</p>
<p>[b\'Hello, TensorFlow!\']{.mark}</p>
<p>&gt;&gt;&gt;</p>
</blockquote>
<p>Ctr-D gets out of the Python Tensorflow test.</p>
<p>Install TensorFlow on MacOS with GPU (NVIDIA CUDA)</p>
<p>not supported in this course</p>
<p>After you have all CUDA configuration working, you may want to use the
version of Tensorflow or compile it from source. Lots of work ...</p>
<p>pip3 install
https://storage.googleapis.com/74thopen/tensorflow_osx/tensorflow-1.8.0-cp36-cp36m-macosx_10_13_x86_64.whl</p>
<p>To compile from source</p>
<p><a href="https://github.com/zylo117/tensorflow-gpu-macosx">[https://github.com/zylo117/tensorflow-gpu-macosx]{.underline}</a></p>
<p><a href="https://docs.bazel.build/versions/master/install-os-x.html#install-with-installer-mac-os-x">[https://docs.bazel.build/versions/master/install-os-x.htmlinstall-with-installer-mac-os-x]{.underline}</a></p>
<p><a href="https://storage.googleapis.com/74thopen/tensorflow_osx/index.html">[https://storage.googleapis.com/74thopen/tensorflow_osx/index.html]{.underline}</a></p>
<p>1.INSTALL NVIDIA DRIVER\
\
2.INSTALL NVIDIA CUDA TOOLKIT (9.1 OR LATER)\
\
3.INSTALL NVIDIA CUDA CUDNN (7.0 OR LATER)\
\
4.SET UP CUDA ENVIRONMENT (MAKE SURE</p>
<p>nvcc -V</p>
<p>WORKS AND PRINTS CUDA VERSION)</p>
<p>5.INSTALL XCODE/COMMAND LINE TOOL 9.3+</p>
<p>6.INSTALL HOMEBREW</p>
<p>7.INSTALL COREUTILS USING</p>
<p>brew install coreutils</p>
<p>brew install llvm</p>
<p>brew install cliutils/apple/libomp</p>
<p>download bazel</p>
<p><a href="https://github.com/bazelbuild/bazel/releases">[https://github.com/bazelbuild/bazel/releases]{.underline}</a></p>
<p>./bazel-0.18.0-installer-darwin-x86_64.sh --user</p>
<p>add /Users/[user_name]{.mark}/bin</p>
<p>sudo nano /etc/paths</p>
<blockquote>
<p>/usr/local/bin</p>
<p>/usr/bin</p>
<p>/bin</p>
<p>/usr/sbin</p>
<p>/sbin</p>
<p>[/Users/jack/bin]{.mark}</p>
</blockquote>
<p>Close the terminal then open a new terminal so the path is in effect.
Basel will work.</p>
<p>cd projects</p>
<p>git clone https://github.com/zylo117/tensorflow-gpu-macosx</p>
<p>Change directory to where you downloaded the tensorflow source</p>
<p>cd tensorflow-gpu-macosx</p>
<p>./configure</p>
<p>Just an example for my macbook pro 2013</p>
<p>Found possible Python library paths:</p>
<p>/Users/jack/miniconda3/lib/python3.6/site-packages</p>
<p>Please input the desired Python library path to use. Default is
[/Users/jack/miniconda3/lib/python3.6/site-packages]</p>
<p>Do you wish to build TensorFlow with Google Cloud Platform support?
[Y/n]: n</p>
<p>No Google Cloud Platform support will be enabled for TensorFlow.</p>
<p>Do you wish to build TensorFlow with Hadoop File System support?
[Y/n]: n</p>
<p>No Hadoop File System support will be enabled for TensorFlow.</p>
<p>Do you wish to build TensorFlow with Amazon AWS Platform support?
[Y/n]: n</p>
<p>No Amazon AWS Platform support will be enabled for TensorFlow.</p>
<p>Do you wish to build TensorFlow with Apache Kafka Platform support?
[Y/n]: n</p>
<p>No Apache Kafka Platform support will be enabled for TensorFlow.</p>
<p>Do you wish to build TensorFlow with XLA JIT support? [y/N]: n</p>
<p>No XLA JIT support will be enabled for TensorFlow.</p>
<p>Do you wish to build TensorFlow with GDR support? [y/N]: n</p>
<p>No GDR support will be enabled for TensorFlow.</p>
<p>Do you wish to build TensorFlow with VERBS support? [y/N]: n</p>
<p>No VERBS support will be enabled for TensorFlow.</p>
<p>Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n</p>
<p>No OpenCL SYCL support will be enabled for TensorFlow.</p>
<p>Do you wish to build TensorFlow with CUDA support? [y/N]: y</p>
<p>CUDA support will be enabled for TensorFlow.</p>
<p>Please specify the CUDA SDK version you want to use. [Leave empty to
default to CUDA 9.0]: 9.1</p>
<p>Please specify the location where CUDA 9.1 toolkit is installed. Refer
to README.md for more details. [Default is /usr/local/cuda]:
/usr/loca/cuda/include</p>
<p>Invalid path to CUDA 9.1 toolkit.
/usr/loca/cuda/include/lib/libcudart.9.1.dylib cannot be found</p>
<p>Please specify the CUDA SDK version you want to use. [Leave empty to
default to CUDA 9.0]: 9.1</p>
<p>Please specify the location where CUDA 9.1 toolkit is installed. Refer
to README.md for more details. [Default is /usr/local/cuda]:</p>
<p>Please specify the cuDNN version you want to use. [Leave empty to
default to cuDNN 7.0]: 7.0</p>
<p>Please specify the location where cuDNN 7 library is installed. Refer to
README.md for more details. [Default is /usr/local/cuda]:</p>
<p>Please specify a list of comma-separated Cuda compute capabilities you
want to build with.</p>
<p>You can find the compute capability of your device at:
https://developer.nvidia.com/cuda-gpus.</p>
<p>Please note that each additional compute capability significantly
increases your build time and binary size. [Default is:
3.5,7.0]3.0,5.2,6.1</p>
<p>Do you want to use clang as CUDA compiler? [y/N]: n</p>
<p>nvcc will be used as CUDA compiler.</p>
<p>Please specify which gcc should be used by nvcc as the host compiler.
[Default is /usr/bin/gcc]:</p>
<p>Do you wish to build TensorFlow with MPI support? [y/N]: n</p>
<p>No MPI support will be enabled for TensorFlow.</p>
<p>Please specify optimization flags to use during compilation when bazel
option \"--config=opt\" is specified [Default is -march=native]:</p>
<p>Would you like to interactively configure ./WORKSPACE for Android
builds? [y/N]: n</p>
<p>Not configuring the WORKSPACE for Android builds.</p>
<p>Preconfigured Bazel build configs. You can use any of the below by
adding \"--config=\&lt;&gt;\" to your build command. See tools/bazel.rc for
more details.</p>
<p>--config=mkl Build with MKL support.</p>
<p>--config=monolithic Config for mostly static monolithic build.</p>
<p>Configuration finished</p>
<p>MCPB01:tensorflow-gpu-macosx jack\$</p>
<p>bazel build --config=cuda --config=opt
--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --action_env PATH
--action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH
//tensorflow/tools/pip_package:build_pip_package</p>
<p>Install Donkey on your MacOS - not supported in this course</p>
<p>This is assuming you have the virtual env created already with Python3</p>
<p>If needed, create the projects directory</p>
<p>mkdir projects</p>
<p>cd projects</p>
<p>Activate the virtual environment</p>
<p>source env/bin/activate</p>
<p>Look for the [(env)]{.mark} in front of your command line. It is
indication that env is active</p>
<p>To deactivate an environment type deactivate from inside the environment</p>
<p>To activate, make sure you are at the projects directory then type
source env/bin/activate</p>
<p>This is like you did on RPI but I had my car under the \~/projects
directory</p>
<p>Let\'s get the latest Donkey Framework from Tawn Kramer</p>
<p>git clone https://github.com/tawnkramer/donkey\
pip install -e donkey[pc]</p>
<p>Create a car</p>
<p>donkey createcar --path \~/projects/d2t</p>
<p>from here you can transfer data from your RPI, train,</p>
<p>create a model (autopilot), transfer the model to the RPI, test it.</p>
<h1 id="donkeycar-ai-framework_1">DonkeyCar AI Framework</h1>
<p>Donkey AI Framework Explained</p>
<p><a href="https://ori.codes/artificial-intelligence/">[https://ori.codes/artificial-intelligence/]{.underline}</a></p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.a264c092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.4e0fa4ba.min.js"></script>
      
    
  </body>
</html>