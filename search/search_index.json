{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ECE/MAE 148: Introduction to Autonomous Vehicles","text":"<p>Documentation is under development.</p>"},{"location":"win23team1/","title":"Team 1","text":"Stoplight, Streetsign and Person Detection  MAE-ECE 148 Final Project <p> Team 1 Winter 2023 </p> Table of Contents <ol> <li> Team Members </li> <li>Final Project</li> <ul> <li>Primary goals</li> <li>Final Project Documentation</li> </ul> <li>Early Quarter</li> <ul> <li>Mechanical Design</li> <li>Electronic Hardware</li> <li>Autonomous Laps</li> </ul> <li>Acknowledgments</li> <li>Contact</li> </ol>"},{"location":"win23team1/#team-members","title":"Team Members","text":"<p>Arturo Amaya (Left), Arjun Naageshwaran (Middle), Hariz Megat Zariman (Right)</p> Team Member Major and Class  <ul> <li>Arturo Amaya - Computer Engineering (EC26) - Class of 2023</li> <li>Arjun Naageshwaran - MAE Ctrls &amp; Robotics (MC34) - Class of 2024</li> <li>Hariz Megat Zariman - Computer Engineering (EC26) - Class of 2024</li> </ul>"},{"location":"win23team1/#final-project","title":"Final Project","text":"<p>The goal of this project was a three-fold application of computer vision. Using the OAKD Lite camera, we aimed to recognize stoplights, traffic signs and people and use those visual inputs to change the state of our car.</p>"},{"location":"win23team1/#primary-goals","title":"Primary Goals:","text":"<p>1) Red, yellow and green stoplights would make the car stop, slow down, and go 2) (Stretch goal) Stop signs, speed limit signs, and right turn signs would be recognized and make the car perform their designated function 3) Individuals could be recognized and followed by the car with dynamic throttle and steering, while the car simultaneously recognized and performed stoplight and streetsign functions</p>"},{"location":"win23team1/#goal-red-yellow-and-green-stoplights","title":"Goal : Red, Yellow and Green stoplights","text":"<p>The car was able to detect red and green images succesfully. When shown red, the car completely stopped, and would only move forward when a green light was shown. Due to time constraints and inconsistent lighting conditions, the car was not able to detect a yellow light consistently but would be able to reduce throttle on detection.</p>"},{"location":"win23team1/#goal-person-following","title":"Goal : Person Following","text":"<p>The car was able to identify person objects using the OAKD camera. By doing so, it was then capable of following a person by adjusting steering values based on how far the person strayed from the center of the camera's FOV. Furthermore, the car adjusted its throttle such that the further away a person is, the faster it goes and stops when approaching to near.</p>"},{"location":"win23team1/#stretch-goal-street-sign-detection","title":"Stretch Goal: Street Sign Detection","text":"<p>Our team was able to obtain obtain the Mapillary Dataset (containing many real-world street signs) and extract the relevant files and labels which were useful to our project (such as the U-turn and stop sign). Unfortunately, due to time constraints, unlabeled images and issues with training the dataset (slow locally, incorrect format for GPU cluster and many others), we were unable to reach this goal on time. However, we were able to implement the movement methods for the car object if it were to identify these signs,</p> <p>See <code>car.py</code> for these methods.</p>"},{"location":"win23team1/#final-project-documentation","title":"Final Project Documentation","text":"<ul> <li>Final Project Proposal</li> <li>Final Project Update 3/7</li> <li>Final Project Update 3/14</li> <li>Final Project Presentation</li> </ul>"},{"location":"win23team1/#early-quarter","title":"Early Quarter","text":""},{"location":"win23team1/#mechanical-design","title":"Mechanical Design","text":"<p>Prefabricated parts of the car were given to us, but parts like the base plate, dual camera mount, LIDAR stand, and cases for sensitive electronics (VESC, GNSS, Servo PWM, etc.) were either 3D printer or laser cut. These are a few of the models compared to their real-life, 3D printed counterparts.</p> Top Camera Mount Bottom Camera Mount LIDAR Mount Camera Mount (Physical) LIDAR Mount (Physical)"},{"location":"win23team1/#electronic-hardware","title":"Electronic Hardware","text":"<p>Our team used only the electronic components given to us. In particular, we focused primarily on the OAK-D camera, Jetson NANO and the GNSS board (only used for the 3 GPS Autonomous Laps). When assembling the circuit, we used the following circuit diagram (given by the TAs of the class):</p>"},{"location":"win23team1/#autonomous-laps","title":"Autonomous Laps","text":"<p>Below is a youtube playlist of the car completing 3 autonomous laps using the DonkeyCar framework under different conditions. </p> <p></p>"},{"location":"win23team1/#acknowledgments","title":"Acknowledgments","text":"<p>Credited Code Examples: * Traffic light example Git * DepthAI Git * VESC Object Drive Folder * DonkeyCar Framework</p> <p>Special thanks to Professor Jack Silberman, and TAs Kishore Nukala and Moises Lopez (WI23), and to all our amazing classmates of Winter 2023</p>"},{"location":"win23team1/#contact","title":"Contact","text":"<ul> <li>Hariz Megat Zariman - hzariman@gmail.com | mqmegatz@ucsd.edu</li> <li>Arjun Naageshwaran - arjnaagesh@gmail.com | anaagesh@ucsd.edu</li> <li>Arturo Amaya - a1amaya@ucsd.edu | aramaya@ucsd.edu</li> </ul>"},{"location":"win23team2/","title":"Team 2","text":""},{"location":"win23team2/#team-2-ecemae-148-final-report","title":"Team 2 ECE/MAE 148 Final Report","text":""},{"location":"win23team2/#wave-the-team-2-fast-2-furious","title":":wave: The Team: 2 Fast 2 Furious","text":"<p>(Left to Right) - Elias Fang (CSE) - Ainesh Arumugam (ECE) - Matthew Merioles (ECE) - Junhao \"Michael\" Chen (MAE)</p>"},{"location":"win23team2/#project-overview","title":"\ud83d\udcdd Project Overview","text":"<p>Mario Kart in real-life? That's basically what we did. We designed a boost system similar to those in Mario Kart, where detecting colored pieces of paper on the track can change throttle for a short period of time. Depending on the color of the \"pad\" the car drives over, it will either speed up, slow down, or stop for a few seconds, just like in the game!</p> <p></p>"},{"location":"win23team2/#our-robot","title":"\ud83c\udfce Our Robot","text":""},{"location":"win23team2/#birds-eye","title":"Bird's Eye","text":""},{"location":"win23team2/#front","title":"Front","text":""},{"location":"win23team2/#left","title":"Left","text":""},{"location":"win23team2/#right","title":"Right","text":""},{"location":"win23team2/#back","title":"Back","text":""},{"location":"win23team2/#schematic","title":"Schematic","text":""},{"location":"win23team2/#final-project","title":"\ud83c\udf44 Final Project","text":""},{"location":"win23team2/#what-we-promised","title":"What We Promised","text":""},{"location":"win23team2/#must-haves","title":"Must haves","text":"<p>[X] Distinguishing different colors through the camera</p> <p>[X] Adjust the throttle based on the color</p>"},{"location":"win23team2/#nice-to-haves","title":"Nice to haves","text":"<p>[X] Have the car detect a flat piece of paper on the track (like a booster pad)</p> <p>[ ] Combine with lane-following algorithm</p>"},{"location":"win23team2/#gantt-chart","title":"Gantt Chart","text":"<p> https://sharing.clickup.com/9010060626/g/h/8cgn7aj-87/769d44f22562beb</p>"},{"location":"win23team2/#what-we-accomplished","title":"What We Accomplished","text":""},{"location":"win23team2/#color-detection","title":"Color Detection","text":"<ul> <li>Used OpenCV for color detection and edge tracing</li> <li>Used color mask algorithm to detect proportion of frame that color takes up</li> <li>Detected multiple colors at the same time</li> <li>Determined HSVs for orange, pink, and blue</li> </ul> <p>Demo</p>"},{"location":"win23team2/#pyvesc","title":"PyVESC","text":"<ul> <li>Connection through external webcam</li> <li>Different RPM values are sent through PyVesc to achieve different speed for different colors marked by different states:</li> <li>Blue (Boost) = speed up for 3 sec</li> <li>Pink (Slow)= slow down for 3 sec</li> <li>Orange (Stop) = stop for 3 sec</li> <li>Neutral (Normal) = constant rpm</li> </ul> <p>Blue Demo</p> <p>Pink Demo</p> <p>Orange Demo</p>"},{"location":"win23team2/#presentation","title":"Presentation","text":"<p>https://docs.google.com/presentation/d/1oJPRLYIKvHUXEIK9hoYpPFoFAyHuG6sE7ZrU9NQPG8g/edit?usp=sharing</p>"},{"location":"win23team2/#code","title":"Code","text":"<p>https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart.py</p>"},{"location":"win23team2/#possible-future-work","title":"Possible Future Work","text":"<ul> <li>Change the colored paper into Mario Kart items (mushroom, bananas, etc.) for the car to identify</li> <li>Allow the car to run autonomously on a track and still apply speed changes</li> <li>Race with other teams \ud83d\ude09</li> </ul>"},{"location":"win23team2/#autonomous-laps","title":"\ud83c\udfc1 Autonomous Laps","text":"<p>DonkeyCar</p> <p>ROS2 Line Following</p> <p>ROS2 Lanes</p> <p>GNSS</p>"},{"location":"win23team2/#additional-work-done","title":"Additional Work Done","text":"<ul> <li>Our team was tasked with implementing a depth feature with our contour function, using depthAI to integrate with the OAK-D Camera!</li> <li>Our updated code is now able to measure how far the contoured object is, measured in cm!</li> </ul> <p>[Depth Demo] https://www.youtube.com/watch?v=NYIz7--TpgY [Updated Code] https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart_depth.py</p>"},{"location":"win23team2/#acknowledgements","title":"Acknowledgements","text":"<p>Thanks for Professor Jack Silberman, TA Kishore Nukala, and TA Moises Lopez!</p>"},{"location":"win23team3/","title":"Final Report for Team 3 (ECE/MAE 148)","text":""},{"location":"win23team3/#team-members","title":"Team Members","text":"<ul> <li>Nathaniel Barnaby - ECE</li> <li>Yang-Jie Qin - ECE</li> <li>Cheuk Hin Bryan Cheng - MAE</li> <li>Patrick Nguyen - MAE</li> </ul>"},{"location":"win23team3/#project-overview","title":"Project Overview","text":"<ul> <li>Our initial final project was a combination of the usage of an IMU and GNSS to implement position tracking. An IMU is an inertial measurement unit, composing of gyros, accelerometers, and more sensors to track the movement of something. With these sensors, the car's location can be estimated off of a general initial GPS location with the addition to its movement measured by its speed, acceleration, turning, etc. This ended up being too complex for our team which resulted in little progress. We were then assigned new mini tasks which consists of using 2 of the sensors provided in our kits. The assignment was to use the OAK-D camera and the lidar separately to measure depth of both a small (0.15m) object and a larger (0.5m) object at different distances. We ended up comparing results of both objects at distances of 0.5, 1, 2, 3, and 4 meters. We would then compare the outputed values from the sensors to what the actual correspond measurment. A comparison between the accuracy of depth finding between the Oak-D camera and lidar would also be necesasry. A second task was assigned which was to output a face recognition system out of the OAK-D camera. </li> </ul>"},{"location":"win23team3/#results","title":"Results","text":"<ul> <li>For the distance measurement assignment, both the camera and lidar were able to successfully measure distance for the small the large object at the different ranges. </li> <li>For the camera, it was accurate at determining smaller distances, but at larger distances (3+ meters) error seemed to begin growing exponentionally. The difference between small and large objects was negligible as long as the area in which the distances were averaged fit within the object, which at further distances can start causing fluctuations with smaller objects. </li> <li>For the lidar, it was accurate at determining all distances with a linear or almost static amount of error. At smaller distances this was larger than the error of the camera, but at larger distances it vastly outperformed the camera due to the nature of the camera's exponential error. Additionally, the lidar had to be hand-calibrated, so with more time the error could have been lowered due to this effect. Also, since we recorded distance measurements within a range of 0.4 degrees, the measurement would be inaccurate with a smaller object at longer distances. This could be overcame by decreasing the range and waiting longer.</li> <li> <p>For most scenarios the lidar seems to be the winning choice for distance measurement. While at lower distances the camera seemed to outperform the lidar, the lidar seems to be more consistent with its measurements than the camera. Additionally, the lidar offers 360 degrees of distance measurements while the camera only works in one direction.</p> </li> <li> <p>DepthAI Distance Measurement with ~0.5 m^2 Object Video Link</p> </li> <li> <p>DepthAI Distance Measurement with 0.15 m^2 Box Video Link</p> </li> <li> <p>LiDAR Distance Measurement with ~0.5 m^2 Object Video Link</p> </li> <li> <p>LiDAR Distance Measurement with 0.15 m^2 Box Video Link</p> </li> <li> <p>As for the camera face recognition, we were succesfully able to output video display which recognizes faces. This is a relatively fast responding system. It outputs the number of faces it recognizes which we tested from 0-3 faces real time. There is some error within the system as it can be innacurate thinking other shiny objects and or parts of a face are another face. It is also not limited to stationary faces as it recognizes people moving too. </p> </li> <li>Face Recognition Video Link</li> </ul>"},{"location":"win23team3/#gantt-chart","title":"Gantt Chart","text":""},{"location":"win23team3/#hardware-mechanical-design","title":"Hardware: Mechanical Design","text":"<p>\\ Camera/flashlight Mount</p> <p>\\ Electronics Tray</p> <p>\\ Front/rear Electronics Plate Offset</p> <p>\\ GPS Mount</p> <p>\\ IMU Mount</p> <p>\\ Jetson Case Key Mount</p> <p>\\ Jetson Nano Main Case</p> <p>\\ Lidar Tower</p> <p>\\ Servo Voltage Converter</p> <p>\\ Vesc Power Distributor</p>"},{"location":"win23team3/#previous-designs","title":"Previous Designs","text":""},{"location":"win23team3/#electronic-components","title":"Electronic Components","text":"<p>\\ Jetson Nano</p> <p>\\ OAK-D Camera</p> <p>\\ Lidar LD06</p>"},{"location":"win23team3/#electronic-wiring-schematic","title":"Electronic Wiring Schematic","text":""},{"location":"win23team3/#final-set-up","title":"Final Set Up","text":"<p> Bird's Eye View</p> <p> Left View</p> <p> Right View</p>"},{"location":"win23team3/#packages-and-drivers","title":"Packages and Drivers","text":"<ul> <li>cv2 (OpenCV)</li> <li>depthai (DepthAI)</li> <li>numpy</li> <li>math</li> <li>binascii</li> </ul>"},{"location":"win23team3/#milestones","title":"Milestones","text":"<ul> <li>Face Recognition using DepthAI - Detects faces through a webcam and displays a count in the terminal</li> <li>Distance Measurement using DepthAI - Using the disparity between the left and right cameras of the OAKD, distance can be calculated. This was averaged over an area to give an estimated distance of an object.</li> <li>Distance Measurement using LiDAR - Using a LiDAR is is relatively simple to detect distances in a 360 degree range. By averaging distances over a very small range (0.4 degrees) we determined the distance of an object.</li> </ul>"},{"location":"win23team3/#potential-future-workunaccomplished-goals","title":"Potential Future Work/Unaccomplished Goals","text":"<ul> <li>Recognizing and labeling specific faces</li> <li>Running code off of the OAK-D Camera instead of needing an external computer to run the code.</li> </ul>"},{"location":"win23team3/#presentations","title":"Presentations","text":"<p>-Final Project Proposal -Final Presentation</p>"},{"location":"win23team3/#acknowledgments","title":"Acknowledgments","text":"<p>Professor Jack Silberman, TA Kishore Nukala, Moises Lopez-Mendoza, Design and Innovation Building, all of our wonderful classmates</p>"},{"location":"win23team4/","title":"Team 4 Final Project Report","text":""},{"location":"win23team4/#members-vasanth-senthil-ece-eddy-rodas-limamae-lingpeng-mengece","title":"Members: Vasanth Senthil (ECE), Eddy Rodas Lima(MAE), Lingpeng Meng(ECE)","text":""},{"location":"win23team4/#physical-setup","title":"Physical Setup","text":""},{"location":"win23team4/#initial-goals","title":"Initial Goals","text":""},{"location":"win23team4/#objective","title":"Objective","text":"<ul> <li>Make our RoboCar follow sound using sound localization from multiple microphones.</li> </ul>"},{"location":"win23team4/#must-haves","title":"Must Haves","text":"<ul> <li>Car is able to determine approximate direction of an audio source</li> <li>Car moves in towards audio source once direction of audio is determined</li> </ul>"},{"location":"win23team4/#nice-to-haves","title":"Nice to Haves","text":"<ul> <li>Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume</li> <li>Accurate movement towards source</li> </ul>"},{"location":"win23team4/#accomplishments","title":"Accomplishments","text":"<ul> <li>Researched and experimented with different methods of sound localization, such as using time of arrival delay between microphones</li> <li>Used the pyaudio library to collect streams of microphone data from each of the three microphone, and processed this data in order to tell direction real-time based on sound level</li> <li>Used the pyvesc library to create a custom movement file that allows us to easily move the RoboCar based on provided angles and throttle until specified conditions are met</li> <li>The microphone processing file called upon the movement functions after determining current direction.</li> <li>We also spread the left and right microphones to minimize the noise created by the car\u2019s movement, which would influence the accuracy of direction calculations.</li> </ul>"},{"location":"win23team4/#demo-videos","title":"Demo Videos","text":"<ul> <li>Static Source Left</li> <li>Static Source Right</li> <li>Moving Source Front</li> <li>Moving Source Back</li> <li>Moving Source Further Away</li> </ul>"},{"location":"win23team4/#issues-faced","title":"Issues Faced","text":"Issue Solution Original idea of using time delay between microphones was not possible due to jetson limitations Decided to use volume of sound signals instead of microphone timing in order to determine direction of audio. Sound from car\u2019s movement, such as servo motors and wheels, were causing our robot to turn randomly. Created wings out of cardboard which lifted and spread out the left/right microphones away from the wheels. Random/sudden loud noises from the environment made the car turn in wrong directions, even when originally was moving correctly towards audio source. Averaged the audio signals of each microphone over the last few ticks using a buffer in order to reduce impact of outlier noises. Required tuning, as a small buffer would not remove outliers as effectively while a large buffer reduced reaction time of robot\u2019s movement when direction of audio actually changed"},{"location":"win23team4/#what-did-not-work","title":"What did not work","text":"<ul> <li>Original plan was to use the time delay between when microphones head a noise over a sound threshold in order to calculate angle of sound. Problem with that is the Jetson Nano is running Linux, and due to background processes and overhead, it is not precise enough to identify the timing needed for this method to work when the microphones are this close together. This may have been possible with a real-time system such as an Arduino</li> <li>Another method was using real-time localized sound by performing beamforming using microphones in array, but we were not certain if we had enough time to implement and debug this method in time (received the microphones on Saturday) as it was significantly more complex than the other options. </li> </ul>"},{"location":"win23team4/#next-steps-if-we-had-more-time","title":"Next Steps (If we had more time)","text":"<ul> <li>Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume, would help filter interference with background noises.</li> <li>Function based on sound difference between microphones to get precise steering angles. Current only steers forward, full left, full right.</li> <li>More accurate movement with our backwards direction</li> <li>Minimize unwanted noise coming from either surroundings or the vehicle.</li> </ul>"},{"location":"win23team5/","title":"Final Project Repository for Team 5 of the 2023 Winter Class MAE ECE 148 at UCSD","text":"<p>Our Final Project has one main objective, which is inspired by a pet dog that plays fetch. Our goal is to design a robot that can identify a green ball, like a tennis ball, locate it, move towards it, pick it up, and return back to its initial location. We achieved this using an OpenCV-based vision system to recognize the ball and Pyvesc to control the car's movements. We also designed a claw mechanism to pick up the ball when it's within range and a servo to move the ball into the claw.</p> <p>In summary, our project demonstrates the capabilities of an autonomous robot that can navigate an environment, recognize objects, and perform tasks like fetching. With further improvements, this type of robot could have many potential applications in various industries.</p> <p> </p>"},{"location":"win23team6/","title":"MAE/ECE 148 Winter 2023 at UCSD","text":""},{"location":"win23team6/#team-6","title":"TEAM 6","text":"<p>Our project uses the OAK-D camera, a roboflow YOLO model, PyVESC module, and an Arduino-powered camera mount to get our car to scan its surroundings until it finds a basketball and drive until it is within about 0.5 m of the ball.</p>"},{"location":"win23team6/#car-assembly","title":"Car Assembly","text":""},{"location":"win23team6/#vehicle-body","title":"Vehicle Body","text":""},{"location":"win23team6/#camera-mount","title":"Camera Mount","text":""},{"location":"win23team6/#tech-stack","title":"Tech Stack","text":""},{"location":"win23team6/#roboflowoak-module","title":"RoboflowOak Module","text":"<p>We used roboflow to train a ball detection model and host the model. We, then, made API calls to hosted model to retrieve predictions on frame captures from the OAK-D camera. Once a ball is found, we wrote a script to calculate the angle between the center of the bounding box drawn around the detected ball and the centerline of the camera (which by default is the center of the frame).</p>"},{"location":"win23team6/#pyvesc-module","title":"PyVESC Module","text":"<p>We used the pyvesc module to set our servo's angle once a detection has been made. The steering angle is proportional to the calculated angle. Once the steering is set, we increase throttle for about half a second and then stop the motor to make another detection. We then loop over these steps until the ball is within 0.5 m of the frame. Our stopping condition was for the width of the bounding box of the ball to be a certain ratio of the total frame width. The ratio is hardcoded based on fine-tuning to get the car to stop at about 0.5 m.</p>"},{"location":"win23team6/#arduino-board","title":"Arduino Board","text":"<p>We used an arduino nano to control the camera mount. The camera mount can rotate horizontally (yaw-equivalent) within a range of 180 degrees. And it can move up and down (pitch-equivalent) within a range of 90 degrees. This is used to move the camera around so it can scan the surroundings for a ball (in case the ball is not in frame). </p>"},{"location":"win23team6/#motorized-camera-mount","title":"Motorized Camera Mount","text":"<p>We designed a camera mount that is actuated by 2 servos, one controls the camera's pitch angle and the other controls the camera's yaw angle. The mount elevates the camera 5 inches amove the vehicle's mounting plate. It allows the camera to turn and scan for the target ball. </p> <p>https://user-images.githubusercontent.com/58583277/227646168-1071f237-de95-4f92-ad65-a90ee5fe2b01.mp4</p>"},{"location":"win23team6/#how-to-run-the-code","title":"How To Run The Code","text":"<p>One can run python run.py from inside the Jetson Nano mounted on their car. This will load the model and also detect the motor and begin the purported task of finding a basketball. If no basketball is found, it will remain stationary. The 148remote/ folder contains a cool arduino program for proof of concept. It moves the camera mount through its full range of motion in a rhythmic fashion. </p>"},{"location":"win23team6/#vehicle-in-action","title":"Vehicle In Action","text":"<p>Click Here For Video</p>"},{"location":"win23team6/#future-improvements","title":"Future Improvements","text":"<p>We can use the Jetson to control the servo motors through the Arduino. We can account for the yaw angle of the camera mount and add that to the steering, so that the car can steer toward targets that is not in the field of view of the camera when the camera is pointing straight forward. We can also improve the precision of the ball-recognition model by using more pictures of the ball to train the model. </p>"},{"location":"win23team6/#team-6-wall-e-used-to-be-cyclops","title":"Team 6: Wall-E (used to be Cyclops)","text":"<ul> <li>Saathvik Dirisala (Data Science)</li> <li>Victor Chen (Computer Engineering)</li> <li>Yang Song (Mechanical Engineering) </li> </ul>"},{"location":"win23team7/","title":"Team 7","text":""},{"location":"win23team7/#ucsd-ecemae-148-2023-winter-team-7","title":"UCSD ECE/MAE-148 2023 Winter Team 7","text":""},{"location":"win23team7/#team-members","title":"Team Members","text":"<p>Francisco Downey (BENG), Jonathan Xiong (ECE), Nicholas Preston (MAE), Karthik Srinivasan (MAE)</p>"},{"location":"win23team7/#final-project-overview","title":"Final Project Overview","text":"<p>For our final project, we made our car drive from point A to B, given starting and ending GPS points. </p>"},{"location":"win23team7/#assembled-car-design","title":"Assembled Car Design","text":"<p> LIDAR was set in the front of the car. This was an easy decision for the team since we cared for varying obstacles crossing the points that comprise the path from Point A to Point B. Essentially, it was only important to care for obstacles that come into the front of the moving car. GNSS was secure near the center of the car with the antenna placed high and toward the rear.</p>"},{"location":"win23team7/#donkeycar-3-autonomous-laps","title":"DonkeyCar - 3 Autonomous Laps","text":"<p>https://user-images.githubusercontent.com/103704890/227807530-35ed2ea5-2bc0-4b8b-983d-ca6bed659390.mp4</p>"},{"location":"win23team7/#ros2-line-following-3-autonomous-laps","title":"ROS2 Line Following - 3 Autonomous Laps","text":"<p>https://user-images.githubusercontent.com/103704890/227801950-dfd60ddd-300e-4af8-9878-c62338184269.mp4</p>"},{"location":"win23team7/#ros2-left-lane-3-autonomous-laps","title":"ROS2 Left Lane - 3 Autonomous Laps","text":"<p>https://user-images.githubusercontent.com/103704890/227804039-c1aafb7a-8bf6-4f86-89cf-1fa21c7ea5d2.mp4</p>"},{"location":"win23team7/#gps-3-autonomous-laps","title":"GPS - 3 Autonomous Laps","text":"<p>https://user-images.githubusercontent.com/103704890/227804055-d9c3ccce-ef8d-427d-a6b6-e1db12eba440.mp4 Important to note for the final project, many of the configurations that were found to work in the GPS 3 autonomous laps were used for the final project. The GPS points making up the path were 0.55 meters apart. The nature of the csv points used in the final project matched the one used for this assignment. This allowed the project to move faster as the configurations did work out. See section Algorithmic Design of Directing Car from GPS point A to B.</p>"},{"location":"win23team7/#final-project","title":"Final Project","text":"<p>Plan: Go from point A to B with object detection. </p> <p>Overview Originally, we were going to have LIDAR detect objects in front of the car so the car can see what it needs to go around. However, we did not have enough time to see how to use LIDAR, since there was no assignment with it and not enough time at end of quarter given rain. We, however, got quite experienced with the GPS functionality of the car. Using the GPS modules, we were able to direct the car using a path of car-readble GPS coordinates.</p> <p>Algorithmic Design of Directing Car from GPS point A to B By the end of the quarter, we wrote a program, AtoB.py, which requires two sets of GPS coordinates as inputs. This program generates a .csv file containing a path of GPS coordinates about .55 meters apart from point A to B. These generated points latitude longitude formatted and relative to the base station antenna. The inputted absolute (planet's) GPS coordinates, thus, needed to be translated to these relative coordinates and converted to rectangular from polar positions. It took some testing to increase the precision of the translation as we did not know the exact conversion constants.</p> <p>Demonstration</p> <p>https://user-images.githubusercontent.com/103704890/227807009-ec59e649-c068-4543-9dba-9ddb16ea8ee5.mp4</p>"},{"location":"win23team8/","title":"Team 8 Final Project Proposal","text":""},{"location":"win23team8/#team-members","title":"Team Members","text":"<p> - Youssef Georgy | Electrical &amp; Computer Engineering - Rizzi Galibut | Mechanical &amp; Aerospace Engineering - Shuhang Xu | Computer Science - Kavin Raj | Cognitive Science w/ Emphasis in Machine Learning</p> <p>Such a lovely team!!</p>"},{"location":"win23team8/#project-overall","title":"Project Overall","text":"<p>A waiter-bot that takes visual input from the camera, navigates autonomously to different specified locations (i.e., tables) and then back to the starting point. Use image-detection via camera to give robocar a location or GPS coordinates to navigate  to, then use GPS data to plot path there and avoid any obstacles in the way. The robocar will be able to take any location given (provided it\u2019s in range of the network connection) and determine how to get there</p>"},{"location":"win23team8/#physical-setup","title":"Physical Setup","text":""},{"location":"win23team8/#gantt-chart","title":"Gantt Chart","text":""},{"location":"win23team8/#demonstration","title":"Demonstration","text":""},{"location":"win23team8/#using-depthai-for-text-recognition","title":"Using DepthAI for text recognition","text":"<p>Accomplished: - Used DepthAI to enable the camera to detect numbers which are associated with different tables (e.g., 001, 002, etc.) and different CSV files - Originally started by looking at OpenCV and Tesseract for OCR - These are not SpatialAI platforms so accomplishing what we were trying to do would be much harder - Default code provided in DepthAI library launched windows that displayed video stream and words detected, which worked when directly connected to camera through host computer but running code through Jetson would require a container to launch these windows - Needed to find a way to disable them and have camera run in the background Default code also rewrote the variable that contained the decoded text each time it detected something - Had to rewrite and remove sections of the code to stop detection once certain prompts are given - Light conditions really mattered when showing prompt to camera - Worked better during the day and when shown prompt on a backlit-screen (with white background)</p> <p>What did't work as expected: - Camera could read \u201c001\u201d but not \u201c1\u201d, etc., so we decided to use a set of numbers instead - Number recognition is much more accurate than word recognition so we decided to use numbers as the prompt</p>"},{"location":"win23team8/#using-gps-to-record-different-paths","title":"Using GPS to record different paths","text":"<ul> <li>GPS navigation code is based on the USCD donkeycar GPS library</li> <li>We are trying to make it stay in auto-pilot mode by default and reset the origin in the beginning</li> <li>Depend on what text is detected with DepthAI, make it move following the pre-recorded path</li> <li>When it moves back to the origin, stop and start detecting next text</li> </ul>"},{"location":"win23team8/#desired-but-not-accomplished","title":"Desired but not accomplished","text":"<ul> <li>Include the ability to restart the process once the waiter-bot returned to the starting point without having to manually run the script again</li> <li>Have waiter-bot stay at the table for a certain amount of time before returning to the starting point</li> <li>Incorporate the LiDAR so the waiter-bot can avoid obstacles (e.g., students walking past) while traveling to tables</li> <li>Could also be used to detect when the waiter-bot reaches the table and ensure it doesn\u2019t crash into it</li> <li>Have controller inputs programmed in so the waiter-bot can be truly autonomous.  Currently still requires human input</li> </ul>"},{"location":"win23team9/","title":"Team 9","text":"ECE/MAE 148 Winter 2023 Team 9 <p>     JetBuddy     Indoor Delivery Bot based on DepthAI, OpenCV and LiDAR </p> Table of Contents <li>Team Members</li> <li> Hardware and Schematics <ul> <li>Parts</li> <li> Schematics</li> </ul> </li> <li>Final Project</li> <ul> <li>Abstract</li> <li>Part 1: Human Detection and Following with Depthai and PyVesc</li> <li> Part 2: Stopping Mechanism with Lidar</li> <li> Part 3: Facial Recognition</li> <li> Part 4: Spatial Detection with DepthAI</li> </ul> <li>Reflection</li> <ul> <li>Challenges</li> <li> Potential Improvements</li> </ul> <li>Presentation Files</li> <li>Reference</li>"},{"location":"win23team9/#team-members","title":"Team members","text":"<ul> <li>Ben Zhang (ECE)</li> <li>Joseph Katona (ECE) </li> <li>Yichen Yang (ECE)</li> <li>Zijian Wang (MAE)</li> </ul>"},{"location":"win23team9/#hardware","title":"Hardware","text":""},{"location":"win23team9/#parts","title":"Parts","text":""},{"location":"win23team9/#full-assembly","title":"Full Assembly","text":""},{"location":"win23team9/#mounting-plate","title":"Mounting Plate","text":""},{"location":"win23team9/#jetson-case","title":"Jetson Case","text":""},{"location":"win23team9/#camera-lidar-mount","title":"Camera LiDAR Mount","text":""},{"location":"win23team9/#final-project-delivery-box","title":"Final Project Delivery Box","text":""},{"location":"win23team9/#schematics","title":"Schematics","text":""},{"location":"win23team9/#wire-diagram","title":"Wire Diagram","text":""},{"location":"win23team9/#final-project","title":"Final Project","text":""},{"location":"win23team9/#abstract","title":"Abstract","text":"<p>This project aims to develop a delivery system for our robocar that can detect and follow humans while also incorporating a stopping mechanism to prevent collisions. Additionally, the robot will utilize facial recognition to identify individuals and personalize interactions.</p>"},{"location":"win23team9/#part-1-human-detection-and-following-with-depthai-and-pyvesc","title":"Part 1: Human Detection and Following with Depthai and PyVesc","text":"<p>The OAKD camera will be used to detect and track humans in the robot's vicinity. The PyVesc motor controllers will then be used to move the robot in the direction of the detected human.</p>"},{"location":"win23team9/#required-components","title":"Required Components","text":"<ul> <li>Tiny-Yolov3 model integrated in DepthAi for object detection</li> <li>PyVesc Python package for robocar control</li> </ul>"},{"location":"win23team9/#algorithm-workflow","title":"Algorithm Workflow","text":"<ul> <li>Use Tiny-Yolov3 to detect the bounding box of the person in the OAKD camera's field of view.</li> <li>Determine the position of the person by finding the central line of the bounding box, and denote the x-axis value as x0.</li> <li>Calculate the error between the central line of the frame (416x416 pixels), e = x - x0.</li> <li>Calculate the steering value using the formula: v = (Kp * e) / 416 + 0.5, where Kp = 1.</li> <li>Use PyVesc to control steering by calling vesc.set_servo(v).</li> </ul>"},{"location":"win23team9/#additional-settings","title":"Additional Settings","text":"<ul> <li>Use vesc.set_rpm() to run the car once it detects people.</li> <li>The steering value is sampled at a rate of 5Hz to prevent frequent drifting.</li> </ul>"},{"location":"win23team9/#part-2-stopping-mechanism-with-lidar","title":"Part 2: Stopping Mechanism with Lidar","text":"<p>The Lidar sensor will be used to detect obstacles in the robot's path. If an obstacle is detected, the robot will stop moving and wait for the obstacle to clear before continuing on its path.</p>"},{"location":"win23team9/#the-lidar-on-this-robot-aim-to","title":"The LiDAR on this robot aim to","text":"<ul> <li>Detect anything that is in a close range</li> <li>If the position is too clase, the robot will stop to avoid collision</li> <li>The robot will back up after it stop for a while and still detect obstacle is close</li> <li>Transform raw binary data from LiDAR to numerical data through BinASCII library</li> </ul>"},{"location":"win23team9/#how-to-read-lidar","title":"How to read LiDAR?","text":"<ul> <li>Each measurement data point of LiDAR is consists of a distance value of 2 bytes and a confidence of 1 byte</li> <li>We transform this data through chopping it to bytes and translate it.</li> <li> <p>We get the angle by getting the start angle and end angle.</p> </li> <li> <p>Putting all the distance into a list and it will stop the car if there\u2019s an object within certain distance that LiDAR detected.</p> </li> </ul> <p></p>"},{"location":"win23team9/#part-3-facial-recognition","title":"Part 3: Facial Recognition","text":"<p>The robot will be equipped with a facial recognition system, using a webcam, that will allow it to identify individuals and personalize interactions. Once it recognizes the right person, the delivery box will open. The facial recognition software uses a simple python import of facial_recognition. In the facial_recognition library all we do is use openCV to capture images for the frames and use facial_recognitions \"matching\" function to to add a box around the persons face. In our case when this value is detected over an interval then a true value is then sent to the box to open.</p> <p></p>"},{"location":"win23team9/#part-4-spatial-detection-with-depthai","title":"Part 4: Spatial Detection with DepthAI","text":"<p>Utilizing Depthai's pipeline system we take their spatial location pipeline to simply calculate the distance of individual from the camera. The Object detection pipeline detects a person and creates a bounded box, then with the x and y coordinates from the bounded box we can pinpoint where we want the camera to point. After these coordinates are gathered the z location is stored in a circular list. This is because the bounded box and tracker of object distance aren't always in sync so some erroneous values are given. Once we have around 50 samples then we take the average to get a good idea of what the distance of the person from the car is. Finally we utilize pyvescs set_rpm() features to give out a more smooth acceleration system. So, basically if you're far away the robot will speed up and slow down as it moves closer to you.  Get more info on Spatial Depth here </p>"},{"location":"win23team9/#gantt-chart","title":"Gantt Chart","text":""},{"location":"win23team9/#demonstrations","title":"Demonstrations","text":"<p>The Video might not show up, please go to img folder for full demo.</p> <p> </p>    Your browser does not support HTML video."},{"location":"win23team9/#reflection","title":"Reflection","text":""},{"location":"win23team9/#challenges","title":"Challenges","text":"<ul> <li>Getting everything to work together<ul> <li>Different libraries working together and all send signals to PyVESC</li> <li>Everything worked fine on a local machine but when running on the Jetson, crashes would occur</li> </ul> </li> <li>Scope of the original idea<ul> <li>Mapping the path for future references using SLAM</li> </ul> </li> <li>Depth ai pipeline caused crashes<ul> <li>X-Link Problem(Serial bus issues)</li> </ul> </li> <li>Translate raw LiDAR output to data we need</li> <li>Making the car look smooth</li> <li>Better algorithm to adjust speed(rpm)</li> </ul>"},{"location":"win23team9/#potential-improvements","title":"Potential Improvements","text":"<ul> <li>Implement all the features together flawlessly<ul> <li>Currently cannot run together good due to delay from different components</li> </ul> </li> <li>Get the locking mechanism working<ul> <li>Locking mechanism to make sure the right receiver get the package</li> </ul> </li> <li>LiDar also scans the path for future path planning<ul> <li>Trying to find a person if it cannot detect anything</li> </ul> </li> </ul> <p>Maybe try different frameworks since we can use different libraries without limitation in ROS or donkeycar</p>"},{"location":"win23team9/#presentations","title":"Presentations","text":"<li>Project Proposal &amp; Progress Report</li> <li>Final Presntation</li>"},{"location":"win23team9/#reference","title":"Reference","text":"<p>We would like to give special thanks to:</p> <ul> <li>Professor Jack Silberman</li> <li>TA Moises Lopez</li> <li>TA Kishore Nukala</li> <li>All The teams that helped us on the way</li> </ul>"},{"location":"win23team12/","title":"ECE 148 Winter 2023 Team 12 Final Project","text":""},{"location":"win23team12/#team-members","title":"Team members","text":"<ul> <li>Jake Kindley (ECE)</li> <li>Yiteng Zhao (ECE)</li> <li>Noah Jones (MAE)</li> </ul>"},{"location":"win23team12/#overview","title":"Overview","text":"<p>We want to create a sorter bot similar to warehouse bots that picks up a package, follow the path with certain color and delivers it to designated dropoff location based on the labels on that package. We plan to put AR tags as labels on the package, and each AR tag is mapped to an id associated with either the color of the lane our bot should follow or stop action when it reaches the designated dropoff zone. At dropoff zone, the bot will perform a series of maneuver and return to the starting point.</p>"},{"location":"win23team12/#robot-design-implementation","title":"Robot Design &amp; Implementation","text":""},{"location":"win23team12/#software","title":"Software","text":"<p>Our code is running on ROS2 and is modified based on the provided lane following code from class. The following flowchart shows the original relationship between each component in the provided lane following code from class: </p> <p>We added a node responsible for detecting AR tag, read states from AR tag, and changing lane detector's behavior. </p> <p>In our new node, we have defined two types of AR tags:</p> <ul> <li>Type 1: Declares the lane color our bot should follow</li> <li>Type 2: Stop signal that indicates the bot to drop the package</li> </ul> <p>The default behavior when no AR tag is being detected is following blue lane until a type 1 tag is detected.</p> <p></p>"},{"location":"win23team12/#hardware","title":"Hardware","text":"<p>We designed a bracket as package holder that snaps on the front bumper of the car and holds one package:</p> <p></p> <p></p>"},{"location":"win23team12/#showcase","title":"Showcase","text":""},{"location":"win23team12/#assembled-robots","title":"Assembled Robots","text":""},{"location":"win23team12/#final-project-demo","title":"Final Project Demo","text":""},{"location":"win23team12/#remarks","title":"Remarks","text":"<p>Our video demonstrated our bot's capability of detecting AR tag, selecting the corresponding lane color, following path to dropoff zone, dropping off package in front of stop sign, and return to the starting point. Potential improvements for this project includes adding obstacle avoidance, redesigning the package holder to hold the package above ground, adjusting camera position for better view angle of the lane, and adding capability to navigate through multiple junctions.</p>"},{"location":"win23team13/","title":"Team 13","text":""},{"location":"win23team13/#final-project-robot-mapping","title":"Final Project: Robot Mapping","text":""},{"location":"win23team13/#team-13-girish-muhammad-andy-and-van","title":"Team 13: Girish, Muhammad, Andy, and Van","text":""},{"location":"win23team13/#ece-mae-148-winter-2023","title":"ECE MAE 148, Winter 2023","text":"<p>Welcome to the project report for Team 13! This page contains a report of all the progress we made throughout this busy and fun quarter, including our final project.</p> <p>Team 13's assembled RC Car with the lidar placed at the front.</p>"},{"location":"win23team13/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Final Project: Robot Mapping         - Team 13: Girish, Muhammad, Andy, and Van         - ECE MAE 148, Winter 2023</li> <li>Table of Contents</li> <li>The Team</li> <li>Final Project Abstract</li> <li>Hardware Setup<ul> <li>Base Plate</li> <li>Camera Mount</li> <li>Jetson Nano Case</li> <li>Electronics Circuit Diagram</li> </ul> </li> <li>Software Documentation</li> <li>Autonomous Laps</li> <li>Acknowledgements</li> <li>Credit and References</li> </ul>"},{"location":"win23team13/#the-team","title":"The Team","text":"Girish Krishnan [LinkedIn] Muhammad Bintang Gemilang Andy Zhang Zhengyu (Van) Huang Electrical Engineering Mechanical Engineering Electrical Engineering Computer Engineering"},{"location":"win23team13/#final-project-abstract","title":"Final Project Abstract","text":"<p>Our final project was themed around mapping an unknown environment. Our project involved the following tasks.</p> <p>What we promised</p> <ul> <li>[\u2714] To implement SLAM (Simultaneous Localization and Mapping) using a lidar. This effectively creates a map of the environment around the robot, showing the locations of all objects present.</li> <li>[\u2714] To display the map generated from SLAM in real-time using a web application.</li> </ul> <p>The challenges faced during the project were:</p> <ul> <li>Integrating the web application for live previewing (HTML/CSS/JS) with the Python code needed to run SLAM.</li> <li>Avoiding delays in the updating map.</li> </ul> <p>The accomplishments of the project were:</p> <ul> <li>We were able to achieve a decent visualization that updates over time as the robot is driven around</li> <li>The visualization/map can be saved easily for tasks such as path planning.</li> </ul> <p>Final Presentation</p> <ul> <li> <p>Link to Final Presentation</p> </li> <li> <p>Link to video showing real-time mapping</p> </li> </ul> <p>Weekly Update Presentations</p> <ul> <li>Project Proposal</li> <li>Week 8</li> <li>Week 9</li> <li>Week 10</li> </ul> <p>Gantt Chart</p> <p></p>"},{"location":"win23team13/#hardware-setup","title":"Hardware Setup","text":"<ul> <li>3D Printing: Camera Mount, Jetson Nano Case, GPS (GNSS) Case.</li> <li>Laser Cutting: Base plate to mount electronics and other components.</li> </ul> <p>Parts List</p> <ul> <li>Traxxas Chassis with steering servo and sensored brushless DC motor</li> <li>Jetson Nano</li> <li>WiFi adapter</li> <li>64 GB Micro SD Card</li> <li>Adapter/reader for Micro SD Card</li> <li>Logitech F710 controller</li> <li>OAK-D Lite Camera</li> <li>LD06 Lidar</li> <li>VESC</li> <li>Anti-spark switch with power switch</li> <li>DC-DC Converter</li> <li>4-cell LiPo battery</li> <li>Battery voltage checker/alarm</li> <li>DC Barrel Connector</li> <li>XT60, XT30, MR60 connectors</li> </ul> <p>Additional Parts used for testing/debugging</p> <ul> <li>Car stand</li> <li>USB-C to USB-A cable</li> <li>Micro USB to USB cable</li> <li>5V, 4A power supply for Jetson Nano</li> </ul>"},{"location":"win23team13/#base-plate","title":"Base Plate","text":"<p>All measurements shown above are in millimeters (mm)</p> <p>Our base plate was laser cut on a thick acrylic sheet. The circular hole at the end of the base plate is meant to hold the power on/off button. The long holes in the side of the plate are meant for wires to easily pass to and from the bottom of the plate.</p>"},{"location":"win23team13/#camera-mount","title":"Camera Mount","text":"Camera Holder Base for attachment to base plate <p>The two parts of the camera mount shown above were screwed together. The angle of the camera was carefully chosen (facing downward approximately 10 degrees from the vertical) so that the road ahead is clearly visible. This is essential for accurate results in OpenCV/ROS2 autonomous laps.</p> <p>One of our older camera mount designs is shown below.</p> <p>This camera mount consists of three parts: one base for attachment to the base plate, one middle piece to connect the base and the camera, and the camera holder. This camera mount design allows you to rotate the camera up and down. However, it is important that the rotating hinge is screwed securely so that the hinge doesn't wobble out while the robot does autonomous laps!</p>"},{"location":"win23team13/#jetson-nano-case","title":"Jetson Nano Case","text":"<p>Credit to flyattack from Thingiverse, see: https://www.thingiverse.com/thing:3532828</p> <p>This case is excellent because it is robust and doesn't break easily, unlike most common Jetson Nano cases.</p>"},{"location":"win23team13/#electronics-circuit-diagram","title":"Electronics Circuit Diagram","text":"<p>Note: some of these components and connections will vary depending on the exact components you have - check the component specifications carefully.</p>"},{"location":"win23team13/#software-documentation","title":"Software Documentation","text":"<p>To install all the necessary Python modules needed, run the following on the Jetson Nano.</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>For our final project, we implemented a real-time visualization system for the Hector SLAM algorithm implemented using the lidar sensor. The base code for the SLAM algorithm is accessible in the Docker container provided to us in class, and the code for the real-time implementation is present in the slam_gui folder of this repository.</p> <p>The SLAM real-time visualization GUI that we built has the following features:</p> <ul> <li>A web application whose routes are made using FastAPI in Python. Uvicorn is used to run the web server.</li> <li>HTML and JS to update the map in real-time</li> <li>The HTML and JS is interfaced with Python, ROS1, ROS2 and ROSBridge, so that the data collected is displayed on the web app.</li> <li>The interfacing process is difficult to implement directly in Python, so we use subprocessing to call relevant bash scripts that handle the processes in ROS1 and ROS2. These subprocesses are made to run in parallel using threading in Python.</li> </ul> <p>To run the visualizer, first open up a docker container containing the ucsd_robocar ROS packages.</p> <p>Run the following:</p> <pre><code>cd slam_gui\npython slam_map.py\n</code></pre> <p>This sets up the web app running on the Jetson Nano (although the app could potentially be run on any device, provided it can communicate with the Jetson Nano using the relevant ROS topics).</p> <p>Opening up the web app on http://localhost:8000 reveals the GUI showing the results of SLAM in real-time. The code in slam_map.py can be adjusted to fine-tune the time-delay that occurs as the map updates.</p> <p></p> <p>Additional Scope for the Final Project</p> <p>Although SLAM is useful for mapping an unknown environment, it can be useful to integrate GPS data with SLAM to provide better location accuracy. To implement this in Python, we created the folder gps_slam that contains starter code with lidar, PyVESC, and GPS implementation and a basic SLAM algorithm with the Kalman filter (implemented using the filterpy library in Python). This additional, nice-to-have part of the project hasn't been tested out yet, but we plan to get it working soon.</p>"},{"location":"win23team13/#autonomous-laps","title":"Autonomous Laps","text":"<p>As part of the class deliverables and as preparation for the final project, here are our autonomous laps videos:</p> <p>Donkey Sim</p> <ul> <li>Local Computer: https://youtu.be/lXEStSEVikQ</li> <li>GPU training: https://youtu.be/4_BzKP9-XAQ</li> <li>External Server: https://youtu.be/Yvo1yqRJhX4</li> </ul> <p>Physical Robot</p> <ul> <li>DonkeyCar: https://youtu.be/bPUSS2g0Ves</li> <li>Lane detection using OpenCV + ROS2: https://youtu.be/omcDCBSrl2I</li> <li>Inner lane: https://youtu.be/9hN8HUlGcas</li> <li>Outer lane: https://youtu.be/nXZNPscVlX0</li> <li>GPS: https://youtu.be/Y3I9AWW1R6o</li> </ul>"},{"location":"win23team13/#acknowledgements","title":"Acknowledgements","text":"<p>Thanks Prof. Jack Silberman and TAs Moises Lopez and Kishore Nukala for an awesome quarter! See you in DSC 178 next quarter, professor Jack ;)</p>"},{"location":"win23team13/#credit-and-references","title":"Credit and References","text":"<ul> <li> <p>Jetson Nano Case Design: https://www.thingiverse.com/thing:3532828</p> </li> <li> <p>Lidar (LD06) Python Tutorial: https://github.com/henjin0/LIDAR_LD06_python_loder</p> </li> <li>PyVESC: https://github.com/LiamBindle/PyVESC</li> <li>SLAM tutorial, Dominic Nightingale. https://gitlab.com/ucsd_robocar/ucsd_robocar_nav1_pkg/-/tree/master/</li> </ul>"},{"location":"win23team14/","title":"Team 14","text":"<p>ECE - MAE 148 Team 14 Winter 2023 Repository</p> <p>Project Goal: </p> <p>We used the OAK-D camera to run object detection (within the camera) and track different traffic details (common signs, speed limits, etc.)</p> <p>Software and Hardware Description:</p> <p>The OAK-D and depthAI are AI-enabled stereo cameras that allow for depth perception and 3D mapping. They are powerful tools for computer vision applications, such as object detection and tracking. The depthAI is a board that enables faster processing of images by offloading the computational workload from the main processor to dedicated hardware.  YOLO (You Only Look Once) is a real-time object detection system that is capable of detecting objects in an image or video feed. It is a popular deep learning model that is used in many computer vision applications, including self-driving cars and robotics. PyVesc is a Python library for controlling VESC-based motor controllers. The VESC is an open-source ESC (Electronic Speed Controller) that is widely used in DIY robotics projects. PyVesc allows for easy communication with the VESC and provides an interface for setting motor parameters and reading sensor data.</p> <p>Project Overview:</p> <p>We first used the OAK-D and depthAI to detect stop signs in the robot's field of view. Then, we executed the deep learning model YOLO to process the camera feed and identify the stop sign(text detection can be another method to achieve the same function). Once the stop sign is detected, we implemented PyVesc to send a command to the motor controller to stop the robot and started to set up the OAK-D and depthAI cameras by installing the necessary software libraries. YOLO is capable of detecting multiple objects simultaneously, so we needed to filter out the stop sign from other detected objects. However, we needed a blob converter to take different data types and convert them into a Binary Large Object (BLOB) that could fit in our code. Finally, once the stop sign is detected, we accessed PyVesc to send a command to the motor controller to stop the robot. In summary, the integration of OAK-D, depthAI, YOLO, and PyVesc allows for efficient and accurate stop sign detection and safe stopping of the robot. This implementation can be further customized and optimized for specific robotic platforms and use cases.</p> <p>Final Projet Presentation: </p> <p>https://docs.google.com/presentation/d/1BTMwfktHvDzfzYd6oSnHeaQTEBbtYg8wEMnqoWkamHE/edit?usp=sharing</p> <p>Final Project Video:</p> <p>https://drive.google.com/file/d/1OnO5qWczQbH_aVrgKAtejwLMw6-fFSRW/view?usp=share_link</p> <p>Team Members: Anish Kulkarni, Manuel Abitia, Zizhe Zhang.</p> <p>Special Thanks to: Professor Silberman, Kishore, Moises, and Freddy C. the Robot.</p>"},{"location":"win23team15/","title":"Final Project Repository for Team 15 of the 2023 Winter Class MAE ECE 148 at UCSD","text":"<p>Our Final Project uses the AI controlled autonomous vehicle developed in early course sections to implement a driving protocol based on hand signals. To do this, we utilize the GPS-tracking and following code developed/given in class, and combined it with the DepthAI gesture recognition software pack. Our idea developed from our interest in the OAK-D Lite camera\u2019s stereo vision system and ability to run DepthAI within the camera for processing. To use both of these functions, we chose to run a hand detection program that uses Google Mediapipe to combine these features in a unique project that no one on our team had tried before. Using this bleeding edge hardware was really interesting and showed the potential in the given components and also in general the viability of gesture-based control even in relatively low-cost projects. </p> <p>Below are examples of how our hand detection tracker will recognize our gestures </p> <p> </p>"},{"location":"win23team15/#car-physical-setup","title":"Car Physical Setup","text":"<p>Our car setup used a large piece of acrylic to connect across the RC car strut towers to support all of our electronics. The acrylic had two grooves along the entire length that were 3 inches apart, allowing us to reconfigure our electronics layout as the class progressed. We knew we would be given different devices through the length of the class, so this modular setup gave us the ability to adapt to them quickly rather than redesigning everytime. We also made heave use of 3D printing, which further cemented the viability of our acrylic rail mount system, since we just had to design a mount that could take mounting screws three inches apart. This enabled us to quickly and with low effort include different configurations and components. We then further used 3D-printed components to functionalize parts of our design, such as a height-adjustable camera mount, and a lidar mount employing the same functionality.</p> <p> </p> <p>Our camera setup was originally a rigid mount on the front of the car, but as we tested our initial Donkey Car laps on the outdoor track we recognized the need for adjusting our angle to detect the lines better. This led to our tall hinging mount that gave us better viewing angles that we could set up within seconds for testing.</p> <p> </p>"},{"location":"win23team15/#software","title":"Software","text":"<p>In total, we used three different software packages either from discord, or developed in the course of the class, those beeing the depthai software pack, the d5 GPS control system and the donkeycar Software.</p>"},{"location":"win23team15/#depthai-software-pack","title":"depthai software pack","text":"<p>To detect hand gestures, we use the Luxonis DepthAI Software on the Luxonis OAK-D stereo-vision camera. This software pack brings plenty of functionality, from simple hand tracking, two two-handed interactions, all the way to gesture recognition. We then modified the existing codebase to suit our needs, including specialized code to regognize a set of largely custom, non-included hand gestures:  - Thumbs-up gesture to activate driving - Held up flat hand as a stop signal  - different number of fingers (with the thumb not pointing outwards) changes the speed in a four level control scheme - pointing the index finger with thumb outstreched left or right to change direction of steering </p>"},{"location":"win23team15/#d5-gps-control","title":"d5 GPS-control","text":"<p>In the d5 subdirectory, we used the donkeycar interfaces to program a simple GPS training and following routine to generate a path for the car to follow. This part of the project is still partly in development, as currently only the speed can be varied with hand signals. We largely reused tech generated and tought to us in the course of the class, providing a nice framework on which to test our system.</p>"},{"location":"win23team15/#donkeycar-software","title":"DonkeyCar Software","text":"<p>We use the preconfigured DonkeyCar software package to facilitate control and interfacing of the Jetson nano embedded system with the rest of the RC car. We implemented a special interrupt in the signal chain to be able to pass in different speed values, which are first stored by the modified DepthAI software pack into the file finalproject/comm.txt as a simple string command corresponding to the recognized hand sign. which is then read into the DonkeyCar VESC control subroutine to change the throttle values dynamically. </p>"},{"location":"win23team15/#operation","title":"Operation","text":"<p>One can either run the depthai software on its own with the VESC class implemented right in the file demo_bdf.py by just executing it with all lines commented in, or, if it is desired to just utilize the throttle values, comment out the VESC related code, run the demo_bpf.py and then simultaneaously run the d5 donkeycar code with <code>./python manage.py</code> drive. Then, steering will be controlled by the GPS following algorithm, while the speed is controlled via the detected hand signals. The follwing youtube videos show the basic functionality of our project, and how it interacts with the complex drivetrain of the car.</p> <p>Basic Command Gesture</p> <p>Throttle Changing Values</p>"},{"location":"win23team15/#team-15-boba-team","title":"Team 15: Boba Team","text":"<ul> <li>Reinwei Bai</li> <li>Manu Mittal</li> <li>Reisandy Lamdjani</li> <li>Moritz Wagner</li> </ul>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/","title":"UCSD Robocar Using RC Controller","text":""},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#arduino-micro-pro-leonard-compatible-22mar23-v20","title":"Arduino Micro Pro - Leonard Compatible 22Mar23 - V2.0","text":""},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#expresslrs-24ghz-elrs-radio-long-range-digital","title":"ExpressLRS 2.4GHz (ELRS) Radio Long Range Digital","text":"<p>#Here are the radios we have for UCSD evGKart</p> <p>#There radios use the open source ELRS</p> <p>#There are several advantages on ELRS: Long Range Low Latency, several suppliers, software upgradable</p> <p>#There are few options for radios. We got the radios below because they were compacts, seemed rugged, and received good reviews</p> <p>#These radios will allow us to keep it all digital without the need to use PWM/PPM. ex: Radio UART - &gt; Single Board Computer (SBC).</p> <p>We will use a microcontroller (MCU) to translate near real-time the protocol used on the radios into serial communication with a SBC using USB and also use the MCU for the emergency stop (off) [EMO] directly from the radio command vs. using separate radio for the EMO.</p> <p>Since the radio for PPM/PWM was affordable we have one too in case we want to directly control RC Cars and or use a multiplexer as part of our EMO. This would require two PWM like cables for each radio channel used.</p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#betafpv-literadio-3-pro-radio-transmitter-elrs-supports-external-nano-tx-module","title":"BetaFPV LiteRadio 3 Pro Radio Transmitter- ELRS (Supports External Nano TX Module)","text":"Links Image https://betafpv.com/collections/tx/products/literadio-3-pro-radio-transmitter  https://support.betafpv.com/hc/en-us/articles/5987468200601-Manual-for-Lite-Radio3-Pro   https://www.getfpv.com/betafpv-literadio-3-pro-radio-transmitter-elrs-supports-external-nano-tx-module.html"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#mateksys-expresslrs-24ghz-receiver-elrs-r24-d","title":"MATEKSYS ExpressLRS 2.4GHz Receiver - ELRS R24 D","text":"Links Image https://www.getfpv.com/mateksys-expresslrs-2-4ghz-receiver-elrs-r24-d.html"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#mateksys-expresslrs-24ghz-receiver-pwm-elrs-r24-p6","title":"MATEKSYS ExpressLRS 2.4GHz Receiver - PWM ELRS-R24-P6","text":"Links Image https://www.getfpv.com/mateksys-expresslrs-2-4ghz-receiver-pwm-elrs-r24-p6.html"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#upgrading-the-elrs-firmware","title":"Upgrading the ELRS firmware","text":"<p>24Mar23</p> <p>#Get back to revise these to show version  3.2.0 again.</p> <p>I had to use 3.2.0 because it is the version that ELRS configurator has for the receiver. It seems I need to keep the ERLSv2.luna vs3 for the radio to read the luna script. I hope they will fix it in the future.</p> <p>_#There is a good software that helps on on the upgrade  https://www.expresslrs.org/quick-start/installing-configurator/</p> <p>#They support Windows, Mac, and Linux</p> <p>#Ideally you upgrade the radios Tx and Rx on the same session to make sure they have the same firmware version</p> <p>#Also if you have multiple of the same Tx and Rx doing them consecutively can save you time and help reduce the risk of having different firmware versions.</p> <p>#For the BETA FPV 3 PRO  https://www.expresslrs.org/quick-start/transmitters/betfpvlr3pro/</p> <p>#Turn on the TX radio #Plum the USB C cable connected to your computer #The radio will ask what connection to use #Select USB Serial (Debug) #You will need to look for the Radio in the connections options</p> <p>#Version 2.5.2 </p> <p>#Build &amp; Flash #The first time it will take a while to download, install tools and build the firmware</p> <p> </p> <p>#Download and save on your computer the the LUA Script file #Unplug the USB cable, turn off the radio, turn on again #To upload the LUA script to the radio, unplug the USB cable if connected, connect it again #Select USB Storage (SD) #Then you can use your computer to upload the LUA script you saved earlier https://www.expresslrs.org/quick-start/transmitters/lua-howto/  Download the ELRSv3 Lua Script (you can simply right-click, save-as) into your radio's SD Card under the Scripts/Tools  #elrsV2.lua #Also, let's delete the older .lua script from the root of the DISK_IMG of the radio #Remove the USB cable from the radio #Hold the right top joystick to the left for 1~2 seconds #Tools #EspressLRS #See if the configuration loads, here is where I have problems with V3.x for some problem. V2.5.2 works just fine #The BetaFPV LiteRadio 3 Pro Radio Transmitter uses the two smaller joystick to navigate the configuration menu Moving the right side change the GUI on the display Moving the right side to the left and holding it for few seconds get you into the settings</p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#receiver-matekys-r24-d-24ghz","title":"Receiver MATEKYS R24-D 2.4Ghz","text":"<p>https://www.expresslrs.org/quick-start/receivers/matek2400/   If this is the first time you're flashing/updating your receiver or you're updating it from a previous 2.x firmware via WiFi, first ensure that it has version 2.5.2. Once it has the 2.5.2 flashed, update to 3.x.  #Connect to the Wifi Network the receiver has created. It should be named something like ExpressLRS RX with the same expresslrs password as the TX Module Hotspot. #After the Rx radio boots, wait to see the LED flashing quick. That is an indicadion that its Access Point and web server is running. #Using a web browser http://10.0.0.1/  #Connect to the website of the device and upload new firmware  Then if you connect the receiver to your local WiFi you can get to it by usings its IP address or name. You can scan the network and look for a device called elrs_rx  ex:  http://elrs_rx.local  #Because we will be on the field and not necessarily close to WiFi, let\u2019s leave the AP of the radio on so we can configure it. Not very safe since someone can connect to it while the AP is on. We will check how to protect it with a  password on it a bit later. Moreover, I did not see how to name the radios, it would be hard to know what Rx radios is what in the network. #Let's get the Rx to be on 115200 baud rate #Just connect to the webinterface of the Rx and set the baudrate #We need to use a USB to TTL cable - example from Amazon #Or you can use the embedded WiFi, how cool is that?  Lets try the UART first</p> <p>Red wire of the USB to TTL = + Black wire of the USB to TTL = - Green = White = </p> <p>#Press and hold the boot button while connecting the USB to TTL device to your computer  http://www.mateksys.com/?portfolio=elrs-r24#tab-id-3 For ELRS-R24-D, if update from 2.x to 3.x, Pls select target MATEK 2400 RX R24D and click on Force Flash https://github.com/kkbin505/Simple_RX</p> <ul> <li> <p>Readme.md</p> </li> <li> <p>CRSF decode library for arduino atmega32u4/328p.</p> </li> <li> <p>Based on arduino SBUS decode, modified to decode crsf protocol from elrs receiver</p> </li> </ul> <p>#Let's keep it simple first just by using Arduinos, then we will try a Raspberry PI Pico, and then UCSD DRTC if we want to make everything CAN https://www.amazon.com/gp/product/B09C5H78BP/ref=ppx_yo_dt_b_asin_title_o01_s02?ie=UTF8&amp;psc=1 #I got this coming too so we use a better USB connector, USB C vs. uUSB. #I just did not see there was a version with USB C earlier. https://www.amazon.com/gp/product/B0BCW67NJP/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;psc=1  https://github.com/kkbin505/Simple_RX  </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#some-background-and-references-elrs-radio-long-range-digital","title":"Some Background and References ELRS Radio (Long Range Digital)","text":"<p>TCIII \u2014 03/17/2023 1:11 PM Hi Jack. I spent half of today learning about how to update (flash) the BETAFPV gamepad/plug-in transmitter/receiver software. Both the gamepad or the plug-in transmitter software version has to be the same as the receiver and vice versa. The gamepad configuration and update flashing can be done with the BETAFPV Configurator while even BETAFPV uses the ELRS Configurator to update the plug-in transmitter and receiver. Nothing like consistency. \ud83d\ude04 The BETAFPV LiteRadio 3 gamepad RC transmitter can output 25, 50, or 100 mw while the plug-in RC transmitter can output 100, 250, or 500 mw and comes with ELRS version 2.01.  The Matek ELRS six channel PWM receiver comes with ELRS version 3.0 while the BETAFPV Nano serial output receiver comes with ELRS version 1.0.0-RC5 so I am going to have to reprogram both receivers with ELRS version 2.01. These LoRa gamepads and receivers are definitely not for beginners. \ud83d\ude32 TCIII \u2014 Yesterday at 7:06 AM Hi Jack. I think that it would be best to converse about the Expresslrs LoRa gamepad project on DM until we get the circuitry and software validated, otherwise we might wind up with DC Users trying to implement a LoRa gamepad system that has not been full vetted for functionality and have substantial issues. TCIII \u2014 Yesterday at 7:25 AM Hi Jack. I bought a couple of Arduino Pro Micros so that I could use the crsf_decode_hid.ino sketch from https://github.com/kkbin505/Simple_RX The sketch compiled just fine after I installed the additional required libraries and I will connect the serial output of the BetaFPV serial output Nano receiver to the Pro Micro and see what kind of output I get using the serial monitor. The Arduino Joystick Library (https://github.com/MHeironimus/ArduinoJoystic+kLibrary) is used by the crsf_decode_hid.ino sketch, but the included sketches in the library are designed to take individual joystick and button inputs connected to the Pro Micro and and produce a HID joystick that can be used by PCs for games.</p> <p>Could you please take a look at the crsf_decode_hid.ino sketch as I understand the debug portion, but I don't see where he is outputting the decoded crsf gamepad joystick/button HID values on the USB port. If the crsf_decode_hid.ino sketch is really making the Nano Receiver crsf output look like a HID gamepad then we have it made, though it does require the Pro Micro to interface the Receiver to the SBC as an HID device. \ud83d\ude42</p> <p>I am using a BetaFPV LiteRadio 3 RC gamepad to do the testing compared to your BetFPV LiteRadio 3 Pro. All of the BetaFPV products are still using Expresslrs version 2.x when Expresslrs version 3.x is now available. The BetaFPV LiteRadio 3 can only be programmed with the BetaFPV Configurator so that gamepad is limited to Expresslrs version 2.x where as your BetFPV LiteRadio 3 Pro can be programmed with the Expresslrs Configurator and can be programmed with Expresslrs 3.x. Remember, both the RC gamepad and the receiver must have the same software version of Expresslrs and pass phrase to bind correctly. GitHub GitHub - kkbin505/Simple_RX: crsf protocol decode for avr crsf protocol decode for avr. Contribute to kkbin505/Simple_RX development by creating an account on GitHub. GitHub - kkbin505/Simple_RX: crsf protocol decode for avr GitHub GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... An Arduino library that adds one or more joysticks to the list of HID devices an Arduino Leonardo or Arduino Micro can support. - GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha...</p> <p>This afternoon I will try to determine the voltage of the BetaFPV Nano serial channel receiver output. It will either be 5 vdc or 3.3 vdc. The Nano receiver is powered by 5 vdc, but there is a 3.3 vdc on the Nano receiver pwb because the receiver/processor and WIFI ICs work on 3.3 vdc. So it would seem to me that the serial channel is a 3.3 vdc signal output which will be compatible with either the Rpi or Nano GPIO bus for onboard crsf decoding. Though I like the fact that the Pro Micro, when programmed with the crsf_decode_hid.ino sketch looks just like a HID compliant game controller which makes it portable between RC cars. Additionally the Joystick Descriptor in the crsf_decode_hid.ino sketch allows the customizing of the crsf data stream as to the type of LoRa game controller in use.  TCIII \u2014 Yesterday at 9:53 AM Observations concerning BetaFPV Expresslrs products: 1. Only the BetaFPV LiteRadio 3 Pro can be programmed with the Expresslrs Configurator, unlike the LiteRadio 3 and the LiteRadio 2 SE which require the BetaFPV Configurator, which allows access to the latest Expresslrs version 3.x. 2. The LiteRadio 3 and the LiteRadio 2 SE must programmed with the BetaFPV Configurator because they use a custom version of Expresslrs which is stuck at version 2.x.\ud83d\ude41  3. All of the BetaFPV receivers can be programmed with the Expresslrs Configurator, but keep in mind that both the BetaFPV RC gamepads and receivers must have the same version of the software and pass phrase to bind. 4. Therefore if you are using a LiteRadio 3 or a LiteRadio 2 SE, you cannot upgrade the software of any of the BetaFPV receivers to Expresslrs 3.x or you will lose binding capability. 5. The default baud rate of the BetaFPV Nano serial channel receiver UART is 420000 baud so I had to use the Expresslrs Configurator to change the Nano receiver's UART baud rate to 115200 baud to work with the Pro Micro. 6. Unfortunately the Expresslrs Configurator will only show Expresslrs version 2.5 and up for flashing and the Nano receiver for my purposes needed software version 2.x to update the UART baud rate. 7. Fortunately the Expresslrs Configurator allows access to the Expresslrs software version archives so I could use Expresslrs version 2.0 to update the UART baud rate. \ud83d\ude42   Since I bought the BetaFPV LiteRadio 3, which has an external transmitter port, I also purchased the BetaFPV Nano RF TX Module that can be programmed with the Expresslrs Configurator fortunately. The stock BetaFPV LiteRadio 3 has an output of 100 mw while the BetaFPV Nano RF TX Module can be programmed for outputs of 100, 250, and 500 mw. The higher transmitter output power will obviously result in shortened battery run times and are probably not necessary as 100 mw can be good for over 10 km LOS. \ud83d\ude04 JackSilb \u2014 Yesterday at 10:02 AM You need a USB to TTL to connect it to a Jetson Nano or RPI USB no? Unless you want go direct into the I/Os. TCIII \u2014 Yesterday at 10:06 AM Using the Pro Micro programmed with the crsf_decode_hid.ino sketch allows the BetaFPV Nano serial channel receiver appear to be a HID compliant game controller, like a BT gamepad, to either the Rpi or the Nano without any additional hardware. JackSilb \u2014 Yesterday at 10:06 AM I have the Lite Radio 3 Pro with the cute small display. That will be useful for displaying some info. JackSilb \u2014 Yesterday at 10:07 AM We had something similar for the Teensy too. Stopped using it once we got the VESCs. TCIII \u2014 Yesterday at 10:07 AM That is excellent as you can program any BetaFPV receiver with the latest Expresslrs software. JackSilb \u2014 Yesterday at 10:08 AM Here is the design challenge. If you are using the ELRS radio UART direct to the JTN (Jetson Nano) or RPI and from there USB or Can to the VESC, how can we use the Multiplexer for the EMO. That works only for PWM. On the VESC you can configure a pin for a EMO, we would need a I/O from the Rx radio to bring it 1/0 for EMO direct into the VESC vs. using the JTN or RPI. Ideally, I will find a Rx radio with UART and some I/O capability that I can use a channel, lets say Channel 4 as the EMO switch. JackSilb \u2014 Yesterday at 10:21 AM Maybe this is I can pass the UART info along. Use the PWM for the EMO on/off (1/0) at the VESC TCIII \u2014 Yesterday at 10:31 AM The UART data is in crsf format and will need to be decoded using a Python Program: https://pypi.org/project/crsf-parser/ There are multichannel BetaFPV Expresslrs PWM receivers and I have one: https://www.amazon.com/BETAFPV-ExpressLRS-Compatible-Multirotors-Helicopters/dp/B09WHLJ2GN/ref=sr_1_2?crid=17HV1H415IRJR&amp;keywords=BETAFPV+ExpressLRS+Micro+Receiver&amp;qid=1679420003&amp;sprefix=betafpv+expresslrs+micro+receiver%2Caps%2C107&amp;sr=8-2  PyPI crsf-parser A package to parse and create CRSF (Crossfire) frames, developed primarily to interoperate with ExpressLRS Image BETAFPV ExpressLRS Micro Receiver Support 5 CH PWM Outputs Failsafe... Binding Procedure The Micro receiver comes with officially major release V2.0.0 protocol and has not been set for a Binding Phrase. So please make sure the RF TX module works on officially major release V2.0.0 protocol and no Binding Phrase has been set beforehand. Enter binding mode by plugging ... JackSilb \u2014 Yesterday at 10:42 AM I guess we can do this with ELRS UART -&gt; Arduino or Teensy USB HID -&gt; JTN or RPI  and Arduino or Teensy I/O to the disable pin of the VESC. As long as the Arduino / Teensy code is solid and we have a watchdog we should be good. Adding the MCU in the mix get us going with lots of future opportunities... I need this for our evGoKart ASAP. I need to get some time to play with me. Lots of work(x2) on on the way Do you want me to share with you our work using the Teensy 4.0 including the PCB?  I can send you a PCB too. TCIII \u2014 Yesterday at 10:45 AM I will have more time on Wednesday to test out the Pro Micro setup with the BetaFPV LoRa gamepad and receiver and get back to you with the results. TCIII \u2014 Yesterday at 10:46 AM Address: 11859 SE 91st Circle, Summerfield, FL 34491. JackSilb \u2014 Yesterday at 10:47 AM Can we do the Arduino code to be compatible with the Raspberry Pico? https://www.youtube.com/watch?v=Q97bFwjQ_vQ YouTube Jan Lunge Pi Pico + KMK = the perfect combo for Custom Keyboards Image https://www.youtube.com/watch?v=__QZQEOG6tA YouTube Print 'N Play Use A Raspberry Pi Pico as a HID [Gamepad, Keyboard, Mouse, and Mul... Image Probably a better HW than Arduino Pro Micro TCIII \u2014 Yesterday at 10:56 AM Probably as the Arduino sketch code is C++. JackSilb \u2014 Yesterday at 11:04 AM I purchased few of the Arduinos and Raspberry PI to experiment. Image Charging the controller already. We go from there. JackSilb \u2014 Yesterday at 11:12 AM Yeah, I have a similar from MATEKSYS too. At UCSD, I went away from the PWMs. We go from the JTN to the VESC using USB. The VESC can use the PPM input as output to drive a servo. My current need is for the autonomous evGoKart. Talking you made me find a solution for long distance EMO cheap vs. using the long range telemetry 3DR like only for the EMO. We were using it for the JoStick too with a ROS application on an RPI. Too complex. I will give a try using  ELRS UART -&gt; (Arduino Pro Micro or Teensy or Raspberry PICO) USB HID -&gt; USB (Jetson or RPI)  and (Arduino Pro Micro or Teensy or Raspberry PICO) I/O to the disable pin of the VESCs. We have one VESC for the throttle and one for the steering. TCIII \u2014 Yesterday at 11:34 AM I have a whole bunch of 3DR Telemetry Transceivers on 915 Mhz. \ud83d\ude42  They were only good for LOS even at 100 mw and highly directional too. \ud83d\ude41 TCIII \u2014 Yesterday at 11:38 AM I bought three of the Hiltego Arduino Pro Micros to work with the BetaFPV Nano receiver and all three programmed just fine with a Sparkfun version of Blinky as a test of their functionality. \ud83d\ude42 I have one Rpi PICO that Ed convinced me to buy as it it very fast compare to the Arduino Minis.  REMEMBER that the BetaFPV LiteRadio 3 and the Pro must have the left joystick in the down position when you turn them on or they will buzz and flash the power button RED. As a result of this characteristic, don't energize your car ESC until you have the left joystick at the neutral position (assuming Mode 2) which is basically straight up or your car will go into reverse at full speed.\ud83d\ude32  TCIII \u2014 Today at 6:16 AM Hi Jack. I measured the Nano receiver serial output signal voltage with my digital o'scope this morning and it appears to be 3.43 v which should be safe as an input to either the Nano or the Rpi GPIO bus for direct decoding of the crsf signal. TCIII \u2014 Today at 7:19 AM Hi Jack. I connected the BetaFPV Nano serial output receiver to the Rx input of the Pico Micro running the crsf_decode_hid.ino sketch and viewed the Pico Micro USB HID output in the PC Game Controller Panel \"game controller settings\". I have the RC gamepad frame rate set at the default 150 frames/sec and the joystick display responses are virtually instantaneous and smooth unlike some gamepads. \ud83d\ude42  The \"+\" cursor in the X Axis / Y Axis box is presently controlled by the right joystick horizontal and vertical axes though the \"+\" moves in the opposite direction from the joystick direction of the vertical axis on the gamepad while the left joystick vertical axis (throttle) controls the Z axis red bar and the horizontal axis controls the X axis rotation. The gamepad left push on push off SA switch controls the Y rotation red bar while the left three way switch SB controls the Z rotation red bar.  Observations concerning the Joystick axes and switches definitions:</p> <p>The Joystick HID descriptor report, shown below, controls the definition of the joystick axes and switches:</p> <p>Joystick_ Joystick(JOYSTICK_DEFAULT_REPORT_ID,JOYSTICK_TYPE_GAMEPAD,   0, 0,                 // Button Count, Hat Switch Count   true, true, true,  // X, Y, Z   true, true, true,  // Rx, Ry, Rz   false, false,          // Rudder, Throttle   false, false, false);    // Accelerator, Brake, Steering</p> <p>Based on a search of the IoT for information concerning the HID descriptor report, the HID descriptor shown above does not match any of the HID descriptor report documentation I could find. I suspect that the HID descriptor report shown above is unique to the Arduino Joystick Library requirements and we need to understand how to modify the HID descriptor report to meet the DC BT gamepad joystick/button requirements. TCIII \u2014 Today at 8:05 AM So we now have a choice on how we connect the LoRa RC gamepad receivers to our SBCs:</p> <ol> <li> <p>Connect the Nano receiver UART serial output to a serial Rx pin on the SBC GPIO bus and create a part (https://pypi.org/project/crsf-parser/) that mimics a joystick gamepad or</p> </li> <li> <p>Connect the Nano receiver UART serial output to a Pico Micro running the crsf_decode_hid.ino sketch which makes the crsf output of the Nano receiver look like a HID gamepad (game controller) though it might require some modification of the Joystick HID descriptor report in the sketch to get the RC gamepad to work with DC as a BT gamepad.</p> </li> </ol> <p>Comments?  PyPI crsf-parser A package to parse and create CRSF (Crossfire) frames, developed primarily to interoperate with ExpressLRS Image TCIII \u2014 Today at 10:23 AM Hi Jack. Do you think that any of your graduate students would be interested in writing a DC part to decode the crsf data stream frames and provide functional gamepad output? \ud83d\ude42 If so, I will be glad to work with them.  TCIII \u2014 Today at 11:30 AM I don't think that the Micro HID output looks completely like a PS4/XBox gamepad so the HID joystick descriptor report will probably have to be adjusted to look like a standard gamepad. Since there is no CONTROLLER_TYPE = defined for this HID RC gamepad, Users will have to use the Joystick Wizard to create a custom joystick?</p> <p>One of the issue I see is that LoRa RC gamepads are Mode 2 gamepads where the throttle is on the left joystick and the steering is on the right joystick. \ud83d\ude41  A standard BT gamepad is a Mode 1 gamepad where the throttle is on the right joystick and the steering is on the left joystick. TCIII \u2014 Today at 12:22 PM I suspect that the only way that this LoRa RC gamepad is really going to be really \"user transparent\" is to create a part that decodes and process the crsf serial data stream to create a standard gamepad?  TCIII \u2014 Today at 12:42 PM Hi Jack. Here is a tutorial on how to use an RC Radio transmitter as a gamepad controller: https://www.plop.at/en/rc.html It might give us some insight on modifying, if necessary, the crsf_decode_hid.ino sketch to simulate a generic gamepad? I have contacted the author of the crsf_decode_hid.ino sketch about the joystick HID descriptor report that he used and how he created it. TCIII \u2014 Today at 1:39 PM Hi Jack. Attached is a screen shot of what the output of the RC Gamepad/Pro Micro USB HID Game Controller Panel looks like. The \"+\" cursor in the X Axis / Y Axis box is presently controlled by the right joystick horizontal (Steering) and vertical axes though the \"+\" moves in the opposite direction from the joystick direction of the vertical axis on the gamepad while the left joystick vertical axis (Throttle) controls the Z axis red bar and the horizontal axis controls the X axis rotation. The gamepad left push on push off SA switch controls the Y rotation red bar while the left three way switch SB controls the Z rotation red bar. Image </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#some-background-and-references-pwm-ppm","title":"Some background and References PWM PPM","text":"<p>Hi guys, I have a question about modifying the controller for the donkey car base on this link https://docs.google.com/document/d/1yx1mF0dgEHLx5S3Vqss8YFFkW4thCrl9fpgVliHcuhw/edit  we are able to get the signals from controller to the Arduino after modifying the controller, but now we need to know how to connect the Arduino Leonardo to PWM.</p> <p>Ehsan, the Arduino connects to the JTN or RPI. The PWM board does not change until we have the Teensy in the loop. Therefore, the PWM board is the same. It uses I2C to communicate with the JTN or RPI. That is why it is not in the instructions.</p> <p>Basically the Arduino is simulating a JS0 (Joystick) in a Linux machine. It converts PWM signals from the RC controllers as it was a Game/Pad Joystick. The PWM board is still the same, nothing changes.</p> <p>You need to get the latest Donkey, 3.1.1 if I am not mistaken, and change the Joystick Type on myconfig.py along with your calibration values, Webcam, and use channel 1 for steering, channel 2 for Throttle.</p> <p>The only difference on the myconfig.py for using the Arudino (Leonardo) simulating the Joystick is the setting on the Joystick/game controller type. Everything else is as you were using a regular Donkey setup.</p> <p>https://github.com/n6wxd/wireless-rc-adapter/blob/master/wireless-rc-adapter-2.1/src/PinChangeInterrupt/README.md  Arduino Uno/Nano/Mini: All pins are usable Arduino Mega: 10, 11, 12, 13, 50, 51, 52, 53, A8 (62), A9 (63), A10 (64), A11 (65), A12 (66), A13 (67), A14 (68), A15 (69)  Arduino Leonardo/Micro: 8, 9, 10, 11, 14 (MISO), 15 (SCK), 16 (MOSI)  HoodLoader2: All (broken out 1-7) pins are usable  Attiny 24/44/84: All pins are usable  Attiny 25/45/85: All pins are usable  Attiny 13: All pins are usable  Attiny 441/841: All pins are usable  ATmega644P/ATmega1284P: All pins are usable  Arduino Leonardo/Micro allows interrupts on : 8, 9, 10, 11, 14 (MISO), 15 (SCK), 16 (MOSI) So, let\u2019s use 8, 9, 10</p> <p>https://github.com/n6wxd/wireless-rc-adapter/blob/master/wireless-rc-adapter-2.1/pwm.ino</p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)","text":"<pre><code>const uint8_t RC_PINS[6] = {11, 10, 9, 8, PB2, PB1};\n</code></pre> <p>https://github.com/n6wxd/wireless-rc-adapter  https://github.com/wireless-rc-adapter/wireless-rc-adapter/wiki  We just need 3 channels from the RC Rx  The Arduino board we got out of Amazon.com (Pro Micro) it is compatible with Arduino Leonardo firmware not Arduino Pro Micro. Also, it does not have Pin 11 available for a connection.</p> <p>We need to change the pins used at the board and Arduino IDE to be 10,9,8</p> <p>At pwm.ino From </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_1","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)","text":"<p>Ch1 -&gt; Pin 8 Ch2 -&gt; Pin 9 Ch3 -&gt; Pin 10</p> <p>Also, Ch3 \u201c+\u201d and \u201c-\u201d are used to power the RC Rx Radio Ch3+ (middle pin) -&gt; Pin VCC (5V) Ch3- -&gt; Pin GND (Ground)</p> <p>Note: The power to the Arduino comes from the USB cable that connects to the JTN or RPI. The power to the RC radio comes from the Arduino. https://inventr.io/blogs/arduino/arduino-pro-micro-review-scroller   The GPIO for Jetson Nano and RPI are the same. Connect the PWM board using I2C. See instructions for ECE MAE 148. Nothing changes here. The steering servo and the ESC connect to the PWM board, not Arudino. </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#how-to-make-it-work","title":"How to Make it Work","text":"<p>Install the latest version of the Arduino IDE from here </p> <p>Download the Arduino files from here to a local directory https://github.com/n6wxd/wireless-rc-adapter</p> <p>Plug the Arduino into one of your computer USB port Configure the Arduino GUI to use Arduino Leonardo Configure the USB Port used for the Arduino </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#wireless-rc-adapter-21ino","title":"wireless-rc-adapter-2.1.ino","text":"<p>Open wireless-rc-adapter-2.1.ino in the Arduino GUI </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#pwmino","title":"pwm.ino","text":"<p>The Arduino IDE  should show a tab for pwm.ino Click over the tab pwm.ino Change the pins as below. Our radio has 3 channels, we just use the first 3 pins numbered 10,9,8 </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_2","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)","text":"<pre><code>const uint8_t RC_PINS[6] = {8, 9, 10, PB2, PB1,11};\n</code></pre> <p> Save the file </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#ledino","title":"led.ino","text":"<p>Change back what Steve B. modified for the LEDs From </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_3","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)","text":"<pre><code>#define RXLED 13  // RXLED is on pin 17&lt;br&gt;\n#define TXLED 30  // TXLED is on pin 30&lt;br&gt;&lt;br&gt;\n</code></pre> <p>To</p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_4","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)","text":"<pre><code>#define RXLED 17  // RXLED is on pin 17&lt;br&gt;\n#define TXLED 30  // TXLED is on pin 30&lt;br&gt;\n</code></pre> <p>Save the file</p> <p>Upload the wireless-rc-adapter-2.1.ino into the Aruduino, the LEDs should blink when uploading the software then blink in different patterns depending on software stage</p> <p>See here https://github.com/wireless-rc-adapter/wireless-rc-adapter/wiki </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#wireless-rc-adapter-21ino_1","title":"wireless-rc-adapter-2.1.ino","text":"<p>First make sure you pair your RC Tx and Rx ...</p> <p>To verify that the software is working you can enable the serial debugging. Edit wireless-rc-adapter-2.1.ino Remove the comments in the front of //#define SERIAL_DEBUG and //#define SERIAL_SPD 115200  // &gt;&gt;&gt; Serial-Debug options for troubleshooting &lt;&lt;&lt; define SERIAL_DEBUG  // Enable Serial Debug by uncommenting this line define SERIAL_SPD 115200  // Set debug bitrate between 9600-115200 bps (default: 9600)</p> <p>Save the file Upload / run</p> <p>Then using the terminal from the Arduino IDE you can follow the boot process and calibrations. You can also use that to verify the calibration and values the Arduino is receiving from the RC receiver.</p> <p>After you verify that the 3 channels are working, comment the serial DEBUG lines back, save, upload/run</p> <p>// &gt;&gt;&gt; Serial-Debug options for troubleshooting &lt;&lt;&lt;</p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#define-serial_debug-enable-serial-debug-by-uncommenting-this-line","title":"define SERIAL_DEBUG  // Enable Serial Debug by uncommenting this line","text":""},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#define-serial_spd-115200-set-debug-bitrate-between-9600-115200-bps-default-9600","title":"define SERIAL_SPD 115200  // Set debug bitrate between 9600-115200 bps (default: 9600)","text":"<p> You can test the JS0 operation in a Linux machine, including the Jetson Nano (JTN) and Raspberry PI (RPI) with #Open a terminal #ls /dev/input/ #this command should show a js0 device listed e.g, #ls /dev/input _#by-id  by-path  event0  event1  js0  mice</p> <p>#Then lets try reading joystick values #sudo apt-get install joystick #sudo jstest /dev/input/js0  If you don\u2019t see the joystick working on the JTN or RPI, then don\u2019t try to use it on Donkey. </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#calibration","title":"Calibration","text":"<p>On every startup it tries to load calibration data from the long-term memory and decides if those values are still correct or out of sync. The algorithm triggers a calibration automatically if necessary. Calibration can be triggered manually with powering the adapter while the CAL_CHANNEL is on full duty cycle. In other words the throttle must be up before start and it is automatically starts calibration on boot. When the device is in calibration mode, the leds are flashing steady on the board, and all the channel minimum and maximum values are getting recorded. During this time move all the configured sticks and pots/switches on the transmitter remote to its extents. After there is no more new min's or max's found the algorithm finishing the calibration within CAL_TIMEOUT by checking and saving the values in the long-term memory (eeprom). Leds are flashing twice and turns off, the adapter is now available as joystick on the device it is plugged in.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/","title":"UCSD Robocar Framework","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#1-introduction","title":"1. Introduction","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#2-ucsd-robocar-framework-breakdown","title":"2. UCSD Robocar Framework Breakdown","text":"<p>The UCSD Robocar Framework is a collection of ROS 2 packages for each of the hardware components used on the robocar (e.g. the camera, VESC, LiDAR, etc.). The Nav package acts as the \"brain\" of the collection since it interacts with each of the other independent packages.</p> <p>Having standalone packages instead of one major package makes deployment more robust. Additionally, as the robot becomes more sophisticated, the number of associated packages would likely increase to achieve many different types of tasks depending on the application.</p> <p>So the idea is to develop a package that could in general be used on any car-like robot as well as being able to choose what packages your robot really needs without having to use the entire framework.</p> <p>For example, lets say another company developed their own similar sensor, actuator and nav packages but they have not researched into lane detection. Instead of using the entire UCSD Robocar framework, they could easily just deploy the lane detection package and have some interpreter in their framework read the messages from the lane detection package to suit their needs.</p> <p>Link to the official git repo: ROS 2</p> <p>Note: The hub2 package is a metapackage. For specific details about any individual package, click on any of the packages in either hub to be taken to that packages' main repository.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#21-packages","title":"2.1 Packages","text":"<p>Each UCSD ROS package has a README.md that explains in detail what config, nodes, launch files it has as well as topic/message information. When troubleshooting, consider outlining what problem you are having and what package that most likely the cause of such an error. Then reference the README for that package.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#211-nav","title":"2.1.1 Nav","text":"<p>The navigation package (nav_pkg) is the \"brain\" of the UCSD Robocar framework because it keeps all the launch files in its package to launch any node/launch file from the other packages used in the framework. This makes using the framework easier because you only really have to remember the name of the nav_pkg and what launch file you want to use rather than having to remember all the other package names and their own unique launch files.</p> <p>NAV2 README</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#212-lane-detection","title":"2.1.2 Lane Detection","text":"<p>The lane detection package is one method of navigating by identifying and tracking road markers. The basic principle behind this package is to detect road markers using openCV and then compute whats called the \u201ccross-track-error\u201d which is the difference between the center axis of the car and the centroid (center of \u201cmass\u201d) of the road mark which is then fed into a PID controller for tracking.</p> <p>Lane Detection2 README</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#213-sensor","title":"2.1.3 Sensor","text":"<p>The sensor package contains all the required nodes/launch files needed to use the sensors that are equipped to the car.</p> <p>Sensor2 README</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#214-actuator","title":"2.1.4 Actuator","text":"<p>The actuator package contains all the required nodes/launch files needed to use the actuators that are equipped to the car.</p> <p>Actuator2 README</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#217-basics","title":"2.1.7 Basics","text":"<p>The path package contains all the required nodes/launch files needed to subscribe/publish to the sensor/actuator messages within the framework for fast algorithm prototyping</p> <p>Basics2 README</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#22-updating-all-packages","title":"2.2 Updating All Packages","text":"<p>A utility function was added to the <code>~/.bashrc</code> script that will automatically update all the packages in the framework and then rebuild and source it so it will be ready to start using ROS2!</p> <p>To do so, in your terminal:</p> <pre><code>upd_ucsd_robocar\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#23-launch-files","title":"2.3 Launch Files","text":"<p>The launch file diagrams below show the very general approach of how the packages communicate with one another. With ROS, it just comes down to a combination of starting launch files and sending messages (through topics) to nodes. For specific details about messages types, topics, services and launch files used, please go to the readme for the specific package of interest!</p> <p>The nav_pkg is at the base of each of the diagrams and rooting from it are the launch files it calls that will launch other nodes/launch files from all the other packages in the framework.</p> <p>In ROS2, a dynamically built launch file (at run-time) is used to launch all the different nodes/launch files for various purposes such as data collection, navigation algorithms and controllers. This new way of creating launch files has now been simplified by just adding an entry to a yaml file of where the launch file is and a separate yaml file to indicate to use that launch file or not. There is only one file to modify and all that needs to be changed is either putting a \u201c0\u201d or a \u201c1\u201d next to the list of nodes/launch files. To select the nodes that you want to use, put a \u201c1\u201d next to it otherwise put a \u201c0\u201d which means it will not activate. In the figures below, instead of including the entire ros2 launch command, you will only see the names of the launch files that need to be turned on in the node config file explained more in detail here</p> <p></p> <p>ROS2-FOXY: <code>all_components.launch.py, sensor_vizualization.launch.py</code></p> <p></p> <p>ROS2-FOXY: <code>all_components.launch.py, teleop_joy_vesc_launch.launch.py</code></p> <p></p> <p>ROS2-FOXY: <code>all_components.launch.py, camera_nav_calibration.launch.py</code></p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#3-developer-tools","title":"3. Developer Tools","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#31-ros-guidebooks","title":"3.1 ROS Guidebooks","text":"<p>Links provided below are guides for ROS and ROS2 which include many examples, terminal commands and general concept explanations of the various features in ROS and ROS2.</p> <ul> <li>UCSD ROS Guidebook</li> <li>UCSD ROS2 Guidebook</li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#32-gitlab","title":"3.2 Gitlab","text":"<p>Since the framework uses a meta package (a package that contains multiple packages) we refer to individual packages as submodules.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#321-adding-new-submodules","title":"3.2.1 Adding New Submodules","text":"<ol> <li><code>git submodule add &lt;remote_url&gt;</code></li> <li><code>git commit -m \"message\"</code></li> <li><code>git push</code></li> </ol>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#322-updating-local-submodules-with-remote-submodules","title":"3.2.2 Updating local submodules with remote submodules","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#323-updating-remote-submodules-with-local-submodules","title":"3.2.3 Updating remote submodules with local submodules","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#324-removing-submodules","title":"3.2.4 Removing submodules","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#325-adding-an-existing-package-to-git","title":"3.2.5 Adding an existing package to git","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#33-docker","title":"3.3 Docker","text":"<p>Below is a go-to list of docker commands that can be used with the framework:</p> <p>Some new lingo: * Container name: NAMES</p> <ul> <li> <p>Image name: REPOSITORY</p> </li> <li> <p>Image tag ID (comparable to branches in git): TAG</p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#331-pullingrunning","title":"3.3.1 Pulling/Running","text":"<ul> <li> <p>pulling image from docker hub: <code>docker pull REPOSITORY:TAG</code></p> </li> <li> <p>starting a stopped container: <code>docker start NAMES</code></p> </li> <li> <p>stopping a container: docker stop NAMES</p> </li> <li> <p>Using multiple terminals for a single docker container: <code>docker exec -it NAMES bash</code></p> </li> <li> <p>build docker image and git it a new name and tag: <code>docker build -t REPOSITORY:TAG .</code></p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#332-updatingcreatingsharing","title":"3.3.2 Updating/Creating/Sharing","text":"<ul> <li> <p>Saving changes made while in a container to the original image (change tag to create a new image): <code>docker commit name_of_container REPOSITORY:TAG</code></p> </li> <li> <p>Create a new image from a container: <code>docker tag NAMES REPOSITORY:TAG</code></p> </li> <li> <p>Pushing an image to Dockerhub: <code>docker push REPOSITORY:TAG</code></p> </li> <li> <p>Share files between host and Docker container:</p> </li> <li>From host to docker container: <code>docker cp foo.txt container_id:/foo.txt</code></li> <li>From docker container to host: <code>docker cp container_id:/foo.txt foo.txt</code></li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#333-listing","title":"3.3.3 Listing","text":"<ul> <li> <p>list all images: <code>docker images</code></p> </li> <li> <p>list all running containers: <code>docker ps</code></p> </li> <li> <p>list all containers (including stopped): <code>docker ps -a</code></p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#334-deleting","title":"3.3.4 Deleting","text":"<ul> <li> <p>delete specific container: <code>docker rm NAMES</code></p> </li> <li> <p>delete specific image: <code>docker rmi REPOSITORY:TAG</code></p> </li> <li> <p>delete ALL containers: <code>docker rm -f $(docker ps -a -q)</code></p> </li> <li> <p>delete ALL images: <code>docker rmi -f $(docker images -q)</code></p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#4-accessing-docker-images","title":"4. Accessing Docker Images","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#41-ucsd-robocar-image","title":"4.1 UCSD Robocar Image","text":"<p>Link to image on Docker Hub: Docker Image</p> <p>Computer Architecture: ARM (Jetson)</p> <p>To pull the image from a terminal:</p> <pre><code>docker pull djnighti/ucsd_robocar:latest\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#42-docker-setup","title":"4.2 Docker Setup","text":"<p>The exact \"recipe\" to build this image can be found here</p> <p>If using the virtual machine, this has already been done for you.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#421-enable-x_11-port-forwarding","title":"4.2.1 Enable X_11 Port Forwarding","text":"<ol> <li>On your HOST machine (not the Jetson) enter these commands (Will have to enter every time)</li> </ol> <pre><code>xhost +\nssh -X jetson@ip_address\n</code></pre> <ol> <li>Now on the Jetson, run the following commands to obtain sudo access for docker commands (only needs to be ran once)</li> </ol> <pre><code>sudo usermod -aG docker ${USER}\nsu ${USER}\n</code></pre> <ol> <li>Now check that if X_11 forwarding is working:</li> </ol> <pre><code>xeyes\n</code></pre> <p>If some googly eyes pop up, X_11 is ready to go. IF X_11 PORT FORWARDING IS NOT SETUP, follow steps here to get it set up. Then come back here to continue the steps below.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#422-update-docker-daemon","title":"4.2.2 Update Docker Daemon","text":"<ol> <li>Now modify the Docker <code>daemon.json</code> file (just delete the previous version, then create a new one)</li> </ol> <pre><code>sudo rm /etc/docker/daemon.json \nsudo nano /etc/docker/daemon.json\n</code></pre> <ol> <li>Within the empty <code>daemon.json</code> file, add:</li> </ol> <pre><code>{\n    \"runtimes\": {\n        \"nvidia\": {\n            \"path\": \"nvidia-container-runtime\",\n            \"runtimeArgs\": []\n        }\n    },\n    \"default-runtime\": \"nvidia\"\n}\n\n</code></pre> <ol> <li>Save changes to the file and reboot the Jetson:</li> </ol> <pre><code>sudo reboot now\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#423-running-a-container","title":"4.2.3 Running a Container","text":"<ol> <li>SSH back into the Jetson with the -X flag which enables X_11 Forwarding</li> </ol> <pre><code>ssh -X jetson@ip_address\n</code></pre> <ol> <li>Create a new function in the ~/.bashrc file with command line arguments to easily run a container</li> </ol> <pre><code>gedit ~/.bashrc\n</code></pre> <p>or </p> <pre><code>nano ~/.bashrc\n</code></pre> <ol> <li>Copy this into the bottom of the .bashrc:</li> </ol> <pre><code>robocar_docker ()\n{\n    docker run \\\n    --name ${1}\\\n    -it \\\n    --privileged \\\n    --net=host \\\n    -e DISPLAY=$DISPLAY \\\n    -v /dev/bus/usb:/dev/bus/usb \\\n    --device-cgroup-rule='c 189:* rmw' \\\n    --device /dev/video0 \\\n    --volume=\"$HOME/.Xauthority:/root/.Xauthority:rw\" \\\n    djnighti/ucsd_robocar:${2:-devel}\n}\n\n</code></pre> <p>Notice the two arguments we have made for the bash command:</p> <p>\\${1}: This will be the name of the container, ex. Name_this_container</p> <p>\\${2:devel}: This is the tag id of the image you want to launch a container from. If nothing is specified when calling at the command line (example shown below), the \u201cdevel\u201d tag will be run. </p> <p>Don't modify the bash function \u2014 the arguments are intentional and are not meant to be hard-coded.</p> <ol> <li>Source the ~/.bashrc script so the current terminal can see the new function we just added</li> </ol> <pre><code>source ~/.bashrc\n</code></pre> <ol> <li>Run the following command to enter the docker container</li> </ol> <pre><code>robocar_docker &lt;CONTAINER_NAME&gt;\n</code></pre> <ol> <li>To access the same docker container from another terminal (do this for as many terminals you want)</li> </ol> <pre><code>docker exec -it &lt;CONTAINER_NAME&gt; bash\n</code></pre> <p>At this point the docker setup is complete but don't forget to refer to the useful docker commands sections which includes deleting, creating and updating images locally and remotely.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#43-workspaces-in-docker-container","title":"4.3 Workspaces in Docker Container","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#432-ros2_ws","title":"4.3.2 ros2_ws","text":"<p>ROS version: ROS2-FOXY</p> <p>This workspace contains source compiled packages from ucsd_robocar_hub2</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#433-sensor2_ws","title":"4.3.3 sensor2_ws","text":"<p>ROS version: ROS2-FOXY</p> <p>This workspace contains source compiled packages for various sensors in our inventory.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#44-ros-bridge","title":"4.4 ROS Bridge","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#45-utility-functions-in-bashrc","title":"4.5 Utility functions in <code>~/.bashrc</code>","text":"<ul> <li>Updating all packaging in the ucsd_robocar framework from gitlab: <code>upd_ucsd_robocar</code></li> <li>Source Noetic and ALL ROS packages and start roscore: <code>source_ros1_init</code></li> <li>Source Noetic and ALL ROS packages <code>source_ros1_pkg</code></li> <li>Source Noetic and ALL ROS packages and put user in ros1_ws: <code>source_ros1</code></li> <li>Source foxy and ALL ROS2 packages: <code>source_ros2_pkg</code></li> <li>Source foxy and ALL ROS2 packages and put user in ros2_ws: <code>source_ros2</code></li> <li>Build all packages in ucsd_robocar: <code>build_ros2</code></li> <li>Source ROS bridge: <code>source_ros_bridge</code></li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#5-source-ros-version","title":"5. Source ROS Version","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#51-source-ros1","title":"5.1 Source ROS1","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#52-source-ros2","title":"5.2 Source ROS2","text":"<p>We need to source ROS Foxy and the ros2_ws, below is an alias command that will do that automatically. The alias will also place you in the ros2_ws. This command needs to be run in every new terminal you want to use ROS2 in.  </p> <p>From the terminal:</p> <pre><code>source_ros2\n</code></pre> <p>Another alias was made to rebuild the package if any changes were made to the source code. It will put you in the ros2_ws, then perform a colcon build and then source install/setup.bash to reflect the changes made.</p> <p>From the terminal (This is only needs to be ran in 1 terminal, the changes will be reflected everywhere):</p> <pre><code>build_ros2\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#53-source-ros-bridge","title":"5.3 Source ROS Bridge","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#6-hardware-configuration","title":"6. Hardware Configuration","text":"<p>Not all robots have the same hardware especially when it comes to their sensors and motors and motor controllers. This quick section shows how to select the hardware that is on your robot. There are differences between ROS1 and ROS2 on how this configuration works so please read accordingly. This configuration is only necessary for the UCSD Robocar Image and NOT UCSD Robocar Simple ROS Image.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#61-ros1","title":"6.1 ROS1","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#62-ros2","title":"6.2 ROS2","text":"<p>In ROS2, the hardware configuration is as simple as flipping a switch. Since the launch files in ROS2 are now in python, we can dynamically build launch files! This means no more need to have several different \u201ccar configs\u201d that may have different hardware on them and instead have a single launch file that is capable of launching any component you need by changing a single number (that number is explained below)! There is only one file to modify and all that needs to be changed is either putting a \u201c0\u201d or a \u201c1\u201d next to the list of hardware in the file. To select the hardware that your robot has and that you want to use, put a \u201c1\u201d next to it otherwise put a \u201c0\u201d which means it will not activate.</p> <p>In the <code>car_config.yaml</code> file, there is a list of actuator and sensor packages that can be used with the car. Set the corresponding funtionality for each component according to the direction above \u2014 once that is done, you must build the packages again:</p> <p>From a terminal:</p> <pre><code>source_ros2\nnano src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml\nbuild_ros2\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#7-node-configuration","title":"7. Node Configuration","text":"<p>This quick section shows how to select the nodes/launch files that are on your robot. There are differences between ROS1 and ROS2 on how this configuration works so please read accordingly. This configuration is only necessary for the UCSD Robocar Image and NOT UCSD Robocar Simple ROS Image.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#72-ros2","title":"7.2 ROS2","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#8-sensor-visualization","title":"8. Sensor Visualization","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#9-manual-control-of-robot-with-joystick","title":"9. Manual Control of Robot with Joystick","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#10-integrating-new-packagescode-into-the-framework","title":"10. Integrating New Packages/Code into the Framework","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#11-navigation","title":"11. Navigation","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#12-data-collection","title":"12. Data Collection","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#13-f1-tenth-simulator","title":"13. F1 Tenth Simulator","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#14-troubleshooting","title":"14. Troubleshooting","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#15-frequently-used-linux-commands","title":"15. Frequently Used Linux Commands","text":""},{"location":"markdown-documentation/15-VESC%20Setup%20Instructions/15-VESC%20Setup%20Instructions/","title":"UCSD VESC Setup Instructions","text":"<p>Version 2.0 30 Dec 2022</p> <p>These instructions are for different versions of the VESC \u2014 pay attention to what version you have. Choosing incorrect firmware may harm the VESC.</p> <p>We will be using the VESC Tool software (linked here) to upgrade the firmware in the VESC and measure parameters from the brushless DC motor (BLDC).</p> <p>PLACE THE CAR ON THE PROVIDED STAND WHEN WORKING WITH THE VESC AND ENSURE THAT THE WHEELS ARE CLEAR AND CAN SPIN FREELY</p> <p>You will need the battery to power the VESC and a long micro USB cable to connect the VESC to your computer. See the instructor if you need a longer cable.</p> <p>If the VESC is connected to the SBC, disconnect them before connecting to the VESC.</p> <p>AGAIN, THE WHEELS OF THE ROBOT WILL SPIN, PLEASE MAKE SURE THE WHEELS ARE CLEAR TO ROTATE.</p>"},{"location":"markdown-documentation/15-VESC%20Setup%20Instructions/15-VESC%20Setup%20Instructions/#vesc-tools","title":"VESC Tools","text":"<p>You can ignore prompts to update VESC Tools to the latest version if you are using the class-provided install.</p> <p></p> <p>To start, connect the battery voltage checker to the LiPo battery BMS pins. Then connect the main battery terminals to the input for the VESC and a micro USB cable between the VESC and your computer.</p> <p>Select the VESC Tools connect icon:</p> <p></p> <p>or </p> <p></p> <p>Note: When you connect the VESC to VESC Tools, it may warn you that there is a newer firmware version for the VESC.</p> <p>Feel free to update it on VESC 6.x versions. Do not upgrade VESC 4.x versions.</p>"},{"location":"markdown-documentation/15-VESC%20Setup%20Instructions/15-VESC%20Setup%20Instructions/#updating-the-firmware","title":"Updating the firmware","text":"<p>As of 30 Dec, 2022, the VESC 6.x firmware to be used is version 6.00. You do not need to update it if the version is the same.</p> <p>Navigate to the Firmware tab on the left side in the VESC Tool.</p> <p></p> <p>Select the arrow without the text \"All\" next to it. Hovering over it should display the text \"Update firmware on the connected VESC\"</p> <p></p> <p>Do not unplug the VESC while the firmware is updating, you may damage the VESC.</p>"},{"location":"markdown-documentation/15-VESC%20Setup%20Instructions/15-VESC%20Setup%20Instructions/#motor-detection","title":"Motor Detection","text":"<p>Go to the Welcome and Wizards tab.</p> <p></p> <p>In the menu at the bottom, select <code>Autoconnect</code>. Once the VESC is connected, select <code>Setup Motors FOC</code>. </p> <p></p> <p>You will be prompted to load the default parameters. Select <code>Yes</code>. Then select <code>Generic</code> and  <code>Medium Outrunner</code> after that.</p> <p></p> <p></p> <p>Select <code>BATTERY_TYPE_LIION_3_0__4_2</code> as the battery type (3.0-4.2 is the voltage range of the cells).</p> <p></p> <p>The batteries we are using currently have 4 cells in series; input that below with a capacity of 4500 mAh. When prompted to set current limits by going to the next section, just proceed without setting them.</p> <p>Now check the <code>Direct Drive</code> setting. The wheel diameter is 100.00mm and the motor is a 4 pole - 8 magnet setup.</p> <p></p> <p>Then run the detection procedure.</p> <p></p> <p>Now determine if your motor direction needs to be reversed.</p> <p></p> <p>Once you are done, press finish at the bottom.</p> <p>Now, write the motor configuration to the VESC using the button along the right side of the screen.</p> <p></p> <p>Then write the app configuration.</p> <p></p>"},{"location":"markdown-documentation/15-VESC%20Setup%20Instructions/15-VESC%20Setup%20Instructions/#sensor-detection","title":"Sensor Detection","text":"<p>We are now going to measure characteristic parameters of the motor.</p> <p>Resistance and Inductance</p> <p></p> <p>Motor Flux Linkage</p> <p></p> <p>Then press <code>Apply</code> at the right side of the window.</p> <p></p> <p>Now check the Hall Sensor table under the <code>Hall Sensors</code> tab at the top. Your table should look something like this:</p> <p></p> <p>Write the motor configuration and the app configuration:</p> <p></p> <p></p>"},{"location":"markdown-documentation/15-VESC%20Setup%20Instructions/15-VESC%20Setup%20Instructions/#enable-servo-control-for-steering","title":"Enable Servo Control for Steering","text":"<p>We need to enable the VESC PWM output in order to control the servo for steering.</p> <p>In <code>General</code> under <code>App Settings</code>, set <code>Enable Servo Output</code> to <code>True</code> and write the app configuration.</p> <p></p> <p></p> <p></p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/","title":"Jetson Nano Configuration","text":"<p>This document will take you through the process of setting up your Jetson Nano. This document is a re-written version of this document, which may have some useful information if you are ever stuck: https://docs.google.com/document/d/1TF1uGAFeDARNwbPkgg9xg4NvWPJvrsHW2I_IN9dWQH0/edit</p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#flashing-image","title":"Flashing Image","text":"<p>A Jetson Nano runs off of a externally mounted microSD card, which will first need an image flashed onto it. A custom UCSD Jetson Nano Developer Kit image with some pre-configured settings and files can be downloaded here: UCSD Image. You will need to install a program to flash this download onto your microSD card, we recommend Etcher: Etcher Download. </p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#using-etcher-to-write-ucsd-image","title":"Using Etcher to Write UCSD Image","text":"<p>1) Using an adapter, plug your mircoSD card into a computer with the zipped image file downloaded 2) Open Etcher, select the zipped file under \"Flash from file\" and the microSD card under \"Select target\", and click \"Flash!\" to write the image to the microSD card 3) After flashing is complete, eject the microSD card from your computer and plug it into the back of the Jetson Nano. NOTE: this is a push-in-to-lock and push-in-to-unlock the uSD card. Please do not pull the uSD card out of the slot before unlocking it, otherwise you may damage your JTN and or the uSD card</p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#mobaxterm-installation","title":"MobaXterm Installation","text":"<p>A good tool for windows machines when doing embedded programming is MobaXterm, which offers more robust ssh communication with single board computers. Here you can find the download link.</p> <p>1) Click on the green \"MobaXterm Home Edition v24.2 (Installer Edition)\" to download a zip file 2) Extract the zip file and launch the installer from the file explorer (not the .dat file) 3) Follow the instructions in the installer 4) When installation is complete, launch MobaXterm and select \"Start new terminal\" to be placed on the command line</p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#powering-jetson-nano","title":"Powering Jetson Nano","text":"<p>To power the Jetson Nano you can use the provided 5V 4A power supply. This plugs into the barrel jack port to the left of the USB ports. The Jetson is recieving power and on if there is a green LED near the microUSB port. However, you may need to give it a minute or two to boot and load software. The fan will NOT turn on until you follow the documentation below to install the proper software. If you wish to power the Jetson through a microUSB connection, remove the jumper on the J48 pins near the barrel jack port. Please keep track of the jumper by leaving in connected to only one of the J48 pins.</p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#jetson-nano-ssh-configuration","title":"Jetson Nano SSH Configuration","text":"<p>Since we do not have any monitors or keyboard connected to our Jetson Nano, we will need to remotely login to the Jetson using Secure Shell (SSH) Protocol. Initially, we will need to remotely login to the Jetson Nano using a wire. While connected over the wire we will do some initial configuration and connect to the UCSDRoboCar wifi network. Once the network is configured, we will then be able to wirelessly SSH into the Jetson over the UCSDRoboCar network.</p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#wire-ssh-communication","title":"Wire SSH Communication","text":"<p>1) Power your Jetson Nano using the details above. Plug a microUSB cable from your computer into the microUSB port on the Jetson Nano. Give the Jetson a minute or two to boot up. NOTE: Not all microUSB cables contain the proper wiring to transmit data. If you are sruggling to establish a connection with the Jetson Nano in the following steps, try switching to another microUSB cable.  2) Open a terminal on your computer and type in: <code>ssh jetson@192.168.55.1</code>. This command is calling ssh and asking to login to the user \"jetson\" on the device \"192.168.55.1\". This IP address is unique to the wired connection, and the network IP address will be something different.  3) You will be prompted to input a password for the user you are logging in as. The default username is <code>jetson</code>, which you specified in the ssh command in step 1. The default password is <code>jetsonucsd</code>, which you will type in after the ssh command when prompted.  4) If login is successful, you will be placed into the home directory of the Jetson Nano. If you want to return to your local device, you can enter <code>exit</code> and you will logoff the Jetson. </p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#wifi-configuration","title":"Wifi Configuration","text":"<p>1) Follow the above steps to log into the Jetson over a USB cable.  2) Ensure the USB wifi adapter supplied with the kit is plugged into the Jetson Nano. 3) Once logged in and in the home directory, lets make sure the network service is running properly: <code>sudo systemctl start networking.service</code> 4) List the available wifi networks: <code>sudo nmcli device wifi list</code>. If your desired wifi network is not being listed you can use <code>sudo nmcli device wifi rescan</code> to refresh the list. If needed, you can try to reboot the entire Jetson Nano with <code>sudo reboot now</code> and try scanning again. NOTE: Rebooting will take a couple minutes and will require you to re-ssh into the Jetson Nano. 5) To connect the Jetson Nano to a listed wifi network: <code>sudo nmcli device wifi connect &lt;ssid_name&gt; password &lt;password&gt;</code>. In the case of UCSDRobocar, the command would be <code>sudo nmcli device wifi connect UCSDRoboCar password UCSDrobocars2018</code>. After a few seconds the terminal should print out a success message similar to: <code>Device 'wlan0' successfully activated with 'bab49f3e-b40c-4201-84f9-972ac83ddcb7'</code> 6) To further ensure the Jetson Nano is connected to Wifi, try pinging google using <code>ping google.com</code>. A message should be printing every second with latency information.  7) The Jetson Nano will now have a unique IP address that identifies the machine on the wifi network. You can check what this IP address is by entering <code>ifconfig</code>, which will return the Jetson Nano's network interface configuration. Under the <code>wlan0</code> section, the Jetson Nano's network IP address should be displayed. In the case below, the network IP address is <code>192.168.113.165</code>. If an IP address is not showing up, ensure that the Wifi connection is properly configured and operating using the steps above. </p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#wifi-power-saving-settings","title":"Wifi Power Saving Settings","text":"<p>If Wifi Power Saving is on, it can cause the Jetson Nano to lag and slow down. Here we show how to turn off power saving mode.</p> <p>1) To temporarily turn off power saving, you can use the command <code>sudo iw dev wlan0 set power_save off</code> 2) If you wish to make the change persistent, you will need to edit a config file using a text editor. Ensure Nano is installed by updating pre-existing packages with <code>sudo apt-get update</code> and installing nano with <code>sudo apt-get install nano</code>. We can edit the file by entering <code>sudo nano /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf</code> and changing <code>wifi.powersave = 3</code> to <code>wifi.powersave = 2</code>. </p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#testing-wifi-ssh-communication","title":"Testing Wifi SSH Communication","text":"<p>We will quickly test that we can communicate and login to the Jetson Nano over the UCSDRoboCar wifi network. While testing the wifi, we can stay logged in on the wire and simply login in a different terminal on our local computer.</p> <p>1) Ensure your laptop is connected to the UCSDRoboCar wifi.  2) Open a new terminal on your local computer and try <code>ssh jetson@&lt;wlan0-IP-address&gt;</code>, where 'wlan0-IP-address' is the IP address you found in the previous section. The password will be the same as the wire SSH. </p> <p></p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#configuring-hostname-and-password","title":"Configuring Hostname and Password","text":"<p>To make the SSH process easier and more secure, we should set a unique static hostname for our Jetson Nano and change the password. This new static hostname can then replace the long IP address in the SSH command.</p> <p>1) Check the current hostname by entering <code>hostnamectl</code>.  2) Change your hostname to something unique using <code>sudo hostnamectl set-hostname &lt;new-hostname&gt;</code>. We recommend including your team name in your Jetson Nano's hostname. Confirm your changes by entering <code>hostnamectl</code> again. You must also change the hostname in /etc/hosts using a text editor: <code>sudo nano /etc/hosts</code>. Change \"jetson\" to the hostname you choose.</p> <p></p> <p>3) You can change the password by simply entering <code>passwd</code> and following the prompts.  4) Make sure to remember your newly selected hostname and password, you will need them to log into your Jetson Nano.  5) Now when logging into your Jetson Nano using SSH, you can replace the IP address with your static hostname. This is useful, as sometimes the network will change the IP address of the Jetson Nano, while the hostname will stay static. <code>ssh jetson@&lt;new-hostname&gt;</code>. Remember, we are just changing the devices identifier, we are still logging into the device as the \"jetson\" user with the password we set, which is why \"jetson\" remains unchanged in the SSH command. NOTE: If you are having trouble connecting, try adding a .lan or .local to the end of your hostname in the ssh command. If you are wired to the jetson, then you might end up connecting over the wire. </p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#fan-configuration","title":"Fan Configuration","text":"<p>Our Jetson Nano's are outfitted with a heat sink and fan for cooling. The image should come pre-loaded with the fan drivers, but if the fan is not spinning here are the steps to download the drivers.</p> <p>1) Enter the projects directory from the home directory <code>cd ~/projects</code>. If a projects folder doesn't exist, make one in the home directory: <code>cd</code> to go to the home directory, then <code>mkdir projects</code> to create a directory named \"projects\", then enter the new directory <code>cd projects</code>.  2) In ~/projects, enter <code>git clone https://github.com/Pyrestone/jetson-fan-ctl.git</code>. This clones the github repository \"jetson-fan-ctl\" into your projects directory. 3) Make sure your packages are updated using <code>sudo apt-get update</code> and install python3-dev using <code>sudo apt install python3-dev</code>.  4) Enter \"jetson-fan-ctl\" using <code>cd jetson-fan-ctl</code> and enter <code>./install.sh</code> to install the fan firmware.  5) You can customize fan settings in <code>/etc/automagic-fan/config.json</code> using a text editor of your choice. The command to edit in nano is <code>sudo nano /etc/automagic-fan/config.json</code>. You can check the status of the fan by entering <code>sudo service automagic-fan status</code>. </p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/","title":"Virtual Machine with DonkeyCar/DonkeySim AI Simulator","text":""},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#vmware-player-installation","title":"VMware Player Installation","text":"<p>Virtual machines (VMs) are essentially computers running on computers \u2014 you can simulate a different operating system than the one native to your computer.</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#prerequisites","title":"Prerequisites","text":"<ul> <li>Download the VMware Player installer depending on your OS; check this document for information for additional information if necessary</li> <li>Windows</li> <li> <p>Intel and Apple Silicon Macs, not verified yet</p> </li> <li> <p>Download the Ubuntu VM image \u2014 make sure you have enough space on your disk   (~40 GB zipped, ~50 GB unzipped)</p> </li> <li> <p>https://drive.google.com/file/d/1aGVPzoEPYW0GxUnVGjzkiNsqqJFgZ7hb/view?usp=sharing</p> </li> <li> <p>Minimum 8 GB system RAM on host machine</p> </li> <li>If your system only has 8 GB of RAM, set the ammount of memory allocated to 5120 MB (5 GB)</li> <li>If your system has at least 16 GB of RAM, enter the VM settings for the image and increase the RAM alloted to 8 GB; the VM must not be running to do so</li> </ul> <p></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#vmware-setup","title":"VMware setup","text":"<p>When you install the VMware Player, you will be prompted on options for the install. As you go through the install wizard be sure that you:</p> <ul> <li>Accept the license agreement</li> <li>Install the WHP automatically</li> <li>Don't enable enhanced keyboard</li> <li>Set a custom file path for the Player (if you desire)</li> <li>Select whether or not you want to opt into the diagnostics agreement</li> <li>Select where you want you your shortcuts to be</li> <li>Then press <code>Install</code></li> </ul> <p>Open VMware Player and select Open a Virtual Machine You will be prompted to select an image to be added \u2014 select the image you downloaded with the <code>.vmx</code>  extension</p> <p>It should now appear in the list on the left of the VMware Player window \u2014 single-click the image and select Edit Virtual Machine Settings</p> <p>Here you can edit the memory settings and any other settings required to run the VM</p> <p>If you experience an error with respect to Intel-VT or AMD-V, disable the virualization engine in the Processors tab </p> <p></p> <p></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#initial-boot-up-of-vm","title":"Initial Boot up of VM","text":"<p>If necessary, enable virtualization in your BIOS/UEFI. When you are ready, start the virtual machine.</p> <ul> <li>Login Credentials</li> <li>User: ucsd</li> <li>Password: UcsdStudent </li> </ul>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#cutting-and-pasting","title":"Cutting and Pasting","text":"<ul> <li>If cutting and pasting is not working from the host to the VM, open a terminal in the VM and run the following commands:</li> </ul> <pre><code>    sudo apt-get autoremove open-vm-tools\n    sudo apt-get install open-vm-tools-desktop\n    sudo reboot now\n</code></pre>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#connecting-game-controller","title":"Connecting Game Controller","text":"<p>Connecting a game controller is useful in order to control the car used in the simulations you will be running and other projects (these can include Playstation or Xbox controllers, or the Logitech controller likely included in your kit).</p> <p>These should be connected using via a USB cable, Bluetooth, or a USB dongle.</p> <p>When connecting a controller, the VM should ask if the input device will be connected to the host system or the virtual machine \u2014 connect it to the VM by selecting the name of the VM.</p> <p> </p> <p></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#verify-controller-connection","title":"Verify Controller connection","text":"<p>The controller will be identified as js0 (or js# if there are multiple joysticks connected to the system)</p> <p>Run the following command in a VM terminal:</p> <pre><code>ls /dev/input\n</code></pre> <p>If the controller is connected, it should appear as js0 in the terminal output.</p> <p></p> <p>To test the joystick controls, run in a terminal:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y jstest-gtk\njstest /dev/input/js0\n</code></pre> <p></p> <p>Then interact with the controller to see the values printed to the terminal change (analog inputs should change smoothly, while digital inputs like button presses change between on and off)</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#custom-controller","title":"Custom Controller","text":"<p>If your controller is not behaving correctly, or you need to generate new controller mappings, you can generate custom controllers. </p> <p>See https://docs.donkeycar.com/parts/controllers/ for controller support; custom mapping is linked at the bottom of the page.</p> <p>To setup a new controller or modify input mappings, you can use the Joystick Wizard (described here: https://docs.donkeycar.com/utility/donkey/#joystick-wizard)</p> <p>The joystick wizard creates a custom controller named \"my_joystick.py\" in the <code>mycar</code> folder. To enable it, in the <code>myconfig.py</code> file, set <code>CONTROLLER_TYPE=\"custom\"</code> </p> <p>To run the wizard, from a terminal in the PATH/TO/mycar directory, run </p> <pre><code>donkey createjs\n</code></pre> <p>To determine if the system can see the input device, jstest can be used. If it is not installed, run <code>sudo apt install joystick</code></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#donkeycar-ai-framework","title":"DonkeyCar AI Framework","text":"<p>This software allows you to train an AI model to run simulated or even physical vehicles using computer vision (either virtually or in reality).</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#launching-the-simulator","title":"Launching the Simulator","text":"<p>Using the file explorer in the VM, navigate to <code>~/projects/DonkeySimLinux/</code> and execute the file <code>donkey_sim.x86_64</code></p> <p></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#track-names","title":"Track Names","text":"<p>Depending on the track to be raced on, you need to change the track to train on; those include:</p> <ul> <li>donkey-circuit-launch-track-v0</li> <li>donkey-warren-track-v0</li> <li>donkey-mountain-track-v0</li> </ul>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#customizing-virtual-car","title":"Customizing Virtual Car","text":"<p>From a terminal, run <code>atom myconfig.py</code> from the <code>~/projects/d4_sim/</code> directory.</p> <p>Within the <code>myconfig.py</code> file, change the:</p> <ul> <li>car_name</li> <li>racer_name</li> <li>your country location (under \"country\")</li> <li>a fun fact (under \"bio\")</li> <li>car color (in the dictionary entry for \"body_rgb\")</li> </ul> <p></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#example-config-file","title":"Example Config File","text":"<pre><code># 04Jan22\n# UCSD mods to make easier for the UCSD students to use the Donkey-Sim\n# the following uncommented lines where copied here from the body of myconfig.py below\nDONKEY_GYM = True\n# DONKEY_SIM_PATH = \"remote\"\nDONKEY_SIM_PATH = \"/home/ucsd/projects/DonkeySimLinux/donkey_sim.x86_64\"\n# DONKEY_GYM_ENV_NAME = \"donkey-warren-track-v0\"\nDONKEY_GYM_ENV_NAME = \u201cdonkey-mountain-track-v0\u201d\n# UCSD yellow color in RGB = 255, 205, 0\n# UCSD blue color in RGB = 0, 106, 150\nGYM_CONF = { \"body_style\" : \"car01\", \"body_rgb\" : (255, 205, 0), \"car_name\" : \"UCSD-148-YourName\", \"font_size\" : 30} # body style(donkey|bare|car01) body rgb 0-255\nGYM_CONF[\"racer_name\"] = \"UCSD-148-YourName\"\nGYM_CONF[\"country\"] = \"USA\"\nGYM_CONF[\"bio\"] = \"Something_about_you, ex: Made in Brazil\"\n#\n# SIM_HOST = \"donkey-sim.roboticist.dev\"\n SIM_ARTIFICIAL_LATENCY = 0\nSIM_HOST = \"127.0.0.1\"              # when racing on virtual-race-league use host \"roboticists.dev\"\n# SIM_ARTIFICIAL_LATENCY = 30          # Use the value when you ping roboticists.dev. When racing on virtual-race league, use 0 (zero)\n\n# When racing, to give the ai a boost, configure these values.\nAI_LAUNCH_DURATION = 3            # the ai will output throttle for this many seconds\nAI_LAUNCH_THROTTLE = 1            # the ai will output this throttle value\nAI_LAUNCH_KEEP_ENABLED = True      # when False ( default) you will need to hit the AI_LAUNCH_ENABLE_BUTTON for each use. This is safest. When this True, is active on each trip into \"local\" ai mode.\n#\n# When using a joystick modify these specially USE_JOYSTICK_AS_DEFAULT = True\n# JOYSTICK\n# USE_JOYSTICK_AS_DEFAULT = True     #when starting the manage.py, when True, will not require a --js option to use the joystick\nJOYSTICK_MAX_THROTTLE = 1.0         #this scalar is multiplied with the -1 to 1 throttle value to limit the maximum throttle. This can help if you drop the controller or just don't need the full speed available.\nJOYSTICK_STEERING_SCALE = 0.8       #some people want a steering that is less sensitve. This scalar is multiplied with the steering -1 to 1. It can be negative to reverse dir.\nAUTO_RECORD_ON_THROTTLE = True      #if true, we will record whenever throttle is not zero. if false, you must manually toggle recording with some other trigger. Usually circle button on joystick.\nJOYSTICK_DEADZONE = 0.2             # when non zero, this is the smallest throttle before recording triggered.\n# #Scale the output of the throttle of the ai pilot for all model types.\nAI_THROTTLE_MULT = 1.0              # this multiplier will scale every throttle value for all output from NN models\n#\n</code></pre>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#get-latency-from-remote-server","title":"Get Latency from Remote Server","text":"<p>To get the latency between your computer and the server, ping it using the command </p> <pre><code>  ping donkey-sim.roboticist.dev\n</code></pre> <p></p> <p>Since this computer is on the same network as the server, the delay is much lower than 0.5 ms. When pinging the server within the US, you should expect about 20-60 ms.</p> <p>Replace the value of <code>SIM_ARTIFICIAL_LATENCY</code> with the average ping delay (e.g. <code>SIM_ARTIFICIAL_LATENCY=30</code>)</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#collecting-data","title":"Collecting Data","text":"<p>The AI model works via behavioral cloning. In order to collect data for it, we need to drive the car in the virtual environment.</p> <p>From a terminal, enter the donkey virtual environment with the command:</p> <pre><code>conda activate donkey\n</code></pre> <p>(donkey) should now appear at the beginning of the terminal prompt.</p> <p></p> <p>Enter the donkeycar directory</p> <pre><code>cd ~/projects/d4_sim\n</code></pre> <p>To drive the car in order to collect data, run</p> <pre><code>python manage.py drive\n</code></pre> <p></p> <p>Open a web browser and go to <code>http://localhost:8887</code></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#driving-using-mouse-and-keyboard","title":"Driving using Mouse and Keyboard","text":"<p>From the web address above, you can control the car using a virtual joystick.</p> <p></p> <p>20 laps is recommended for an initial dataset.</p> <p>To stop the DonkeyCar framework, use CTRL + C in the terminal</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#driving-using-a-gamepad","title":"Driving using a Gamepad","text":"<p>To use a physical joystick without using the web browser, edit this section in <code>myconfig.py</code>.</p> <pre><code># #JOYSTICK\n# USE_JOYSTICK_AS_DEFAULT = False      #when starting the manage.py, when True, will not require a --js option to use the joystick\n# JOYSTICK_MAX_THROTTLE = 1         #this scalar is multiplied with the -1 to 1 throttle value to limit the maximum throttle. This can help if you drop the controller or just don't need the full speed available.\n# JOYSTICK_STEERING_SCALE = 1       #some people want a steering that is less sensitve. This scalar is multiplied with the steering -1 to 1. It can be negative to reverse dir.\n#AUTO_RECORD_ON_THROTTLE = True      #if true, we will record whenever throttle is not zero. if false, you must manually toggle recording with some other trigger. Usually circle button on joystick.\n# CONTROLLER_TYPE = 'ps4'            #(ps3|ps4|xbox|pigpio_rc|nimbus|wiiu|F710|rc3|MM1|custom) custom will run the my_joystick.py controller written by the `donkey createjs` command\n# USE_NETWORKED_JS = False            #should we listen for remote joystick control over the network?\n# NETWORK_JS_SERVER_IP = None         #when listening for network joystick control, which ip is serving this information\n# JOYSTICK_DEADZONE = 0.01            # when non zero, this is the smallest throttle before recording triggered.\n</code></pre> <p>Set <code>USE_JOYSTICK_AS_DEFAULT</code> to <code>True</code> and set the controller type <code>CONTROLLER_TYPE</code> to one from the adjacent list (ps3|ps4|...).</p> <p>You may have to uncomment lines in order for them to take effect.</p> <p>When using a controller, the face buttons can have useful functions:</p> <ul> <li>Deleting 100 data points (@20Hz == 5s)</li> <li>Emergency stop</li> <li>Change operations mode (User control, AI model)</li> </ul> <p>Otherwise you may have to determine the function of each button from the terminal outputs when they are pressed.</p> <p>20 laps is recommended for an initial dataset.</p> <p>To stop the DonkeyCar framework, use CTRL + C in the terminal</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#deleting-data-not-to-be-used-in-training","title":"Deleting Data not to be used in Training","text":"<p>Data for training is stored in the <code>~/projects/d4_sim/data</code> directory.</p> <p></p> <p>We can delete data by removing the <code>data</code> folder and creating a new one.</p> <p>Run this command in the <code>d4_sim</code> directory. Be careful \u2014 there is no undoing this if the command runs successfully.</p> <pre><code>rm -rf data\n</code></pre> <p>Then create a new <code>data</code> directory with:</p> <pre><code>mkdir data\n</code></pre>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#training-and-testing","title":"Training and Testing","text":"<p>Using the data in the <code>data</code> folder, we can train a model and give it a name (e.g. 8july24_sim_160x120_20_1.h5)</p> <p>20 laps is recommended for an initial dataset.</p> <p>To do so, run this command from the <code>d4_sim</code> folder.</p> <pre><code>python train.py --model=models/YOUR_MODEL_NAME.h5 --type=linear --tubs=data/\n</code></pre> <p>To test the model, run:</p> <pre><code>python manage.py drive --model=models/YOUR_MODEL_NAME.h5 --type=linear\n</code></pre> <p>Enabling the model is done by pressing the change operation mode button twice. The terminal should state that the car is in AI mode.</p> <p>If the car does not perform well around corners, it could be that throtte and steering data is not being recorded when navigating them.</p> <p>By default, the program records steering only when a throttle input is detected \u2014 when slowing down to corner, this means steering data may not be recorded. To fix this, you can edit in <code>myconfig.py</code>:</p> <pre><code>AUTO_RECORD_ON_THROTTLE = True\n</code></pre> <p>Set this to false. Now, in order to record data, you must press the record button to begin input recording. The terminal will print out when recording is enabled and the amount of samples.</p> <p>If you increase the number of samples recorded after training a model, you can train a new model that uses all of the data in the <code>data</code> folder (old and new \u2014 be sure to give it a different name).</p> <p>To train data from a specific tub and transfer to a previous model:</p> <pre><code>python train.py --tub ~/projects/d4_sim/data/TUB_NAME  --transfer=models/PREVIOUS_MODEL.h5  --model=models/NEW_MODEL.h5\n</code></pre> <p>Tubs are subsections of the data folder that you may create to separate training data. To use all the data in the <code>data</code> folder, do not include a tub name after <code>~/projects/d4_sim/data/</code> in the tub argument.</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#upgrading-to-the-latest-donkey-sim-and-donkey-gym-if-needed","title":"Upgrading to the latest Donkey-Sim and Donkey-Gym (if needed)","text":""},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#common-issues","title":"Common Issues","text":""},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#slow-fps-locally","title":"Slow FPS Locally","text":""},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#ucsd-gpu-cluster-instructions","title":"UCSD GPU Cluster Instructions","text":"<p>Do not use the cluster until you are told the GPU cluster is ready to use.</p> <p>Do not train on the cluster until you have demonstrated model training on your local machine.</p> <p>Instructions from UCSD IT</p> <p>To train our models faster, we can use more powerful GPUs with higher throughputs.</p> <p>On the virtual machine you will be using 2 terminals:</p> <ol> <li> <p>Local Session: Used to interact with the virtual machine</p> </li> <li> <p>Remote Session: From this terminal you will ssh (secure shell) onto the GPU cluster using the proper account. </p> </li> </ol> <p>In the remote session terminal, ssh into the GPU cluster:</p> <pre><code>ssh &lt;username&gt;@dsmlp-login.ucsd.edu\n</code></pre> <p>You will be prompted for a password (case sensitive). No characters will be shown for security purposes.</p> <p></p> <p>Your shell prompt is replaced with your user login for the GPU cluster.</p> <p>You will have access to two containers \u2014 one with only a CPU, and another with the GPU. The GPU clusters are limited, so only use them for training.</p> <p>Available hardware options:</p> <p>Container for transfering data: (2 CPU 4 GB RAM)</p> <pre><code>launch-scipy-ml.sh -i ucsdets/donkeycar-notebook:latest\n</code></pre> <p>Container for training models: (8 CPU, 1 GPU, 16 GB RAM)</p> <pre><code>launch-scipy-ml.sh -g 1 -i ucsdets/donkeycar-notebook:latest\n</code></pre> <p>When creating the GPU container, the terminal should look like:</p> <p></p> <p>You should only have one container open at a time.</p> <p>When launching a container it creates a \"pod\"; in order to exit the pod, run in the terminal:</p> <pre><code>exit\n</code></pre> <p>To confirm that you have exited the container and the instance has successfully been deleted do</p> <pre><code>kubectl get pods\n</code></pre> <p>This should return \"no resources found\".</p> <p></p> <p>If there is a pod, delete it with:</p> <pre><code>kubectl delete pod &lt;POD_NAME&gt;\n</code></pre>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#transfering-data","title":"Transfering Data","text":"<p>In the Remote Session, prepare DonkeyCar.</p> <p>The donkey virtual environment should automatically be invoked for you; otherwise try </p> <pre><code>conda activate donkey\n</code></pre> <p>If donkey is not found, try </p> <pre><code>conda init\n</code></pre> <p>Relogin to the remote session and try the activate command again.</p> <p>Once you are in the virtual environment,</p> <pre><code>mkdir ~/projects\ncd ~/projects\ndonkey createcar --path d4_sim\ncd d4_sim\n</code></pre> <p>In the Local Session</p> <p>To copy over your <code>myconfig.py</code> file:</p> <pre><code>rsync -avr -e ssh myconfig.py &lt;user_name&gt;@dsmlp-login.ucsd.edu:projects/d4_sim/\n</code></pre> <p>To transfer data collected in the local session to the remote session:</p> <pre><code>rsync -avr -e ssh data/&lt;tub_name&gt; &lt;user_name&gt;@dsmlp-login.ucsd.edu:projects/d4_sim/data/\n</code></pre> <p>This sends specific tubs (e.g. tub_#_21-07-13 in this example) to the remote session.</p> <p></p> <p>The tubs should now appear in the remote session.</p> <p></p> <p>The <code>rsync</code> command syncs directories remotely from one system to another. That means it will only copy the differences between the two directories to save time and reduce load. Since the data does not exist initially on the remote system, the first use of <code>rsync</code> will copy the whole folder over to the remote system.</p> <p>Once the data is transferred, close the CPU pod (and verify that it is closed) and open a GPU pod to train on the data.</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#training-on-data","title":"Training on Data","text":"<p>Once the data is transferred to the remote session, training a model on it is the same as on a local session.</p> <p>In the Remote Session</p> <p>You can train multiple tubs at the same time with (the paths to the tubs must be separated by commas, no spaces).</p> <pre><code>python manage.py train --tub data/tub1,data/tub2 --model models/MODEL_NAME.h5 --type=linear\n</code></pre> <p>This should also has the same effect:</p> <pre><code>python train.py --tub data/tub1 --model models/MODEL_NAME.h5 --type=linear\n</code></pre> <p>To alter a previous model with new data:</p> <pre><code>python train.py --tub data/tub1 --model models/NEW_MODEL_NAME.h5 --transfer models/OLD_MODEL_NAME.h5\n</code></pre> <p>Note : If \"imgaug\" is not availible and Donkeysim generates an error, run </p> <pre><code>pip install imgaug\n</code></pre> <p>Once your model training has completed, close the GPU pod (verifying that it has closed) and open a CPU pod to transfer the data back to your local machine.</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#transferring-data-back-to-local-session","title":"Transferring Data back to Local Session","text":"<p>This is done similarly using the <code>rsync</code> command, but the source and destination are flipped since the data is going from the remote session to the local session.</p> <p>In the Local Session</p> <pre><code>rsync -avr -e ssh &lt;user_name&gt;@dsmlp-login.ucsd.edu:projects/d4_sim/models/&lt;model_file&gt; models/\n</code></pre> <p>Now you can test the car in the local session as before.</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#using-a-remote-server-for-the-simulator","title":"Using a Remote Server for the Simulator","text":"<p>The simulator for DonkeyCar (that you found in  <code>~/projects/DonkeySimLinux/</code> and executed with the file <code>donkey_sim.x86_64</code>) can be run from a remote server instead of locally on your machine.</p> <p>The server's name is <code>donkey-sim.roboticist.dev</code></p> <p>You can connect to the remote server by changing the simulator host in the <code>myconfig.py</code> file.</p> <p></p> <p>Set the <code>SIM_HOST</code> from the local IP to <code>donkey-sim.roboticist.dev</code> and set the <code>SIM_ARTIFICIAL_LATENCY</code> to <code>0</code>.</p> <p>Don't forget to change the artificial latency, otherwise your car will experience both real and virtual latency and perform poorly.</p> <p>Since the simulator is running on a remote server, how can I view the car on the track?</p> <p>You can see the car on the livestream on</p> <p>https://www.twitch.tv/roboticists or https://www.twitch.tv/roboticists2</p> <p>The car should appear momentarily after you run the same command to start the car if you have configured the host properly.</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#ros-2","title":"ROS 2","text":""}]}