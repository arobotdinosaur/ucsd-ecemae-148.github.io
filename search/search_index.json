{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ECE/MAE 148: Introduction to Autonomous Vehicles","text":""},{"location":"win23team1/","title":"Team 1","text":"Stoplight, Streetsign and Person Detection  MAE-ECE 148 Final Project <p> Team 1 Winter 2023 </p> Table of Contents <ol> <li> Team Members </li> <li>Final Project</li> <ul> <li>Primary goals</li> <li>Final Project Documentation</li> </ul> <li>Early Quarter</li> <ul> <li>Mechanical Design</li> <li>Electronic Hardware</li> <li>Autonomous Laps</li> </ul> <li>Acknowledgments</li> <li>Contact</li> </ol>"},{"location":"win23team1/#team-members","title":"Team Members","text":"<p>Arturo Amaya (Left), Arjun Naageshwaran (Middle), Hariz Megat Zariman (Right)</p> Team Member Major and Class  <ul> <li>Arturo Amaya - Computer Engineering (EC26) - Class of 2023</li> <li>Arjun Naageshwaran - MAE Ctrls &amp; Robotics (MC34) - Class of 2024</li> <li>Hariz Megat Zariman - Computer Engineering (EC26) - Class of 2024</li> </ul>"},{"location":"win23team1/#final-project","title":"Final Project","text":"<p>The goal of this project was a three-fold application of computer vision. Using the OAKD Lite camera, we aimed to recognize stoplights, traffic signs and people and use those visual inputs to change the state of our car.</p>"},{"location":"win23team1/#primary-goals","title":"Primary Goals:","text":"<p>1) Red, yellow and green stoplights would make the car stop, slow down, and go 2) (Stretch goal) Stop signs, speed limit signs, and right turn signs would be recognized and make the car perform their designated function 3) Individuals could be recognized and followed by the car with dynamic throttle and steering, while the car simultaneously recognized and performed stoplight and streetsign functions</p>"},{"location":"win23team1/#goal-red-yellow-and-green-stoplights","title":"Goal : Red, Yellow and Green stoplights","text":"<p>The car was able to detect red and green images succesfully. When shown red, the car completely stopped, and would only move forward when a green light was shown. Due to time constraints and inconsistent lighting conditions, the car was not able to detect a yellow light consistently but would be able to reduce throttle on detection.</p>"},{"location":"win23team1/#goal-person-following","title":"Goal : Person Following","text":"<p>The car was able to identify person objects using the OAKD camera. By doing so, it was then capable of following a person by adjusting steering values based on how far the person strayed from the center of the camera's FOV. Furthermore, the car adjusted its throttle such that the further away a person is, the faster it goes and stops when approaching to near.</p>"},{"location":"win23team1/#stretch-goal-street-sign-detection","title":"Stretch Goal: Street Sign Detection","text":"<p>Our team was able to obtain obtain the Mapillary Dataset (containing many real-world street signs) and extract the relevant files and labels which were useful to our project (such as the U-turn and stop sign). Unfortunately, due to time constraints, unlabeled images and issues with training the dataset (slow locally, incorrect format for GPU cluster and many others), we were unable to reach this goal on time. However, we were able to implement the movement methods for the car object if it were to identify these signs,</p> <p>See <code>car.py</code> for these methods.</p>"},{"location":"win23team1/#final-project-documentation","title":"Final Project Documentation","text":"<ul> <li>Final Project Proposal</li> <li>Final Project Update 3/7</li> <li>Final Project Update 3/14</li> <li>Final Project Presentation</li> </ul>"},{"location":"win23team1/#early-quarter","title":"Early Quarter","text":""},{"location":"win23team1/#mechanical-design","title":"Mechanical Design","text":"<p>Prefabricated parts of the car were given to us, but parts like the base plate, dual camera mount, LIDAR stand, and cases for sensitive electronics (VESC, GNSS, Servo PWM, etc.) were either 3D printer or laser cut. These are a few of the models compared to their real-life, 3D printed counterparts.</p> Top Camera Mount Bottom Camera Mount LIDAR Mount Camera Mount (Physical) LIDAR Mount (Physical)"},{"location":"win23team1/#electronic-hardware","title":"Electronic Hardware","text":"<p>Our team used only the electronic components given to us. In particular, we focused primarily on the OAK-D camera, Jetson NANO and the GNSS board (only used for the 3 GPS Autonomous Laps). When assembling the circuit, we used the following circuit diagram (given by the TAs of the class):</p>"},{"location":"win23team1/#autonomous-laps","title":"Autonomous Laps","text":"<p>Below is a youtube playlist of the car completing 3 autonomous laps using the DonkeyCar framework under different conditions. </p> <p></p>"},{"location":"win23team1/#acknowledgments","title":"Acknowledgments","text":"<p>Credited Code Examples: * Traffic light example Git * DepthAI Git * VESC Object Drive Folder * DonkeyCar Framework</p> <p>Special thanks to Professor Jack Silberman, and TAs Kishore Nukala and Moises Lopez (WI23), and to all our amazing classmates of Winter 2023</p>"},{"location":"win23team1/#contact","title":"Contact","text":"<ul> <li>Hariz Megat Zariman - hzariman@gmail.com | mqmegatz@ucsd.edu</li> <li>Arjun Naageshwaran - arjnaagesh@gmail.com | anaagesh@ucsd.edu</li> <li>Arturo Amaya - a1amaya@ucsd.edu | aramaya@ucsd.edu</li> </ul>"},{"location":"win23team2/","title":"Team 2","text":""},{"location":"win23team2/#team-2-ecemae-148-final-report","title":"Team 2 ECE/MAE 148 Final Report","text":""},{"location":"win23team2/#wave-the-team-2-fast-2-furious","title":":wave: The Team: 2 Fast 2 Furious","text":"<p>(Left to Right) - Elias Fang (CSE) - Ainesh Arumugam (ECE) - Matthew Merioles (ECE) - Junhao \"Michael\" Chen (MAE)</p>"},{"location":"win23team2/#project-overview","title":"\ud83d\udcdd Project Overview","text":"<p>Mario Kart in real-life? That's basically what we did. We designed a boost system similar to those in Mario Kart, where detecting colored pieces of paper on the track can change throttle for a short period of time. Depending on the color of the \"pad\" the car drives over, it will either speed up, slow down, or stop for a few seconds, just like in the game!</p> <p></p>"},{"location":"win23team2/#our-robot","title":"\ud83c\udfce Our Robot","text":""},{"location":"win23team2/#birds-eye","title":"Bird's Eye","text":""},{"location":"win23team2/#front","title":"Front","text":""},{"location":"win23team2/#left","title":"Left","text":""},{"location":"win23team2/#right","title":"Right","text":""},{"location":"win23team2/#back","title":"Back","text":""},{"location":"win23team2/#schematic","title":"Schematic","text":""},{"location":"win23team2/#final-project","title":"\ud83c\udf44 Final Project","text":""},{"location":"win23team2/#what-we-promised","title":"What We Promised","text":""},{"location":"win23team2/#must-haves","title":"Must haves","text":"<p>[X] Distinguishing different colors through the camera</p> <p>[X] Adjust the throttle based on the color</p>"},{"location":"win23team2/#nice-to-haves","title":"Nice to haves","text":"<p>[X] Have the car detect a flat piece of paper on the track (like a booster pad)</p> <p>[ ] Combine with lane-following algorithm</p>"},{"location":"win23team2/#gantt-chart","title":"Gantt Chart","text":"<p> https://sharing.clickup.com/9010060626/g/h/8cgn7aj-87/769d44f22562beb</p>"},{"location":"win23team2/#what-we-accomplished","title":"What We Accomplished","text":""},{"location":"win23team2/#color-detection","title":"Color Detection","text":"<ul> <li>Used OpenCV for color detection and edge tracing</li> <li>Used color mask algorithm to detect proportion of frame that color takes up</li> <li>Detected multiple colors at the same time</li> <li>Determined HSVs for orange, pink, and blue</li> </ul> <p>Demo</p>"},{"location":"win23team2/#pyvesc","title":"PyVESC","text":"<ul> <li>Connection through external webcam</li> <li>Different RPM values are sent through PyVesc to achieve different speed for different colors marked by different states:</li> <li>Blue (Boost) = speed up for 3 sec</li> <li>Pink (Slow)= slow down for 3 sec</li> <li>Orange (Stop) = stop for 3 sec</li> <li>Neutral (Normal) = constant rpm</li> </ul> <p>Blue Demo</p> <p>Pink Demo</p> <p>Orange Demo</p>"},{"location":"win23team2/#presentation","title":"Presentation","text":"<p>https://docs.google.com/presentation/d/1oJPRLYIKvHUXEIK9hoYpPFoFAyHuG6sE7ZrU9NQPG8g/edit?usp=sharing</p>"},{"location":"win23team2/#code","title":"Code","text":"<p>https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart.py</p>"},{"location":"win23team2/#possible-future-work","title":"Possible Future Work","text":"<ul> <li>Change the colored paper into Mario Kart items (mushroom, bananas, etc.) for the car to identify</li> <li>Allow the car to run autonomously on a track and still apply speed changes</li> <li>Race with other teams \ud83d\ude09</li> </ul>"},{"location":"win23team2/#autonomous-laps","title":"\ud83c\udfc1 Autonomous Laps","text":"<p>DonkeyCar</p> <p>ROS2 Line Following</p> <p>ROS2 Lanes</p> <p>GNSS</p>"},{"location":"win23team2/#additional-work-done","title":"Additional Work Done","text":"<ul> <li>Our team was tasked with implementing a depth feature with our contour function, using depthAI to integrate with the OAK-D Camera!</li> <li>Our updated code is now able to measure how far the contoured object is, measured in cm!</li> </ul> <p>[Depth Demo] https://www.youtube.com/watch?v=NYIz7--TpgY [Updated Code] https://github.com/UCSD-ECEMAE-148/winter-2023-final-project-team-2/blob/main/MarioKart_depth.py</p>"},{"location":"win23team2/#acknowledgements","title":"Acknowledgements","text":"<p>Thanks for Professor Jack Silberman, TA Kishore Nukala, and TA Moises Lopez!</p>"},{"location":"win23team3/","title":"Final Report for Team 3 (ECE/MAE 148)","text":""},{"location":"win23team3/#team-members","title":"Team Members","text":"<ul> <li>Nathaniel Barnaby - ECE</li> <li>Yang-Jie Qin - ECE</li> <li>Cheuk Hin Bryan Cheng - MAE</li> <li>Patrick Nguyen - MAE</li> </ul>"},{"location":"win23team3/#project-overview","title":"Project Overview","text":"<ul> <li>Our initial final project was a combination of the usage of an IMU and GNSS to implement position tracking. An IMU is an inertial measurement unit, composing of gyros, accelerometers, and more sensors to track the movement of something. With these sensors, the car's location can be estimated off of a general initial GPS location with the addition to its movement measured by its speed, acceleration, turning, etc. This ended up being too complex for our team which resulted in little progress. We were then assigned new mini tasks which consists of using 2 of the sensors provided in our kits. The assignment was to use the OAK-D camera and the lidar separately to measure depth of both a small (0.15m) object and a larger (0.5m) object at different distances. We ended up comparing results of both objects at distances of 0.5, 1, 2, 3, and 4 meters. We would then compare the outputed values from the sensors to what the actual correspond measurment. A comparison between the accuracy of depth finding between the Oak-D camera and lidar would also be necesasry. A second task was assigned which was to output a face recognition system out of the OAK-D camera. </li> </ul>"},{"location":"win23team3/#results","title":"Results","text":"<ul> <li>For the distance measurement assignment, both the camera and lidar were able to successfully measure distance for the small the large object at the different ranges. </li> <li>For the camera, it was accurate at determining smaller distances, but at larger distances (3+ meters) error seemed to begin growing exponentionally. The difference between small and large objects was negligible as long as the area in which the distances were averaged fit within the object, which at further distances can start causing fluctuations with smaller objects. </li> <li>For the lidar, it was accurate at determining all distances with a linear or almost static amount of error. At smaller distances this was larger than the error of the camera, but at larger distances it vastly outperformed the camera due to the nature of the camera's exponential error. Additionally, the lidar had to be hand-calibrated, so with more time the error could have been lowered due to this effect. Also, since we recorded distance measurements within a range of 0.4 degrees, the measurement would be inaccurate with a smaller object at longer distances. This could be overcame by decreasing the range and waiting longer.</li> <li> <p>For most scenarios the lidar seems to be the winning choice for distance measurement. While at lower distances the camera seemed to outperform the lidar, the lidar seems to be more consistent with its measurements than the camera. Additionally, the lidar offers 360 degrees of distance measurements while the camera only works in one direction.</p> </li> <li> <p>DepthAI Distance Measurement with ~0.5 m^2 Object Video Link</p> </li> <li> <p>DepthAI Distance Measurement with 0.15 m^2 Box Video Link</p> </li> <li> <p>LiDAR Distance Measurement with ~0.5 m^2 Object Video Link</p> </li> <li> <p>LiDAR Distance Measurement with 0.15 m^2 Box Video Link</p> </li> <li> <p>As for the camera face recognition, we were succesfully able to output video display which recognizes faces. This is a relatively fast responding system. It outputs the number of faces it recognizes which we tested from 0-3 faces real time. There is some error within the system as it can be innacurate thinking other shiny objects and or parts of a face are another face. It is also not limited to stationary faces as it recognizes people moving too. </p> </li> <li>Face Recognition Video Link</li> </ul>"},{"location":"win23team3/#gantt-chart","title":"Gantt Chart","text":""},{"location":"win23team3/#hardware-mechanical-design","title":"Hardware: Mechanical Design","text":"<p>\\ Camera/flashlight Mount</p> <p>\\ Electronics Tray</p> <p>\\ Front/rear Electronics Plate Offset</p> <p>\\ GPS Mount</p> <p>\\ IMU Mount</p> <p>\\ Jetson Case Key Mount</p> <p>\\ Jetson Nano Main Case</p> <p>\\ Lidar Tower</p> <p>\\ Servo Voltage Converter</p> <p>\\ Vesc Power Distributor</p>"},{"location":"win23team3/#previous-designs","title":"Previous Designs","text":""},{"location":"win23team3/#electronic-components","title":"Electronic Components","text":"<p>\\ Jetson Nano</p> <p>\\ OAK-D Camera</p> <p>\\ Lidar LD06</p>"},{"location":"win23team3/#electronic-wiring-schematic","title":"Electronic Wiring Schematic","text":""},{"location":"win23team3/#final-set-up","title":"Final Set Up","text":"<p> Bird's Eye View</p> <p> Left View</p> <p> Right View</p>"},{"location":"win23team3/#packages-and-drivers","title":"Packages and Drivers","text":"<ul> <li>cv2 (OpenCV)</li> <li>depthai (DepthAI)</li> <li>numpy</li> <li>math</li> <li>binascii</li> </ul>"},{"location":"win23team3/#milestones","title":"Milestones","text":"<ul> <li>Face Recognition using DepthAI - Detects faces through a webcam and displays a count in the terminal</li> <li>Distance Measurement using DepthAI - Using the disparity between the left and right cameras of the OAKD, distance can be calculated. This was averaged over an area to give an estimated distance of an object.</li> <li>Distance Measurement using LiDAR - Using a LiDAR is is relatively simple to detect distances in a 360 degree range. By averaging distances over a very small range (0.4 degrees) we determined the distance of an object.</li> </ul>"},{"location":"win23team3/#potential-future-workunaccomplished-goals","title":"Potential Future Work/Unaccomplished Goals","text":"<ul> <li>Recognizing and labeling specific faces</li> <li>Running code off of the OAK-D Camera instead of needing an external computer to run the code.</li> </ul>"},{"location":"win23team3/#presentations","title":"Presentations","text":"<p>-Final Project Proposal -Final Presentation</p>"},{"location":"win23team3/#acknowledgments","title":"Acknowledgments","text":"<p>Professor Jack Silberman, TA Kishore Nukala, Moises Lopez-Mendoza, Design and Innovation Building, all of our wonderful classmates</p>"},{"location":"win23team4/","title":"Team 4 Final Project Report","text":""},{"location":"win23team4/#members-vasanth-senthil-ece-eddy-rodas-limamae-lingpeng-mengece","title":"Members: Vasanth Senthil (ECE), Eddy Rodas Lima(MAE), Lingpeng Meng(ECE)","text":""},{"location":"win23team4/#physical-setup","title":"Physical Setup","text":""},{"location":"win23team4/#initial-goals","title":"Initial Goals","text":""},{"location":"win23team4/#objective","title":"Objective","text":"<ul> <li>Make our RoboCar follow sound using sound localization from multiple microphones.</li> </ul>"},{"location":"win23team4/#must-haves","title":"Must Haves","text":"<ul> <li>Car is able to determine approximate direction of an audio source</li> <li>Car moves in towards audio source once direction of audio is determined</li> </ul>"},{"location":"win23team4/#nice-to-haves","title":"Nice to Haves","text":"<ul> <li>Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume</li> <li>Accurate movement towards source</li> </ul>"},{"location":"win23team4/#accomplishments","title":"Accomplishments","text":"<ul> <li>Researched and experimented with different methods of sound localization, such as using time of arrival delay between microphones</li> <li>Used the pyaudio library to collect streams of microphone data from each of the three microphone, and processed this data in order to tell direction real-time based on sound level</li> <li>Used the pyvesc library to create a custom movement file that allows us to easily move the RoboCar based on provided angles and throttle until specified conditions are met</li> <li>The microphone processing file called upon the movement functions after determining current direction.</li> <li>We also spread the left and right microphones to minimize the noise created by the car\u2019s movement, which would influence the accuracy of direction calculations.</li> </ul>"},{"location":"win23team4/#demo-videos","title":"Demo Videos","text":"<ul> <li>Static Source Left</li> <li>Static Source Right</li> <li>Moving Source Front</li> <li>Moving Source Back</li> <li>Moving Source Further Away</li> </ul>"},{"location":"win23team4/#issues-faced","title":"Issues Faced","text":"Issue Solution Original idea of using time delay between microphones was not possible due to jetson limitations Decided to use volume of sound signals instead of microphone timing in order to determine direction of audio. Sound from car\u2019s movement, such as servo motors and wheels, were causing our robot to turn randomly. Created wings out of cardboard which lifted and spread out the left/right microphones away from the wheels. Random/sudden loud noises from the environment made the car turn in wrong directions, even when originally was moving correctly towards audio source. Averaged the audio signals of each microphone over the last few ticks using a buffer in order to reduce impact of outlier noises. Required tuning, as a small buffer would not remove outliers as effectively while a large buffer reduced reaction time of robot\u2019s movement when direction of audio actually changed"},{"location":"win23team4/#what-did-not-work","title":"What did not work","text":"<ul> <li>Original plan was to use the time delay between when microphones head a noise over a sound threshold in order to calculate angle of sound. Problem with that is the Jetson Nano is running Linux, and due to background processes and overhead, it is not precise enough to identify the timing needed for this method to work when the microphones are this close together. This may have been possible with a real-time system such as an Arduino</li> <li>Another method was using real-time localized sound by performing beamforming using microphones in array, but we were not certain if we had enough time to implement and debug this method in time (received the microphones on Saturday) as it was significantly more complex than the other options. </li> </ul>"},{"location":"win23team4/#next-steps-if-we-had-more-time","title":"Next Steps (If we had more time)","text":"<ul> <li>Car follows specific audio signals (eg. someone\u2019s voice) rather than based on volume, would help filter interference with background noises.</li> <li>Function based on sound difference between microphones to get precise steering angles. Current only steers forward, full left, full right.</li> <li>More accurate movement with our backwards direction</li> <li>Minimize unwanted noise coming from either surroundings or the vehicle.</li> </ul>"},{"location":"win23team5/","title":"Final Project Repository for Team 5 of the 2023 Winter Class MAE ECE 148 at UCSD","text":"<p>Our Final Project has one main objective, which is inspired by a pet dog that plays fetch. Our goal is to design a robot that can identify a green ball, like a tennis ball, locate it, move towards it, pick it up, and return back to its initial location. We achieved this using an OpenCV-based vision system to recognize the ball and Pyvesc to control the car's movements. We also designed a claw mechanism to pick up the ball when it's within range and a servo to move the ball into the claw.</p> <p>In summary, our project demonstrates the capabilities of an autonomous robot that can navigate an environment, recognize objects, and perform tasks like fetching. With further improvements, this type of robot could have many potential applications in various industries.</p> <p> </p>"},{"location":"win23team6/","title":"MAE/ECE 148 Winter 2023 at UCSD","text":""},{"location":"win23team6/#team-6","title":"TEAM 6","text":"<p>Our project uses the OAK-D camera, a roboflow YOLO model, PyVESC module, and an Arduino-powered camera mount to get our car to scan its surroundings until it finds a basketball and drive until it is within about 0.5 m of the ball.</p>"},{"location":"win23team6/#car-assembly","title":"Car Assembly","text":""},{"location":"win23team6/#vehicle-body","title":"Vehicle Body","text":""},{"location":"win23team6/#camera-mount","title":"Camera Mount","text":""},{"location":"win23team6/#tech-stack","title":"Tech Stack","text":""},{"location":"win23team6/#roboflowoak-module","title":"RoboflowOak Module","text":"<p>We used roboflow to train a ball detection model and host the model. We, then, made API calls to hosted model to retrieve predictions on frame captures from the OAK-D camera. Once a ball is found, we wrote a script to calculate the angle between the center of the bounding box drawn around the detected ball and the centerline of the camera (which by default is the center of the frame).</p>"},{"location":"win23team6/#pyvesc-module","title":"PyVESC Module","text":"<p>We used the pyvesc module to set our servo's angle once a detection has been made. The steering angle is proportional to the calculated angle. Once the steering is set, we increase throttle for about half a second and then stop the motor to make another detection. We then loop over these steps until the ball is within 0.5 m of the frame. Our stopping condition was for the width of the bounding box of the ball to be a certain ratio of the total frame width. The ratio is hardcoded based on fine-tuning to get the car to stop at about 0.5 m.</p>"},{"location":"win23team6/#arduino-board","title":"Arduino Board","text":"<p>We used an arduino nano to control the camera mount. The camera mount can rotate horizontally (yaw-equivalent) within a range of 180 degrees. And it can move up and down (pitch-equivalent) within a range of 90 degrees. This is used to move the camera around so it can scan the surroundings for a ball (in case the ball is not in frame). </p>"},{"location":"win23team6/#motorized-camera-mount","title":"Motorized Camera Mount","text":"<p>We designed a camera mount that is actuated by 2 servos, one controls the camera's pitch angle and the other controls the camera's yaw angle. The mount elevates the camera 5 inches amove the vehicle's mounting plate. It allows the camera to turn and scan for the target ball. </p> <p>https://user-images.githubusercontent.com/58583277/227646168-1071f237-de95-4f92-ad65-a90ee5fe2b01.mp4</p>"},{"location":"win23team6/#how-to-run-the-code","title":"How To Run The Code","text":"<p>One can run python run.py from inside the Jetson Nano mounted on their car. This will load the model and also detect the motor and begin the purported task of finding a basketball. If no basketball is found, it will remain stationary. The 148remote/ folder contains a cool arduino program for proof of concept. It moves the camera mount through its full range of motion in a rhythmic fashion. </p>"},{"location":"win23team6/#vehicle-in-action","title":"Vehicle In Action","text":"<p>Click Here For Video</p>"},{"location":"win23team6/#future-improvements","title":"Future Improvements","text":"<p>We can use the Jetson to control the servo motors through the Arduino. We can account for the yaw angle of the camera mount and add that to the steering, so that the car can steer toward targets that is not in the field of view of the camera when the camera is pointing straight forward. We can also improve the precision of the ball-recognition model by using more pictures of the ball to train the model. </p>"},{"location":"win23team6/#team-6-wall-e-used-to-be-cyclops","title":"Team 6: Wall-E (used to be Cyclops)","text":"<ul> <li>Saathvik Dirisala (Data Science)</li> <li>Victor Chen (Computer Engineering)</li> <li>Yang Song (Mechanical Engineering) </li> </ul>"},{"location":"win23team7/","title":"Team 7","text":""},{"location":"win23team7/#ucsd-ecemae-148-2023-winter-team-7","title":"UCSD ECE/MAE-148 2023 Winter Team 7","text":""},{"location":"win23team7/#team-members","title":"Team Members","text":"<p>Francisco Downey (BENG), Jonathan Xiong (ECE), Nicholas Preston (MAE), Karthik Srinivasan (MAE)</p>"},{"location":"win23team7/#final-project-overview","title":"Final Project Overview","text":"<p>For our final project, we made our car drive from point A to B, given starting and ending GPS points. </p>"},{"location":"win23team7/#assembled-car-design","title":"Assembled Car Design","text":"<p> LIDAR was set in the front of the car. This was an easy decision for the team since we cared for varying obstacles crossing the points that comprise the path from Point A to Point B. Essentially, it was only important to care for obstacles that come into the front of the moving car. GNSS was secure near the center of the car with the antenna placed high and toward the rear.</p>"},{"location":"win23team7/#donkeycar-3-autonomous-laps","title":"DonkeyCar - 3 Autonomous Laps","text":"<p>https://user-images.githubusercontent.com/103704890/227807530-35ed2ea5-2bc0-4b8b-983d-ca6bed659390.mp4</p>"},{"location":"win23team7/#ros2-line-following-3-autonomous-laps","title":"ROS2 Line Following - 3 Autonomous Laps","text":"<p>https://user-images.githubusercontent.com/103704890/227801950-dfd60ddd-300e-4af8-9878-c62338184269.mp4</p>"},{"location":"win23team7/#ros2-left-lane-3-autonomous-laps","title":"ROS2 Left Lane - 3 Autonomous Laps","text":"<p>https://user-images.githubusercontent.com/103704890/227804039-c1aafb7a-8bf6-4f86-89cf-1fa21c7ea5d2.mp4</p>"},{"location":"win23team7/#gps-3-autonomous-laps","title":"GPS - 3 Autonomous Laps","text":"<p>https://user-images.githubusercontent.com/103704890/227804055-d9c3ccce-ef8d-427d-a6b6-e1db12eba440.mp4 Important to note for the final project, many of the configurations that were found to work in the GPS 3 autonomous laps were used for the final project. The GPS points making up the path were 0.55 meters apart. The nature of the csv points used in the final project matched the one used for this assignment. This allowed the project to move faster as the configurations did work out. See section Algorithmic Design of Directing Car from GPS point A to B.</p>"},{"location":"win23team7/#final-project","title":"Final Project","text":"<p>Plan: Go from point A to B with object detection. </p> <p>Overview Originally, we were going to have LIDAR detect objects in front of the car so the car can see what it needs to go around. However, we did not have enough time to see how to use LIDAR, since there was no assignment with it and not enough time at end of quarter given rain. We, however, got quite experienced with the GPS functionality of the car. Using the GPS modules, we were able to direct the car using a path of car-readble GPS coordinates.</p> <p>Algorithmic Design of Directing Car from GPS point A to B By the end of the quarter, we wrote a program, AtoB.py, which requires two sets of GPS coordinates as inputs. This program generates a .csv file containing a path of GPS coordinates about .55 meters apart from point A to B. These generated points latitude longitude formatted and relative to the base station antenna. The inputted absolute (planet's) GPS coordinates, thus, needed to be translated to these relative coordinates and converted to rectangular from polar positions. It took some testing to increase the precision of the translation as we did not know the exact conversion constants.</p> <p>Demonstration</p> <p>https://user-images.githubusercontent.com/103704890/227807009-ec59e649-c068-4543-9dba-9ddb16ea8ee5.mp4</p>"},{"location":"win23team8/","title":"Team 8 Final Project Proposal","text":""},{"location":"win23team8/#team-members","title":"Team Members","text":"<p> - Youssef Georgy | Electrical &amp; Computer Engineering - Rizzi Galibut | Mechanical &amp; Aerospace Engineering - Shuhang Xu | Computer Science - Kavin Raj | Cognitive Science w/ Emphasis in Machine Learning</p> <p>Such a lovely team!!</p>"},{"location":"win23team8/#project-overall","title":"Project Overall","text":"<p>A waiter-bot that takes visual input from the camera, navigates autonomously to different specified locations (i.e., tables) and then back to the starting point. Use image-detection via camera to give robocar a location or GPS coordinates to navigate  to, then use GPS data to plot path there and avoid any obstacles in the way. The robocar will be able to take any location given (provided it\u2019s in range of the network connection) and determine how to get there</p>"},{"location":"win23team8/#physical-setup","title":"Physical Setup","text":""},{"location":"win23team8/#gantt-chart","title":"Gantt Chart","text":""},{"location":"win23team8/#demonstration","title":"Demonstration","text":""},{"location":"win23team8/#using-depthai-for-text-recognition","title":"Using DepthAI for text recognition","text":"<p>Accomplished: - Used DepthAI to enable the camera to detect numbers which are associated with different tables (e.g., 001, 002, etc.) and different CSV files - Originally started by looking at OpenCV and Tesseract for OCR - These are not SpatialAI platforms so accomplishing what we were trying to do would be much harder - Default code provided in DepthAI library launched windows that displayed video stream and words detected, which worked when directly connected to camera through host computer but running code through Jetson would require a container to launch these windows - Needed to find a way to disable them and have camera run in the background Default code also rewrote the variable that contained the decoded text each time it detected something - Had to rewrite and remove sections of the code to stop detection once certain prompts are given - Light conditions really mattered when showing prompt to camera - Worked better during the day and when shown prompt on a backlit-screen (with white background)</p> <p>What did't work as expected: - Camera could read \u201c001\u201d but not \u201c1\u201d, etc., so we decided to use a set of numbers instead - Number recognition is much more accurate than word recognition so we decided to use numbers as the prompt</p>"},{"location":"win23team8/#using-gps-to-record-different-paths","title":"Using GPS to record different paths","text":"<ul> <li>GPS navigation code is based on the USCD donkeycar GPS library</li> <li>We are trying to make it stay in auto-pilot mode by default and reset the origin in the beginning</li> <li>Depend on what text is detected with DepthAI, make it move following the pre-recorded path</li> <li>When it moves back to the origin, stop and start detecting next text</li> </ul>"},{"location":"win23team8/#desired-but-not-accomplished","title":"Desired but not accomplished","text":"<ul> <li>Include the ability to restart the process once the waiter-bot returned to the starting point without having to manually run the script again</li> <li>Have waiter-bot stay at the table for a certain amount of time before returning to the starting point</li> <li>Incorporate the LiDAR so the waiter-bot can avoid obstacles (e.g., students walking past) while traveling to tables</li> <li>Could also be used to detect when the waiter-bot reaches the table and ensure it doesn\u2019t crash into it</li> <li>Have controller inputs programmed in so the waiter-bot can be truly autonomous.  Currently still requires human input</li> </ul>"},{"location":"win23team9/","title":"Team 9","text":"ECE/MAE 148 Winter 2023 Team 9 <p>     JetBuddy     Indoor Delivery Bot based on DepthAI, OpenCV and LiDAR </p> Table of Contents <li>Team Members</li> <li> Hardware and Schematics <ul> <li>Parts</li> <li> Schematics</li> </ul> </li> <li>Final Project</li> <ul> <li>Abstract</li> <li>Part 1: Human Detection and Following with Depthai and PyVesc</li> <li> Part 2: Stopping Mechanism with Lidar</li> <li> Part 3: Facial Recognition</li> <li> Part 4: Spatial Detection with DepthAI</li> </ul> <li>Reflection</li> <ul> <li>Challenges</li> <li> Potential Improvements</li> </ul> <li>Presentation Files</li> <li>Reference</li>"},{"location":"win23team9/#team-members","title":"Team members","text":"<ul> <li>Ben Zhang (ECE)</li> <li>Joseph Katona (ECE) </li> <li>Yichen Yang (ECE)</li> <li>Zijian Wang (MAE)</li> </ul>"},{"location":"win23team9/#hardware","title":"Hardware","text":""},{"location":"win23team9/#parts","title":"Parts","text":""},{"location":"win23team9/#full-assembly","title":"Full Assembly","text":""},{"location":"win23team9/#mounting-plate","title":"Mounting Plate","text":""},{"location":"win23team9/#jetson-case","title":"Jetson Case","text":""},{"location":"win23team9/#camera-lidar-mount","title":"Camera LiDAR Mount","text":""},{"location":"win23team9/#final-project-delivery-box","title":"Final Project Delivery Box","text":""},{"location":"win23team9/#schematics","title":"Schematics","text":""},{"location":"win23team9/#wire-diagram","title":"Wire Diagram","text":""},{"location":"win23team9/#final-project","title":"Final Project","text":""},{"location":"win23team9/#abstract","title":"Abstract","text":"<p>This project aims to develop a delivery system for our robocar that can detect and follow humans while also incorporating a stopping mechanism to prevent collisions. Additionally, the robot will utilize facial recognition to identify individuals and personalize interactions.</p>"},{"location":"win23team9/#part-1-human-detection-and-following-with-depthai-and-pyvesc","title":"Part 1: Human Detection and Following with Depthai and PyVesc","text":"<p>The OAKD camera will be used to detect and track humans in the robot's vicinity. The PyVesc motor controllers will then be used to move the robot in the direction of the detected human.</p>"},{"location":"win23team9/#required-components","title":"Required Components","text":"<ul> <li>Tiny-Yolov3 model integrated in DepthAi for object detection</li> <li>PyVesc Python package for robocar control</li> </ul>"},{"location":"win23team9/#algorithm-workflow","title":"Algorithm Workflow","text":"<ul> <li>Use Tiny-Yolov3 to detect the bounding box of the person in the OAKD camera's field of view.</li> <li>Determine the position of the person by finding the central line of the bounding box, and denote the x-axis value as x0.</li> <li>Calculate the error between the central line of the frame (416x416 pixels), e = x - x0.</li> <li>Calculate the steering value using the formula: v = (Kp * e) / 416 + 0.5, where Kp = 1.</li> <li>Use PyVesc to control steering by calling vesc.set_servo(v).</li> </ul>"},{"location":"win23team9/#additional-settings","title":"Additional Settings","text":"<ul> <li>Use vesc.set_rpm() to run the car once it detects people.</li> <li>The steering value is sampled at a rate of 5Hz to prevent frequent drifting.</li> </ul>"},{"location":"win23team9/#part-2-stopping-mechanism-with-lidar","title":"Part 2: Stopping Mechanism with Lidar","text":"<p>The Lidar sensor will be used to detect obstacles in the robot's path. If an obstacle is detected, the robot will stop moving and wait for the obstacle to clear before continuing on its path.</p>"},{"location":"win23team9/#the-lidar-on-this-robot-aim-to","title":"The LiDAR on this robot aim to","text":"<ul> <li>Detect anything that is in a close range</li> <li>If the position is too clase, the robot will stop to avoid collision</li> <li>The robot will back up after it stop for a while and still detect obstacle is close</li> <li>Transform raw binary data from LiDAR to numerical data through BinASCII library</li> </ul>"},{"location":"win23team9/#how-to-read-lidar","title":"How to read LiDAR?","text":"<ul> <li>Each measurement data point of LiDAR is consists of a distance value of 2 bytes and a confidence of 1 byte</li> <li>We transform this data through chopping it to bytes and translate it.</li> <li> <p>We get the angle by getting the start angle and end angle.</p> </li> <li> <p>Putting all the distance into a list and it will stop the car if there\u2019s an object within certain distance that LiDAR detected.</p> </li> </ul> <p></p>"},{"location":"win23team9/#part-3-facial-recognition","title":"Part 3: Facial Recognition","text":"<p>The robot will be equipped with a facial recognition system, using a webcam, that will allow it to identify individuals and personalize interactions. Once it recognizes the right person, the delivery box will open. The facial recognition software uses a simple python import of facial_recognition. In the facial_recognition library all we do is use openCV to capture images for the frames and use facial_recognitions \"matching\" function to to add a box around the persons face. In our case when this value is detected over an interval then a true value is then sent to the box to open.</p> <p></p>"},{"location":"win23team9/#part-4-spatial-detection-with-depthai","title":"Part 4: Spatial Detection with DepthAI","text":"<p>Utilizing Depthai's pipeline system we take their spatial location pipeline to simply calculate the distance of individual from the camera. The Object detection pipeline detects a person and creates a bounded box, then with the x and y coordinates from the bounded box we can pinpoint where we want the camera to point. After these coordinates are gathered the z location is stored in a circular list. This is because the bounded box and tracker of object distance aren't always in sync so some erroneous values are given. Once we have around 50 samples then we take the average to get a good idea of what the distance of the person from the car is. Finally we utilize pyvescs set_rpm() features to give out a more smooth acceleration system. So, basically if you're far away the robot will speed up and slow down as it moves closer to you.  Get more info on Spatial Depth here </p>"},{"location":"win23team9/#gantt-chart","title":"Gantt Chart","text":""},{"location":"win23team9/#demonstrations","title":"Demonstrations","text":"<p>The Video might not show up, please go to img folder for full demo.</p> <p> </p>    Your browser does not support HTML video."},{"location":"win23team9/#reflection","title":"Reflection","text":""},{"location":"win23team9/#challenges","title":"Challenges","text":"<ul> <li>Getting everything to work together<ul> <li>Different libraries working together and all send signals to PyVESC</li> <li>Everything worked fine on a local machine but when running on the Jetson, crashes would occur</li> </ul> </li> <li>Scope of the original idea<ul> <li>Mapping the path for future references using SLAM</li> </ul> </li> <li>Depth ai pipeline caused crashes<ul> <li>X-Link Problem(Serial bus issues)</li> </ul> </li> <li>Translate raw LiDAR output to data we need</li> <li>Making the car look smooth</li> <li>Better algorithm to adjust speed(rpm)</li> </ul>"},{"location":"win23team9/#potential-improvements","title":"Potential Improvements","text":"<ul> <li>Implement all the features together flawlessly<ul> <li>Currently cannot run together good due to delay from different components</li> </ul> </li> <li>Get the locking mechanism working<ul> <li>Locking mechanism to make sure the right receiver get the package</li> </ul> </li> <li>LiDar also scans the path for future path planning<ul> <li>Trying to find a person if it cannot detect anything</li> </ul> </li> </ul> <p>Maybe try different frameworks since we can use different libraries without limitation in ROS or donkeycar</p>"},{"location":"win23team9/#presentations","title":"Presentations","text":"<li>Project Proposal &amp; Progress Report</li> <li>Final Presntation</li>"},{"location":"win23team9/#reference","title":"Reference","text":"<p>We would like to give special thanks to:</p> <ul> <li>Professor Jack Silberman</li> <li>TA Moises Lopez</li> <li>TA Kishore Nukala</li> <li>All The teams that helped us on the way</li> </ul>"},{"location":"win23team12/","title":"ECE 148 Winter 2023 Team 12 Final Project","text":""},{"location":"win23team12/#team-members","title":"Team members","text":"<ul> <li>Jake Kindley (ECE)</li> <li>Yiteng Zhao (ECE)</li> <li>Noah Jones (MAE)</li> </ul>"},{"location":"win23team12/#overview","title":"Overview","text":"<p>We want to create a sorter bot similar to warehouse bots that picks up a package, follow the path with certain color and delivers it to designated dropoff location based on the labels on that package. We plan to put AR tags as labels on the package, and each AR tag is mapped to an id associated with either the color of the lane our bot should follow or stop action when it reaches the designated dropoff zone. At dropoff zone, the bot will perform a series of maneuver and return to the starting point.</p>"},{"location":"win23team12/#robot-design-implementation","title":"Robot Design &amp; Implementation","text":""},{"location":"win23team12/#software","title":"Software","text":"<p>Our code is running on ROS2 and is modified based on the provided lane following code from class. The following flowchart shows the original relationship between each component in the provided lane following code from class: </p> <p>We added a node responsible for detecting AR tag, read states from AR tag, and changing lane detector's behavior. </p> <p>In our new node, we have defined two types of AR tags:</p> <ul> <li>Type 1: Declares the lane color our bot should follow</li> <li>Type 2: Stop signal that indicates the bot to drop the package</li> </ul> <p>The default behavior when no AR tag is being detected is following blue lane until a type 1 tag is detected.</p> <p></p>"},{"location":"win23team12/#hardware","title":"Hardware","text":"<p>We designed a bracket as package holder that snaps on the front bumper of the car and holds one package:</p> <p></p> <p></p>"},{"location":"win23team12/#showcase","title":"Showcase","text":""},{"location":"win23team12/#assembled-robots","title":"Assembled Robots","text":""},{"location":"win23team12/#final-project-demo","title":"Final Project Demo","text":""},{"location":"win23team12/#remarks","title":"Remarks","text":"<p>Our video demonstrated our bot's capability of detecting AR tag, selecting the corresponding lane color, following path to dropoff zone, dropping off package in front of stop sign, and return to the starting point. Potential improvements for this project includes adding obstacle avoidance, redesigning the package holder to hold the package above ground, adjusting camera position for better view angle of the lane, and adding capability to navigate through multiple junctions.</p>"},{"location":"win23team13/","title":"Team 13","text":""},{"location":"win23team13/#final-project-robot-mapping","title":"Final Project: Robot Mapping","text":""},{"location":"win23team13/#team-13-girish-muhammad-andy-and-van","title":"Team 13: Girish, Muhammad, Andy, and Van","text":""},{"location":"win23team13/#ece-mae-148-winter-2023","title":"ECE MAE 148, Winter 2023","text":"<p>Welcome to the project report for Team 13! This page contains a report of all the progress we made throughout this busy and fun quarter, including our final project.</p> <p>Team 13's assembled RC Car with the lidar placed at the front.</p>"},{"location":"win23team13/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Final Project: Robot Mapping         - Team 13: Girish, Muhammad, Andy, and Van         - ECE MAE 148, Winter 2023</li> <li>Table of Contents</li> <li>The Team</li> <li>Final Project Abstract</li> <li>Hardware Setup<ul> <li>Base Plate</li> <li>Camera Mount</li> <li>Jetson Nano Case</li> <li>Electronics Circuit Diagram</li> </ul> </li> <li>Software Documentation</li> <li>Autonomous Laps</li> <li>Acknowledgements</li> <li>Credit and References</li> </ul>"},{"location":"win23team13/#the-team","title":"The Team","text":"Girish Krishnan [LinkedIn] Muhammad Bintang Gemilang Andy Zhang Zhengyu (Van) Huang Electrical Engineering Mechanical Engineering Electrical Engineering Computer Engineering"},{"location":"win23team13/#final-project-abstract","title":"Final Project Abstract","text":"<p>Our final project was themed around mapping an unknown environment. Our project involved the following tasks.</p> <p>What we promised</p> <ul> <li>[\u2714] To implement SLAM (Simultaneous Localization and Mapping) using a lidar. This effectively creates a map of the environment around the robot, showing the locations of all objects present.</li> <li>[\u2714] To display the map generated from SLAM in real-time using a web application.</li> </ul> <p>The challenges faced during the project were:</p> <ul> <li>Integrating the web application for live previewing (HTML/CSS/JS) with the Python code needed to run SLAM.</li> <li>Avoiding delays in the updating map.</li> </ul> <p>The accomplishments of the project were:</p> <ul> <li>We were able to achieve a decent visualization that updates over time as the robot is driven around</li> <li>The visualization/map can be saved easily for tasks such as path planning.</li> </ul> <p>Final Presentation</p> <ul> <li> <p>Link to Final Presentation</p> </li> <li> <p>Link to video showing real-time mapping</p> </li> </ul> <p>Weekly Update Presentations</p> <ul> <li>Project Proposal</li> <li>Week 8</li> <li>Week 9</li> <li>Week 10</li> </ul> <p>Gantt Chart</p> <p></p>"},{"location":"win23team13/#hardware-setup","title":"Hardware Setup","text":"<ul> <li>3D Printing: Camera Mount, Jetson Nano Case, GPS (GNSS) Case.</li> <li>Laser Cutting: Base plate to mount electronics and other components.</li> </ul> <p>Parts List</p> <ul> <li>Traxxas Chassis with steering servo and sensored brushless DC motor</li> <li>Jetson Nano</li> <li>WiFi adapter</li> <li>64 GB Micro SD Card</li> <li>Adapter/reader for Micro SD Card</li> <li>Logitech F710 controller</li> <li>OAK-D Lite Camera</li> <li>LD06 Lidar</li> <li>VESC</li> <li>Anti-spark switch with power switch</li> <li>DC-DC Converter</li> <li>4-cell LiPo battery</li> <li>Battery voltage checker/alarm</li> <li>DC Barrel Connector</li> <li>XT60, XT30, MR60 connectors</li> </ul> <p>Additional Parts used for testing/debugging</p> <ul> <li>Car stand</li> <li>USB-C to USB-A cable</li> <li>Micro USB to USB cable</li> <li>5V, 4A power supply for Jetson Nano</li> </ul>"},{"location":"win23team13/#base-plate","title":"Base Plate","text":"<p>All measurements shown above are in millimeters (mm)</p> <p>Our base plate was laser cut on a thick acrylic sheet. The circular hole at the end of the base plate is meant to hold the power on/off button. The long holes in the side of the plate are meant for wires to easily pass to and from the bottom of the plate.</p>"},{"location":"win23team13/#camera-mount","title":"Camera Mount","text":"Camera Holder Base for attachment to base plate <p>The two parts of the camera mount shown above were screwed together. The angle of the camera was carefully chosen (facing downward approximately 10 degrees from the vertical) so that the road ahead is clearly visible. This is essential for accurate results in OpenCV/ROS2 autonomous laps.</p> <p>One of our older camera mount designs is shown below.</p> <p>This camera mount consists of three parts: one base for attachment to the base plate, one middle piece to connect the base and the camera, and the camera holder. This camera mount design allows you to rotate the camera up and down. However, it is important that the rotating hinge is screwed securely so that the hinge doesn't wobble out while the robot does autonomous laps!</p>"},{"location":"win23team13/#jetson-nano-case","title":"Jetson Nano Case","text":"<p>Credit to flyattack from Thingiverse, see: https://www.thingiverse.com/thing:3532828</p> <p>This case is excellent because it is robust and doesn't break easily, unlike most common Jetson Nano cases.</p>"},{"location":"win23team13/#electronics-circuit-diagram","title":"Electronics Circuit Diagram","text":"<p>Note: some of these components and connections will vary depending on the exact components you have - check the component specifications carefully.</p>"},{"location":"win23team13/#software-documentation","title":"Software Documentation","text":"<p>To install all the necessary Python modules needed, run the following on the Jetson Nano.</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>For our final project, we implemented a real-time visualization system for the Hector SLAM algorithm implemented using the lidar sensor. The base code for the SLAM algorithm is accessible in the Docker container provided to us in class, and the code for the real-time implementation is present in the slam_gui folder of this repository.</p> <p>The SLAM real-time visualization GUI that we built has the following features:</p> <ul> <li>A web application whose routes are made using FastAPI in Python. Uvicorn is used to run the web server.</li> <li>HTML and JS to update the map in real-time</li> <li>The HTML and JS is interfaced with Python, ROS1, ROS2 and ROSBridge, so that the data collected is displayed on the web app.</li> <li>The interfacing process is difficult to implement directly in Python, so we use subprocessing to call relevant bash scripts that handle the processes in ROS1 and ROS2. These subprocesses are made to run in parallel using threading in Python.</li> </ul> <p>To run the visualizer, first open up a docker container containing the ucsd_robocar ROS packages.</p> <p>Run the following:</p> <pre><code>cd slam_gui\npython slam_map.py\n</code></pre> <p>This sets up the web app running on the Jetson Nano (although the app could potentially be run on any device, provided it can communicate with the Jetson Nano using the relevant ROS topics).</p> <p>Opening up the web app on http://localhost:8000 reveals the GUI showing the results of SLAM in real-time. The code in slam_map.py can be adjusted to fine-tune the time-delay that occurs as the map updates.</p> <p></p> <p>Additional Scope for the Final Project</p> <p>Although SLAM is useful for mapping an unknown environment, it can be useful to integrate GPS data with SLAM to provide better location accuracy. To implement this in Python, we created the folder gps_slam that contains starter code with lidar, PyVESC, and GPS implementation and a basic SLAM algorithm with the Kalman filter (implemented using the filterpy library in Python). This additional, nice-to-have part of the project hasn't been tested out yet, but we plan to get it working soon.</p>"},{"location":"win23team13/#autonomous-laps","title":"Autonomous Laps","text":"<p>As part of the class deliverables and as preparation for the final project, here are our autonomous laps videos:</p> <p>Donkey Sim</p> <ul> <li>Local Computer: https://youtu.be/lXEStSEVikQ</li> <li>GPU training: https://youtu.be/4_BzKP9-XAQ</li> <li>External Server: https://youtu.be/Yvo1yqRJhX4</li> </ul> <p>Physical Robot</p> <ul> <li>DonkeyCar: https://youtu.be/bPUSS2g0Ves</li> <li>Lane detection using OpenCV + ROS2: https://youtu.be/omcDCBSrl2I</li> <li>Inner lane: https://youtu.be/9hN8HUlGcas</li> <li>Outer lane: https://youtu.be/nXZNPscVlX0</li> <li>GPS: https://youtu.be/Y3I9AWW1R6o</li> </ul>"},{"location":"win23team13/#acknowledgements","title":"Acknowledgements","text":"<p>Thanks Prof. Jack Silberman and TAs Moises Lopez and Kishore Nukala for an awesome quarter! See you in DSC 178 next quarter, professor Jack ;)</p>"},{"location":"win23team13/#credit-and-references","title":"Credit and References","text":"<ul> <li> <p>Jetson Nano Case Design: https://www.thingiverse.com/thing:3532828</p> </li> <li> <p>Lidar (LD06) Python Tutorial: https://github.com/henjin0/LIDAR_LD06_python_loder</p> </li> <li>PyVESC: https://github.com/LiamBindle/PyVESC</li> <li>SLAM tutorial, Dominic Nightingale. https://gitlab.com/ucsd_robocar/ucsd_robocar_nav1_pkg/-/tree/master/</li> </ul>"},{"location":"win23team14/","title":"Team 14","text":"<p>ECE - MAE 148 Team 14 Winter 2023 Repository</p> <p>Project Goal: </p> <p>We used the OAK-D camera to run object detection (within the camera) and track different traffic details (common signs, speed limits, etc.)</p> <p>Software and Hardware Description:</p> <p>The OAK-D and depthAI are AI-enabled stereo cameras that allow for depth perception and 3D mapping. They are powerful tools for computer vision applications, such as object detection and tracking. The depthAI is a board that enables faster processing of images by offloading the computational workload from the main processor to dedicated hardware.  YOLO (You Only Look Once) is a real-time object detection system that is capable of detecting objects in an image or video feed. It is a popular deep learning model that is used in many computer vision applications, including self-driving cars and robotics. PyVesc is a Python library for controlling VESC-based motor controllers. The VESC is an open-source ESC (Electronic Speed Controller) that is widely used in DIY robotics projects. PyVesc allows for easy communication with the VESC and provides an interface for setting motor parameters and reading sensor data.</p> <p>Project Overview:</p> <p>We first used the OAK-D and depthAI to detect stop signs in the robot's field of view. Then, we executed the deep learning model YOLO to process the camera feed and identify the stop sign(text detection can be another method to achieve the same function). Once the stop sign is detected, we implemented PyVesc to send a command to the motor controller to stop the robot and started to set up the OAK-D and depthAI cameras by installing the necessary software libraries. YOLO is capable of detecting multiple objects simultaneously, so we needed to filter out the stop sign from other detected objects. However, we needed a blob converter to take different data types and convert them into a Binary Large Object (BLOB) that could fit in our code. Finally, once the stop sign is detected, we accessed PyVesc to send a command to the motor controller to stop the robot. In summary, the integration of OAK-D, depthAI, YOLO, and PyVesc allows for efficient and accurate stop sign detection and safe stopping of the robot. This implementation can be further customized and optimized for specific robotic platforms and use cases.</p> <p>Final Projet Presentation: </p> <p>https://docs.google.com/presentation/d/1BTMwfktHvDzfzYd6oSnHeaQTEBbtYg8wEMnqoWkamHE/edit?usp=sharing</p> <p>Final Project Video:</p> <p>https://drive.google.com/file/d/1OnO5qWczQbH_aVrgKAtejwLMw6-fFSRW/view?usp=share_link</p> <p>Team Members: Anish Kulkarni, Manuel Abitia, Zizhe Zhang.</p> <p>Special Thanks to: Professor Silberman, Kishore, Moises, and Freddy C. the Robot.</p>"},{"location":"win23team15/","title":"Final Project Repository for Team 15 of the 2023 Winter Class MAE ECE 148 at UCSD","text":"<p>Our Final Project uses the AI controlled autonomous vehicle developed in early course sections to implement a driving protocol based on hand signals. To do this, we utilize the GPS-tracking and following code developed/given in class, and combined it with the DepthAI gesture recognition software pack. Our idea developed from our interest in the OAK-D Lite camera\u2019s stereo vision system and ability to run DepthAI within the camera for processing. To use both of these functions, we chose to run a hand detection program that uses Google Mediapipe to combine these features in a unique project that no one on our team had tried before. Using this bleeding edge hardware was really interesting and showed the potential in the given components and also in general the viability of gesture-based control even in relatively low-cost projects. </p> <p>Below are examples of how our hand detection tracker will recognize our gestures </p> <p> </p>"},{"location":"win23team15/#car-physical-setup","title":"Car Physical Setup","text":"<p>Our car setup used a large piece of acrylic to connect across the RC car strut towers to support all of our electronics. The acrylic had two grooves along the entire length that were 3 inches apart, allowing us to reconfigure our electronics layout as the class progressed. We knew we would be given different devices through the length of the class, so this modular setup gave us the ability to adapt to them quickly rather than redesigning everytime. We also made heave use of 3D printing, which further cemented the viability of our acrylic rail mount system, since we just had to design a mount that could take mounting screws three inches apart. This enabled us to quickly and with low effort include different configurations and components. We then further used 3D-printed components to functionalize parts of our design, such as a height-adjustable camera mount, and a lidar mount employing the same functionality.</p> <p> </p> <p>Our camera setup was originally a rigid mount on the front of the car, but as we tested our initial Donkey Car laps on the outdoor track we recognized the need for adjusting our angle to detect the lines better. This led to our tall hinging mount that gave us better viewing angles that we could set up within seconds for testing.</p> <p> </p>"},{"location":"win23team15/#software","title":"Software","text":"<p>In total, we used three different software packages either from discord, or developed in the course of the class, those beeing the depthai software pack, the d5 GPS control system and the donkeycar Software.</p>"},{"location":"win23team15/#depthai-software-pack","title":"depthai software pack","text":"<p>To detect hand gestures, we use the Luxonis DepthAI Software on the Luxonis OAK-D stereo-vision camera. This software pack brings plenty of functionality, from simple hand tracking, two two-handed interactions, all the way to gesture recognition. We then modified the existing codebase to suit our needs, including specialized code to regognize a set of largely custom, non-included hand gestures:  - Thumbs-up gesture to activate driving - Held up flat hand as a stop signal  - different number of fingers (with the thumb not pointing outwards) changes the speed in a four level control scheme - pointing the index finger with thumb outstreched left or right to change direction of steering </p>"},{"location":"win23team15/#d5-gps-control","title":"d5 GPS-control","text":"<p>In the d5 subdirectory, we used the donkeycar interfaces to program a simple GPS training and following routine to generate a path for the car to follow. This part of the project is still partly in development, as currently only the speed can be varied with hand signals. We largely reused tech generated and tought to us in the course of the class, providing a nice framework on which to test our system.</p>"},{"location":"win23team15/#donkeycar-software","title":"DonkeyCar Software","text":"<p>We use the preconfigured DonkeyCar software package to facilitate control and interfacing of the Jetson nano embedded system with the rest of the RC car. We implemented a special interrupt in the signal chain to be able to pass in different speed values, which are first stored by the modified DepthAI software pack into the file finalproject/comm.txt as a simple string command corresponding to the recognized hand sign. which is then read into the DonkeyCar VESC control subroutine to change the throttle values dynamically. </p>"},{"location":"win23team15/#operation","title":"Operation","text":"<p>One can either run the depthai software on its own with the VESC class implemented right in the file demo_bdf.py by just executing it with all lines commented in, or, if it is desired to just utilize the throttle values, comment out the VESC related code, run the demo_bpf.py and then simultaneaously run the d5 donkeycar code with <code>./python manage.py</code> drive. Then, steering will be controlled by the GPS following algorithm, while the speed is controlled via the detected hand signals. The follwing youtube videos show the basic functionality of our project, and how it interacts with the complex drivetrain of the car.</p> <p>Basic Command Gesture</p> <p>Throttle Changing Values</p>"},{"location":"win23team15/#team-15-boba-team","title":"Team 15: Boba Team","text":"<ul> <li>Reinwei Bai</li> <li>Manu Mittal</li> <li>Reisandy Lamdjani</li> <li>Moritz Wagner</li> </ul>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/","title":"UCSD Robocar Using RC Controller","text":""},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#arduino-micro-pro-leonard-compatible-22mar23-v20","title":"Arduino Micro Pro - Leonard Compatible 22Mar23 - V2.0","text":""},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#expresslrs-24ghz-elrs-radio-long-range-digital","title":"ExpressLRS 2.4GHz (ELRS) Radio Long Range Digital","text":"<p>#Here are the radios we have for UCSD evGKart</p> <p>#There radios use the open source ELRS</p> <p>#There are several advantages on ELRS: Long Range Low Latency, several suppliers, software upgradable</p> <p>#There are few options for radios. We got the radios below because they were compacts, seemed rugged, and received good reviews</p> <p>#These radios will allow us to keep it all digital without the need to use PWM/PPM. ex: Radio UART - &gt; Single Board Computer (SBC).</p> <p>We will use a microcontroller (MCU) to translate near real-time the protocol used on the radios into serial communication with a SBC using USB and also use the MCU for the emergency stop (off) [EMO] directly from the radio command vs. using separate radio for the EMO.</p> <p>Since the radio for PPM/PWM was affordable we have one too in case we want to directly control RC Cars and or use a multiplexer as part of our EMO. This would require two PWM like cables for each radio channel used.</p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#betafpv-literadio-3-pro-radio-transmitter-elrs-supports-external-nano-tx-module","title":"BetaFPV LiteRadio 3 Pro Radio Transmitter- ELRS (Supports External Nano TX Module)","text":"Links Image https://betafpv.com/collections/tx/products/literadio-3-pro-radio-transmitter  https://support.betafpv.com/hc/en-us/articles/5987468200601-Manual-for-Lite-Radio3-Pro   https://www.getfpv.com/betafpv-literadio-3-pro-radio-transmitter-elrs-supports-external-nano-tx-module.html"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#mateksys-expresslrs-24ghz-receiver-elrs-r24-d","title":"MATEKSYS ExpressLRS 2.4GHz Receiver - ELRS R24 D","text":"Links Image https://www.getfpv.com/mateksys-expresslrs-2-4ghz-receiver-elrs-r24-d.html"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#mateksys-expresslrs-24ghz-receiver-pwm-elrs-r24-p6","title":"MATEKSYS ExpressLRS 2.4GHz Receiver - PWM ELRS-R24-P6","text":"Links Image https://www.getfpv.com/mateksys-expresslrs-2-4ghz-receiver-pwm-elrs-r24-p6.html"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#upgrading-the-elrs-firmware","title":"Upgrading the ELRS firmware","text":"<p>24Mar23</p> <p>#Get back to revise these to show version  3.2.0 again.</p> <p>I had to use 3.2.0 because it is the version that ELRS configurator has for the receiver. It seems I need to keep the ERLSv2.luna vs3 for the radio to read the luna script. I hope they will fix it in the future.</p> <p>_#There is a good software that helps on on the upgrade  https://www.expresslrs.org/quick-start/installing-configurator/</p> <p>#They support Windows, Mac, and Linux</p> <p>#Ideally you upgrade the radios Tx and Rx on the same session to make sure they have the same firmware version</p> <p>#Also if you have multiple of the same Tx and Rx doing them consecutively can save you time and help reduce the risk of having different firmware versions.</p> <p>#For the BETA FPV 3 PRO  https://www.expresslrs.org/quick-start/transmitters/betfpvlr3pro/</p> <p>#Turn on the TX radio #Plum the USB C cable connected to your computer #The radio will ask what connection to use #Select USB Serial (Debug) #You will need to look for the Radio in the connections options</p> <p>#Version 2.5.2 </p> <p>#Build &amp; Flash #The first time it will take a while to download, install tools and build the firmware</p> <p> </p> <p>#Download and save on your computer the the LUA Script file #Unplug the USB cable, turn off the radio, turn on again #To upload the LUA script to the radio, unplug the USB cable if connected, connect it again #Select USB Storage (SD) #Then you can use your computer to upload the LUA script you saved earlier https://www.expresslrs.org/quick-start/transmitters/lua-howto/  Download the ELRSv3 Lua Script (you can simply right-click, save-as) into your radio's SD Card under the Scripts/Tools  #elrsV2.lua #Also, let's delete the older .lua script from the root of the DISK_IMG of the radio #Remove the USB cable from the radio #Hold the right top joystick to the left for 1~2 seconds #Tools #EspressLRS #See if the configuration loads, here is where I have problems with V3.x for some problem. V2.5.2 works just fine #The BetaFPV LiteRadio 3 Pro Radio Transmitter uses the two smaller joystick to navigate the configuration menu Moving the right side change the GUI on the display Moving the right side to the left and holding it for few seconds get you into the settings</p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#receiver-matekys-r24-d-24ghz","title":"Receiver MATEKYS R24-D 2.4Ghz","text":"<p>https://www.expresslrs.org/quick-start/receivers/matek2400/   If this is the first time you're flashing/updating your receiver or you're updating it from a previous 2.x firmware via WiFi, first ensure that it has version 2.5.2. Once it has the 2.5.2 flashed, update to 3.x.  #Connect to the Wifi Network the receiver has created. It should be named something like ExpressLRS RX with the same expresslrs password as the TX Module Hotspot. #After the Rx radio boots, wait to see the LED flashing quick. That is an indicadion that its Access Point and web server is running. #Using a web browser http://10.0.0.1/  #Connect to the website of the device and upload new firmware  Then if you connect the receiver to your local WiFi you can get to it by usings its IP address or name. You can scan the network and look for a device called elrs_rx  ex:  http://elrs_rx.local  #Because we will be on the field and not necessarily close to WiFi, let\u2019s leave the AP of the radio on so we can configure it. Not very safe since someone can connect to it while the AP is on. We will check how to protect it with a  password on it a bit later. Moreover, I did not see how to name the radios, it would be hard to know what Rx radios is what in the network. #Let's get the Rx to be on 115200 baud rate #Just connect to the webinterface of the Rx and set the baudrate #We need to use a USB to TTL cable - example from Amazon #Or you can use the embedded WiFi, how cool is that?  Lets try the UART first</p> <p>Red wire of the USB to TTL = + Black wire of the USB to TTL = - Green = White = </p> <p>#Press and hold the boot button while connecting the USB to TTL device to your computer  http://www.mateksys.com/?portfolio=elrs-r24#tab-id-3 For ELRS-R24-D, if update from 2.x to 3.x, Pls select target MATEK 2400 RX R24D and click on Force Flash https://github.com/kkbin505/Simple_RX</p> <ul> <li> <p>Readme.md</p> </li> <li> <p>CRSF decode library for arduino atmega32u4/328p.</p> </li> <li> <p>Based on arduino SBUS decode, modified to decode crsf protocol from elrs receiver</p> </li> </ul> <p>#Let's keep it simple first just by using Arduinos, then we will try a Raspberry PI Pico, and then UCSD DRTC if we want to make everything CAN https://www.amazon.com/gp/product/B09C5H78BP/ref=ppx_yo_dt_b_asin_title_o01_s02?ie=UTF8&amp;psc=1 #I got this coming too so we use a better USB connector, USB C vs. uUSB. #I just did not see there was a version with USB C earlier. https://www.amazon.com/gp/product/B0BCW67NJP/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&amp;psc=1  https://github.com/kkbin505/Simple_RX  </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#some-background-and-references-elrs-radio-long-range-digital","title":"Some Background and References ELRS Radio (Long Range Digital)","text":"<p>TCIII \u2014 03/17/2023 1:11 PM Hi Jack. I spent half of today learning about how to update (flash) the BETAFPV gamepad/plug-in transmitter/receiver software. Both the gamepad or the plug-in transmitter software version has to be the same as the receiver and vice versa. The gamepad configuration and update flashing can be done with the BETAFPV Configurator while even BETAFPV uses the ELRS Configurator to update the plug-in transmitter and receiver. Nothing like consistency. \ud83d\ude04 The BETAFPV LiteRadio 3 gamepad RC transmitter can output 25, 50, or 100 mw while the plug-in RC transmitter can output 100, 250, or 500 mw and comes with ELRS version 2.01.  The Matek ELRS six channel PWM receiver comes with ELRS version 3.0 while the BETAFPV Nano serial output receiver comes with ELRS version 1.0.0-RC5 so I am going to have to reprogram both receivers with ELRS version 2.01. These LoRa gamepads and receivers are definitely not for beginners. \ud83d\ude32 TCIII \u2014 Yesterday at 7:06 AM Hi Jack. I think that it would be best to converse about the Expresslrs LoRa gamepad project on DM until we get the circuitry and software validated, otherwise we might wind up with DC Users trying to implement a LoRa gamepad system that has not been full vetted for functionality and have substantial issues. TCIII \u2014 Yesterday at 7:25 AM Hi Jack. I bought a couple of Arduino Pro Micros so that I could use the crsf_decode_hid.ino sketch from https://github.com/kkbin505/Simple_RX The sketch compiled just fine after I installed the additional required libraries and I will connect the serial output of the BetaFPV serial output Nano receiver to the Pro Micro and see what kind of output I get using the serial monitor. The Arduino Joystick Library (https://github.com/MHeironimus/ArduinoJoystic+kLibrary) is used by the crsf_decode_hid.ino sketch, but the included sketches in the library are designed to take individual joystick and button inputs connected to the Pro Micro and and produce a HID joystick that can be used by PCs for games.</p> <p>Could you please take a look at the crsf_decode_hid.ino sketch as I understand the debug portion, but I don't see where he is outputting the decoded crsf gamepad joystick/button HID values on the USB port. If the crsf_decode_hid.ino sketch is really making the Nano Receiver crsf output look like a HID gamepad then we have it made, though it does require the Pro Micro to interface the Receiver to the SBC as an HID device. \ud83d\ude42</p> <p>I am using a BetaFPV LiteRadio 3 RC gamepad to do the testing compared to your BetFPV LiteRadio 3 Pro. All of the BetaFPV products are still using Expresslrs version 2.x when Expresslrs version 3.x is now available. The BetaFPV LiteRadio 3 can only be programmed with the BetaFPV Configurator so that gamepad is limited to Expresslrs version 2.x where as your BetFPV LiteRadio 3 Pro can be programmed with the Expresslrs Configurator and can be programmed with Expresslrs 3.x. Remember, both the RC gamepad and the receiver must have the same software version of Expresslrs and pass phrase to bind correctly. GitHub GitHub - kkbin505/Simple_RX: crsf protocol decode for avr crsf protocol decode for avr. Contribute to kkbin505/Simple_RX development by creating an account on GitHub. GitHub - kkbin505/Simple_RX: crsf protocol decode for avr GitHub GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... An Arduino library that adds one or more joysticks to the list of HID devices an Arduino Leonardo or Arduino Micro can support. - GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha... GitHub - MHeironimus/ArduinoJoystickLibrary: An Arduino library tha...</p> <p>This afternoon I will try to determine the voltage of the BetaFPV Nano serial channel receiver output. It will either be 5 vdc or 3.3 vdc. The Nano receiver is powered by 5 vdc, but there is a 3.3 vdc on the Nano receiver pwb because the receiver/processor and WIFI ICs work on 3.3 vdc. So it would seem to me that the serial channel is a 3.3 vdc signal output which will be compatible with either the Rpi or Nano GPIO bus for onboard crsf decoding. Though I like the fact that the Pro Micro, when programmed with the crsf_decode_hid.ino sketch looks just like a HID compliant game controller which makes it portable between RC cars. Additionally the Joystick Descriptor in the crsf_decode_hid.ino sketch allows the customizing of the crsf data stream as to the type of LoRa game controller in use.  TCIII \u2014 Yesterday at 9:53 AM Observations concerning BetaFPV Expresslrs products: 1. Only the BetaFPV LiteRadio 3 Pro can be programmed with the Expresslrs Configurator, unlike the LiteRadio 3 and the LiteRadio 2 SE which require the BetaFPV Configurator, which allows access to the latest Expresslrs version 3.x. 2. The LiteRadio 3 and the LiteRadio 2 SE must programmed with the BetaFPV Configurator because they use a custom version of Expresslrs which is stuck at version 2.x.\ud83d\ude41  3. All of the BetaFPV receivers can be programmed with the Expresslrs Configurator, but keep in mind that both the BetaFPV RC gamepads and receivers must have the same version of the software and pass phrase to bind. 4. Therefore if you are using a LiteRadio 3 or a LiteRadio 2 SE, you cannot upgrade the software of any of the BetaFPV receivers to Expresslrs 3.x or you will lose binding capability. 5. The default baud rate of the BetaFPV Nano serial channel receiver UART is 420000 baud so I had to use the Expresslrs Configurator to change the Nano receiver's UART baud rate to 115200 baud to work with the Pro Micro. 6. Unfortunately the Expresslrs Configurator will only show Expresslrs version 2.5 and up for flashing and the Nano receiver for my purposes needed software version 2.x to update the UART baud rate. 7. Fortunately the Expresslrs Configurator allows access to the Expresslrs software version archives so I could use Expresslrs version 2.0 to update the UART baud rate. \ud83d\ude42   Since I bought the BetaFPV LiteRadio 3, which has an external transmitter port, I also purchased the BetaFPV Nano RF TX Module that can be programmed with the Expresslrs Configurator fortunately. The stock BetaFPV LiteRadio 3 has an output of 100 mw while the BetaFPV Nano RF TX Module can be programmed for outputs of 100, 250, and 500 mw. The higher transmitter output power will obviously result in shortened battery run times and are probably not necessary as 100 mw can be good for over 10 km LOS. \ud83d\ude04 JackSilb \u2014 Yesterday at 10:02 AM You need a USB to TTL to connect it to a Jetson Nano or RPI USB no? Unless you want go direct into the I/Os. TCIII \u2014 Yesterday at 10:06 AM Using the Pro Micro programmed with the crsf_decode_hid.ino sketch allows the BetaFPV Nano serial channel receiver appear to be a HID compliant game controller, like a BT gamepad, to either the Rpi or the Nano without any additional hardware. JackSilb \u2014 Yesterday at 10:06 AM I have the Lite Radio 3 Pro with the cute small display. That will be useful for displaying some info. JackSilb \u2014 Yesterday at 10:07 AM We had something similar for the Teensy too. Stopped using it once we got the VESCs. TCIII \u2014 Yesterday at 10:07 AM That is excellent as you can program any BetaFPV receiver with the latest Expresslrs software. JackSilb \u2014 Yesterday at 10:08 AM Here is the design challenge. If you are using the ELRS radio UART direct to the JTN (Jetson Nano) or RPI and from there USB or Can to the VESC, how can we use the Multiplexer for the EMO. That works only for PWM. On the VESC you can configure a pin for a EMO, we would need a I/O from the Rx radio to bring it 1/0 for EMO direct into the VESC vs. using the JTN or RPI. Ideally, I will find a Rx radio with UART and some I/O capability that I can use a channel, lets say Channel 4 as the EMO switch. JackSilb \u2014 Yesterday at 10:21 AM Maybe this is I can pass the UART info along. Use the PWM for the EMO on/off (1/0) at the VESC TCIII \u2014 Yesterday at 10:31 AM The UART data is in crsf format and will need to be decoded using a Python Program: https://pypi.org/project/crsf-parser/ There are multichannel BetaFPV Expresslrs PWM receivers and I have one: https://www.amazon.com/BETAFPV-ExpressLRS-Compatible-Multirotors-Helicopters/dp/B09WHLJ2GN/ref=sr_1_2?crid=17HV1H415IRJR&amp;keywords=BETAFPV+ExpressLRS+Micro+Receiver&amp;qid=1679420003&amp;sprefix=betafpv+expresslrs+micro+receiver%2Caps%2C107&amp;sr=8-2  PyPI crsf-parser A package to parse and create CRSF (Crossfire) frames, developed primarily to interoperate with ExpressLRS Image BETAFPV ExpressLRS Micro Receiver Support 5 CH PWM Outputs Failsafe... Binding Procedure The Micro receiver comes with officially major release V2.0.0 protocol and has not been set for a Binding Phrase. So please make sure the RF TX module works on officially major release V2.0.0 protocol and no Binding Phrase has been set beforehand. Enter binding mode by plugging ... JackSilb \u2014 Yesterday at 10:42 AM I guess we can do this with ELRS UART -&gt; Arduino or Teensy USB HID -&gt; JTN or RPI  and Arduino or Teensy I/O to the disable pin of the VESC. As long as the Arduino / Teensy code is solid and we have a watchdog we should be good. Adding the MCU in the mix get us going with lots of future opportunities... I need this for our evGoKart ASAP. I need to get some time to play with me. Lots of work(x2) on on the way Do you want me to share with you our work using the Teensy 4.0 including the PCB?  I can send you a PCB too. TCIII \u2014 Yesterday at 10:45 AM I will have more time on Wednesday to test out the Pro Micro setup with the BetaFPV LoRa gamepad and receiver and get back to you with the results. TCIII \u2014 Yesterday at 10:46 AM Address: 11859 SE 91st Circle, Summerfield, FL 34491. JackSilb \u2014 Yesterday at 10:47 AM Can we do the Arduino code to be compatible with the Raspberry Pico? https://www.youtube.com/watch?v=Q97bFwjQ_vQ YouTube Jan Lunge Pi Pico + KMK = the perfect combo for Custom Keyboards Image https://www.youtube.com/watch?v=__QZQEOG6tA YouTube Print 'N Play Use A Raspberry Pi Pico as a HID [Gamepad, Keyboard, Mouse, and Mul... Image Probably a better HW than Arduino Pro Micro TCIII \u2014 Yesterday at 10:56 AM Probably as the Arduino sketch code is C++. JackSilb \u2014 Yesterday at 11:04 AM I purchased few of the Arduinos and Raspberry PI to experiment. Image Charging the controller already. We go from there. JackSilb \u2014 Yesterday at 11:12 AM Yeah, I have a similar from MATEKSYS too. At UCSD, I went away from the PWMs. We go from the JTN to the VESC using USB. The VESC can use the PPM input as output to drive a servo. My current need is for the autonomous evGoKart. Talking you made me find a solution for long distance EMO cheap vs. using the long range telemetry 3DR like only for the EMO. We were using it for the JoStick too with a ROS application on an RPI. Too complex. I will give a try using  ELRS UART -&gt; (Arduino Pro Micro or Teensy or Raspberry PICO) USB HID -&gt; USB (Jetson or RPI)  and (Arduino Pro Micro or Teensy or Raspberry PICO) I/O to the disable pin of the VESCs. We have one VESC for the throttle and one for the steering. TCIII \u2014 Yesterday at 11:34 AM I have a whole bunch of 3DR Telemetry Transceivers on 915 Mhz. \ud83d\ude42  They were only good for LOS even at 100 mw and highly directional too. \ud83d\ude41 TCIII \u2014 Yesterday at 11:38 AM I bought three of the Hiltego Arduino Pro Micros to work with the BetaFPV Nano receiver and all three programmed just fine with a Sparkfun version of Blinky as a test of their functionality. \ud83d\ude42 I have one Rpi PICO that Ed convinced me to buy as it it very fast compare to the Arduino Minis.  REMEMBER that the BetaFPV LiteRadio 3 and the Pro must have the left joystick in the down position when you turn them on or they will buzz and flash the power button RED. As a result of this characteristic, don't energize your car ESC until you have the left joystick at the neutral position (assuming Mode 2) which is basically straight up or your car will go into reverse at full speed.\ud83d\ude32  TCIII \u2014 Today at 6:16 AM Hi Jack. I measured the Nano receiver serial output signal voltage with my digital o'scope this morning and it appears to be 3.43 v which should be safe as an input to either the Nano or the Rpi GPIO bus for direct decoding of the crsf signal. TCIII \u2014 Today at 7:19 AM Hi Jack. I connected the BetaFPV Nano serial output receiver to the Rx input of the Pico Micro running the crsf_decode_hid.ino sketch and viewed the Pico Micro USB HID output in the PC Game Controller Panel \"game controller settings\". I have the RC gamepad frame rate set at the default 150 frames/sec and the joystick display responses are virtually instantaneous and smooth unlike some gamepads. \ud83d\ude42  The \"+\" cursor in the X Axis / Y Axis box is presently controlled by the right joystick horizontal and vertical axes though the \"+\" moves in the opposite direction from the joystick direction of the vertical axis on the gamepad while the left joystick vertical axis (throttle) controls the Z axis red bar and the horizontal axis controls the X axis rotation. The gamepad left push on push off SA switch controls the Y rotation red bar while the left three way switch SB controls the Z rotation red bar.  Observations concerning the Joystick axes and switches definitions:</p> <p>The Joystick HID descriptor report, shown below, controls the definition of the joystick axes and switches:</p> <p>Joystick_ Joystick(JOYSTICK_DEFAULT_REPORT_ID,JOYSTICK_TYPE_GAMEPAD,   0, 0,                 // Button Count, Hat Switch Count   true, true, true,  // X, Y, Z   true, true, true,  // Rx, Ry, Rz   false, false,          // Rudder, Throttle   false, false, false);    // Accelerator, Brake, Steering</p> <p>Based on a search of the IoT for information concerning the HID descriptor report, the HID descriptor shown above does not match any of the HID descriptor report documentation I could find. I suspect that the HID descriptor report shown above is unique to the Arduino Joystick Library requirements and we need to understand how to modify the HID descriptor report to meet the DC BT gamepad joystick/button requirements. TCIII \u2014 Today at 8:05 AM So we now have a choice on how we connect the LoRa RC gamepad receivers to our SBCs:</p> <ol> <li> <p>Connect the Nano receiver UART serial output to a serial Rx pin on the SBC GPIO bus and create a part (https://pypi.org/project/crsf-parser/) that mimics a joystick gamepad or</p> </li> <li> <p>Connect the Nano receiver UART serial output to a Pico Micro running the crsf_decode_hid.ino sketch which makes the crsf output of the Nano receiver look like a HID gamepad (game controller) though it might require some modification of the Joystick HID descriptor report in the sketch to get the RC gamepad to work with DC as a BT gamepad.</p> </li> </ol> <p>Comments?  PyPI crsf-parser A package to parse and create CRSF (Crossfire) frames, developed primarily to interoperate with ExpressLRS Image TCIII \u2014 Today at 10:23 AM Hi Jack. Do you think that any of your graduate students would be interested in writing a DC part to decode the crsf data stream frames and provide functional gamepad output? \ud83d\ude42 If so, I will be glad to work with them.  TCIII \u2014 Today at 11:30 AM I don't think that the Micro HID output looks completely like a PS4/XBox gamepad so the HID joystick descriptor report will probably have to be adjusted to look like a standard gamepad. Since there is no CONTROLLER_TYPE = defined for this HID RC gamepad, Users will have to use the Joystick Wizard to create a custom joystick?</p> <p>One of the issue I see is that LoRa RC gamepads are Mode 2 gamepads where the throttle is on the left joystick and the steering is on the right joystick. \ud83d\ude41  A standard BT gamepad is a Mode 1 gamepad where the throttle is on the right joystick and the steering is on the left joystick. TCIII \u2014 Today at 12:22 PM I suspect that the only way that this LoRa RC gamepad is really going to be really \"user transparent\" is to create a part that decodes and process the crsf serial data stream to create a standard gamepad?  TCIII \u2014 Today at 12:42 PM Hi Jack. Here is a tutorial on how to use an RC Radio transmitter as a gamepad controller: https://www.plop.at/en/rc.html It might give us some insight on modifying, if necessary, the crsf_decode_hid.ino sketch to simulate a generic gamepad? I have contacted the author of the crsf_decode_hid.ino sketch about the joystick HID descriptor report that he used and how he created it. TCIII \u2014 Today at 1:39 PM Hi Jack. Attached is a screen shot of what the output of the RC Gamepad/Pro Micro USB HID Game Controller Panel looks like. The \"+\" cursor in the X Axis / Y Axis box is presently controlled by the right joystick horizontal (Steering) and vertical axes though the \"+\" moves in the opposite direction from the joystick direction of the vertical axis on the gamepad while the left joystick vertical axis (Throttle) controls the Z axis red bar and the horizontal axis controls the X axis rotation. The gamepad left push on push off SA switch controls the Y rotation red bar while the left three way switch SB controls the Z rotation red bar. Image </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#some-background-and-references-pwm-ppm","title":"Some background and References PWM PPM","text":"<p>Hi guys, I have a question about modifying the controller for the donkey car base on this link https://docs.google.com/document/d/1yx1mF0dgEHLx5S3Vqss8YFFkW4thCrl9fpgVliHcuhw/edit  we are able to get the signals from controller to the Arduino after modifying the controller, but now we need to know how to connect the Arduino Leonardo to PWM.</p> <p>Ehsan, the Arduino connects to the JTN or RPI. The PWM board does not change until we have the Teensy in the loop. Therefore, the PWM board is the same. It uses I2C to communicate with the JTN or RPI. That is why it is not in the instructions.</p> <p>Basically the Arduino is simulating a JS0 (Joystick) in a Linux machine. It converts PWM signals from the RC controllers as it was a Game/Pad Joystick. The PWM board is still the same, nothing changes.</p> <p>You need to get the latest Donkey, 3.1.1 if I am not mistaken, and change the Joystick Type on myconfig.py along with your calibration values, Webcam, and use channel 1 for steering, channel 2 for Throttle.</p> <p>The only difference on the myconfig.py for using the Arudino (Leonardo) simulating the Joystick is the setting on the Joystick/game controller type. Everything else is as you were using a regular Donkey setup.</p> <p>https://github.com/n6wxd/wireless-rc-adapter/blob/master/wireless-rc-adapter-2.1/src/PinChangeInterrupt/README.md  Arduino Uno/Nano/Mini: All pins are usable Arduino Mega: 10, 11, 12, 13, 50, 51, 52, 53, A8 (62), A9 (63), A10 (64), A11 (65), A12 (66), A13 (67), A14 (68), A15 (69)  Arduino Leonardo/Micro: 8, 9, 10, 11, 14 (MISO), 15 (SCK), 16 (MOSI)  HoodLoader2: All (broken out 1-7) pins are usable  Attiny 24/44/84: All pins are usable  Attiny 25/45/85: All pins are usable  Attiny 13: All pins are usable  Attiny 441/841: All pins are usable  ATmega644P/ATmega1284P: All pins are usable  Arduino Leonardo/Micro allows interrupts on : 8, 9, 10, 11, 14 (MISO), 15 (SCK), 16 (MOSI) So, let\u2019s use 8, 9, 10</p> <p>https://github.com/n6wxd/wireless-rc-adapter/blob/master/wireless-rc-adapter-2.1/pwm.ino</p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)","text":"<pre><code>const uint8_t RC_PINS[6] = {11, 10, 9, 8, PB2, PB1};\n</code></pre> <p>https://github.com/n6wxd/wireless-rc-adapter  https://github.com/wireless-rc-adapter/wireless-rc-adapter/wiki  We just need 3 channels from the RC Rx  The Arduino board we got out of Amazon.com (Pro Micro) it is compatible with Arduino Leonardo firmware not Arduino Pro Micro. Also, it does not have Pin 11 available for a connection.</p> <p>We need to change the pins used at the board and Arduino IDE to be 10,9,8</p> <p>At pwm.ino From </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_1","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)","text":"<p>Ch1 -&gt; Pin 8 Ch2 -&gt; Pin 9 Ch3 -&gt; Pin 10</p> <p>Also, Ch3 \u201c+\u201d and \u201c-\u201d are used to power the RC Rx Radio Ch3+ (middle pin) -&gt; Pin VCC (5V) Ch3- -&gt; Pin GND (Ground)</p> <p>Note: The power to the Arduino comes from the USB cable that connects to the JTN or RPI. The power to the RC radio comes from the Arduino. https://inventr.io/blogs/arduino/arduino-pro-micro-review-scroller   The GPIO for Jetson Nano and RPI are the same. Connect the PWM board using I2C. See instructions for ECE MAE 148. Nothing changes here. The steering servo and the ESC connect to the PWM board, not Arudino. </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#how-to-make-it-work","title":"How to Make it Work","text":"<p>Install the latest version of the Arduino IDE from here </p> <p>Download the Arduino files from here to a local directory https://github.com/n6wxd/wireless-rc-adapter</p> <p>Plug the Arduino into one of your computer USB port Configure the Arduino GUI to use Arduino Leonardo Configure the USB Port used for the Arduino </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#wireless-rc-adapter-21ino","title":"wireless-rc-adapter-2.1.ino","text":"<p>Open wireless-rc-adapter-2.1.ino in the Arduino GUI </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#pwmino","title":"pwm.ino","text":"<p>The Arduino IDE  should show a tab for pwm.ino Click over the tab pwm.ino Change the pins as below. Our radio has 3 channels, we just use the first 3 pins numbered 10,9,8 </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_2","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)","text":"<pre><code>const uint8_t RC_PINS[6] = {8, 9, 10, PB2, PB1,11};\n</code></pre> <p> Save the file </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#ledino","title":"led.ino","text":"<p>Change back what Steve B. modified for the LEDs From </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_3","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)","text":"<pre><code>#define RXLED 13  // RXLED is on pin 17&lt;br&gt;\n#define TXLED 30  // TXLED is on pin 30&lt;br&gt;&lt;br&gt;\n</code></pre> <p>To</p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#elif-definedarduino_avr_micro-definedarduino_avr_leonardo_4","title":"elif defined(ARDUINO_AVR_MICRO) || defined(ARDUINO_AVR_LEONARDO)","text":"<pre><code>#define RXLED 17  // RXLED is on pin 17&lt;br&gt;\n#define TXLED 30  // TXLED is on pin 30&lt;br&gt;\n</code></pre> <p>Save the file</p> <p>Upload the wireless-rc-adapter-2.1.ino into the Aruduino, the LEDs should blink when uploading the software then blink in different patterns depending on software stage</p> <p>See here https://github.com/wireless-rc-adapter/wireless-rc-adapter/wiki </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#wireless-rc-adapter-21ino_1","title":"wireless-rc-adapter-2.1.ino","text":"<p>First make sure you pair your RC Tx and Rx ...</p> <p>To verify that the software is working you can enable the serial debugging. Edit wireless-rc-adapter-2.1.ino Remove the comments in the front of //#define SERIAL_DEBUG and //#define SERIAL_SPD 115200  // &gt;&gt;&gt; Serial-Debug options for troubleshooting &lt;&lt;&lt; define SERIAL_DEBUG  // Enable Serial Debug by uncommenting this line define SERIAL_SPD 115200  // Set debug bitrate between 9600-115200 bps (default: 9600)</p> <p>Save the file Upload / run</p> <p>Then using the terminal from the Arduino IDE you can follow the boot process and calibrations. You can also use that to verify the calibration and values the Arduino is receiving from the RC receiver.</p> <p>After you verify that the 3 channels are working, comment the serial DEBUG lines back, save, upload/run</p> <p>// &gt;&gt;&gt; Serial-Debug options for troubleshooting &lt;&lt;&lt;</p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#define-serial_debug-enable-serial-debug-by-uncommenting-this-line","title":"define SERIAL_DEBUG  // Enable Serial Debug by uncommenting this line","text":""},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#define-serial_spd-115200-set-debug-bitrate-between-9600-115200-bps-default-9600","title":"define SERIAL_SPD 115200  // Set debug bitrate between 9600-115200 bps (default: 9600)","text":"<p> You can test the JS0 operation in a Linux machine, including the Jetson Nano (JTN) and Raspberry PI (RPI) with #Open a terminal #ls /dev/input/ #this command should show a js0 device listed e.g, #ls /dev/input _#by-id  by-path  event0  event1  js0  mice</p> <p>#Then lets try reading joystick values #sudo apt-get install joystick #sudo jstest /dev/input/js0  If you don\u2019t see the joystick working on the JTN or RPI, then don\u2019t try to use it on Donkey. </p>"},{"location":"documentation/Controller/UCSD%20Robocar%20Using%20BetaFPV%20LiteRadio%203%20Pro/#calibration","title":"Calibration","text":"<p>On every startup it tries to load calibration data from the long-term memory and decides if those values are still correct or out of sync. The algorithm triggers a calibration automatically if necessary. Calibration can be triggered manually with powering the adapter while the CAL_CHANNEL is on full duty cycle. In other words the throttle must be up before start and it is automatically starts calibration on boot. When the device is in calibration mode, the leds are flashing steady on the board, and all the channel minimum and maximum values are getting recorded. During this time move all the configured sticks and pots/switches on the transmitter remote to its extents. After there is no more new min's or max's found the algorithm finishing the calibration within CAL_TIMEOUT by checking and saving the values in the long-term memory (eeprom). Leds are flashing twice and turns off, the adapter is now available as joystick on the device it is plugged in.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/","title":"UCSD Robocar MAE &amp; ECE 148","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/#table-of-contents","title":"Table of Contents","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/#introduction","title":"Introduction","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/#single-board-computer-sbc-basic-setup","title":"Single Board Computer (SBC) Basic Setup","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/#jetson-nano-jtn-configuration","title":"Jetson Nano (JTN) Configuration","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/#jetson-xavier-jnx-configuration","title":"Jetson Xavier (JNX) Configuration","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/#hardware-setup","title":"Hardware Setup","text":"<p>Subsections not included currently \u2014 see the Google Doc.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/#build-opencv-from-source","title":"Build OpenCV from Source","text":"<p>Since January 2020, Nvidia has not provided OpenCV optimized for use with the CUDA cores on the Jetson. Therefore, we must build OpenCV from source in order for it to be accelerated via CUDA.</p> <p>Instructions can either be found in the 60 document, or here.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/#donkeycar-ai-framework","title":"DonkeyCar AI Framework","text":"<p>This section was created referencing this document.</p> <p>Make sure OpenCV was built from source or verify that CUDA support has been enabled.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/#setting-up-the-donkeycar-ai-framework","title":"Setting up the DonkeyCar AI Framework","text":"<p>Begin by ssh-ing into the Jetson (or your SBC of choice). To ensure a proper base for the installation, run:</p> <pre><code>sudo apt update -y\nsudo apt upgrade -y\nsudo usermod -aG dialout jetson\n</code></pre> <p>If packages are being held back, run:</p> <pre><code>sudo apt-get --with-new-pkgs upgrade\n</code></pre> <p>and (?)</p> <pre><code>sudo apt-get install -y build-essential python3 python3-dev python3-pip libhdf5-serial-dev hdf5-tools  libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran libxslt1-dev libxml2-dev libffi-dev libcurl4-openssl-dev libssl-dev libpng-dev libopenblas-dev openmpi-doc openmpi-bin libopenmpi-dev libopenblas-dev git nano\n</code></pre>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/#gpio-access-for-jetson","title":"GPIO Access for Jetson","text":"<p>Install the RPi.GPIO clone for the Jetson Nano, Jetson.GPIO.</p> <pre><code>pip3 install Jetson.GPIO\n</code></pre> <p>If the pip install complains about ownership of the directory, execute:</p> <pre><code>sudo chown -R jetson:jetson /home/jetson/.cache/pip\n</code></pre> <p>Warning: the directory <code>/home/jetson/.cache/pip/http</code> or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.</p> <p>If pip breaks for some reason, you can reinstall with:</p> <pre><code>python3 -m pip uninstall pip\nsudo apt install python3-pip --reinstall\n</code></pre> <p>If the install requests elevated privileges, execute:</p> <pre><code>sudo pip3 install Jetson.GPIO\n</code></pre> <p>If pip has a new version, run:</p> <pre><code>pip3 install --upgrade pip\n</code></pre> <p>As of 18 Sept 2022, the following section does not work with JetPack 4.6.2</p> <p>To make sure the <code>jetson</code> user can use the GPIO, run:</p> <pre><code>sudo groupadd -f -r gpio\nsudo usermod -a -G gpio jetson\nsudo cp /opt/nvidia/jetson-gpio/etc/99-gpio.rules /etc/udev/rules.d/\n</code></pre>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/#creating-a-virtual-environment-for-donkeycar-ai-framework","title":"Creating a Virtual Environment for DonkeyCar AI Framework","text":"<p>If you want the virtual environment to be under the user's home directory, make sure to be on the home directory for the user <code>jetson</code>.</p> <p>If you have not done so, create the projects directory <code>~/projects</code> and a subdirectory to store virtual environments <code>~/projects/envs</code>:</p> <pre><code>cd ~\nmkdir projects\ncd projects\nmkdir envs\ncd envs\npip3 install virtualenv\n</code></pre> <p>If there is a user permission error, run:</p> <pre><code>pip3 install virtualenv --user\n</code></pre> <p>We will create the virtual environment <code>donkey</code> since the framework is based on DonkeyCar. To do so, run:</p> <pre><code>python3 -m virtualenv -p python3 ~/projects/envs/donkey --system-site-packages\n</code></pre> <p>Since the Jetson will be working with Donkey primarily until custom projects begin, we will have it so the donkey environment is enabled whenever loggin in to the Jetson. You can remove this later if needed:</p> <pre><code>echo \"source ~/projects/envs/donkey/bin/activate\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>Whenever a virtual envirnonment is activated, you should see (NAME_OF_VIRTUAL_ENVIRONMENT) before your terminal prompt (e.g. <code>(donkey) jetson@ucsdrobocar-xxx-yy:~$</code>)</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/","title":"DonkeyCar Installation","text":"<p>UCSD RoboCar ECE &amp; MAE 148</p> <p>Version 20.7 - 23Oct2022</p> <p>Prepared by</p> <p>Dr. Jack Silberman</p> <p>Department of Electrical and Computer Engineering</p> <p>and</p> <p>Dominic Nightingale</p> <p>Department of Mechanical and Aerospace Engineering</p> <p>University of California, San Diego</p> <p>9500 Gilman Dr, La Jolla, CA 92093</p> <p></p> <p></p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#table-of-contents","title":"Table of Contents","text":"<p>Table of Contents 2</p> <p>Introduction 4</p> <p>Single Board Computer (SBC) Basic Setup 5</p> <p>Jetson Nano (JTN) Configuration 5</p> <p>Jetson Xavier NX Configuration 5</p> <p>Hardware Setup 6</p> <p>Jetson Nano GPIO Header PINOUT 6</p> <p>VESC 7</p> <p>VESC Hardware V6.x 8</p> <p>Motor Detection 10</p> <p>Sensor Detection 14</p> <p>Enable Servo Control For Steering 16</p> <p>VESC Hardware V4.12 (if you have V6, skip this) 17</p> <p>PWM Controller 18</p> <p>Wiring adafruit board 19</p> <p>Detecting the PWM controller 20</p> <p>Emergency stop - Relay 21</p> <p>Logitech F710 controller 23</p> <p>Other JoyStick Controllers 23</p> <p>LD06 Lidar 25</p> <p>Laser Map 25</p> <p>Mechanical drawing 25</p> <p>Install OpenCV from Source 26</p> <p>DonkeyCar AI Framework 26</p> <p>Setting up the DonkeyCar AI Framework 26</p> <p>Create a virtual environment for the DonkeyCar AI Framework. 27</p> <p>Confirm openCV build from previous steps 28</p> <p>Tensorflow 29</p> <p>PyTorch 32</p> <p>Installing Donkeycar AI Framework 35</p> <p>Create a Car 36</p> <p>Donkeycar manage.py 37</p> <p>Modifying PWM board configuration 37</p> <p>Modifying Camera 37</p> <p>Quick Test 38</p> <p>Modifying Joystick 39</p> <p>Calibration of the Throttle and Steering 41</p> <p>Begin Calibration 42</p> <p>Saving car configuration file 44</p> <p>Driving the Robot to Collect data 46</p> <p>Backup of the uSD Card 50</p> <p>If needed, we have an uSD Card Image Ready for Plug and Play 52</p> <p>ROS with Docker 53</p> <p>Supporting material 54</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#_1","title":"DonkeyCar Installation","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#introduction","title":"Introduction","text":"<p>This document was derived from the DIY RoboCar - Donkey Car Framework</p> <p>Reference information can be found at http://docs.donkeycar.com</p> <p>At UC San Diego's Introduction to Autonomous Vehicles class (ECE MAE148), we use an AI Framework called Donkey Car which is based on Deep Learning / Human behavior cloning as well as we do traditional programming using Robot Operating System (ROS2).</p> <p>DonkeyCar can be seen as the \"hello world\" of affordable scaled autonomous cars</p> <p>We have added other features into our UCSD scale robot cars that are not found at the Default Donkey car build such as a wireless emergency off switch. Therefore, please follow the instructions found in this document vs. the default Donkey built.</p> <p>Another framework we use called UCSD Robocar is primarily maintained and developed by Dominic Nightingale right here at UC San Diego. UCSD Robocar uses ROS and ROS2 for controlling our scaled robot cars which can vary from traditional programming or machine learning to achieve an objective. The framework works with a vast selection of sensors and actuation methods in our inventory making it a robust framework to use across various platforms. Has been tested on 1/16, 1/10, 1/5 scaled robot cars and soon our go-karts.</p> <p>As August 2019 we transitioned from the single board computer (SBC) called Raspberry PI to the Nvidia Jetson Nano. If you are using a Raspberry PI, then search in this document for Raspberry PI (RPI or PI) Configuration.</p> <p>On 28Aug19, we updated the instructions to include Raspberry PI 4B Depending on your Single Board Computer, Jetson Xavier NX, Jetson Nano, then follow the related instructions.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#_2","title":"DonkeyCar Installation","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#single-board-computer-sbc-basic-setup","title":"Single Board Computer (SBC) Basic Setup","text":"<p>We will be using the Ubuntu Linux distribution. In the class you have access to a virtual machine image file with Ubuntu.</p> <p>We won't install ROS2 directly into the SBC, we will be using Docker images and containers.</p> <p>You will install OpenCV from source as part of your learning on compiling and building software from source.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#jetson-nano-jtn-configuration","title":"Jetson Nano (JTN) Configuration","text":"<p>Instructions to configure the Jetson Nano</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#jetson-xavier-nx-jnx-configuration","title":"Jetson Xavier NX (JNX) Configuration","text":"<p>Instructions to Configure the Jetson Xaviver NX</p> <p>Archive location to previous JetPack versions</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#editing-remotely-with-jupyter-notebooks","title":"Editing Remotely with Jupyter Notebooks","text":"<p>Install Jupyter notebook on your Jetson:</p> <p>sudo apt install jupyter-notebook</p> <p>https://ljvmiranda921.github.io/notebook/2018/01/31/running-a-jupyter-notebook/</p> <p>Help document for editing using Jupyter notebook:</p> <p>[Conifguring Jupyter Notebook on SSH(https://docs.google.com/document/d/1ZNACJvKmQNnN7QNMwgqnzjrs9JDdFbiqVFHAuhgillQ/edit?usp=sharing)</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#hardware-setup","title":"Hardware Setup","text":"<p>You should consider breaking the work on building the robots per team member:</p> <p>a)  Someone could start to build OpenCV GPU accelerated in parallel     while you build the robot. It will take several hours building     OpenCV from     source.     Try to divide the work by team members ...</p> <p>b)  Start designing, 3D Printing, Laser Cutting the parts</p> <p>As of Spring 2022, we upgraded all the ECE MAE 148 robots to use VESCs.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#vesc","title":"VESC","text":"<p>VESC is a super cool Electronic Speed Controller (ESC) that runs open source code with significantly more capabilities than a regular RC Car ESC.</p> <p>VESCs are very popular for electrical skateboards, DIY electrical scooters, and robotics.</p> <p>For robotics, one of the capabilities we will use the most is Odometry (speed and position)</p> <p>based on the sensors on brushless motors (sensored) or to some extent, specially using the latest VESCs and firmware, it is also available with brushless motors without sensors (sensorless).</p> <p>https://vesc-project.com/</p> <p>VESC Setup Instructions</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#logitech-f710-controller","title":"Logitech F710 controller","text":"<p>Place your Logitech F710 controller on the [x mode]{.mark}</p> <p>(look for small switch in one of the controller face)</p> <p>Connect the USB JoyStick Dongle into the JTN and then list the input devices again</p> <p>ls /dev/input</p> <p>(env) jetson@ucsdrobocar00:\\~/projects/d3\\$ ls /dev/input</p> <p>by-id event0 event2 event4 mice mouse1</p> <p>by-path event1 event3 [js0]{.mark} mouse0</p> <p>We are looking for a js0</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#other-joystick-controllers","title":"Other JoyStick Controllers","text":"<p>JoyStick Controllers - either the Logitech F-10 or PS4</p> <p>Make sure your myconfig.py on your car directory reflects your controller</p> <p>Connecting the LogiTech Controller is as easy as plugging in the USB dongle at the SBC.</p> <p>[If you want to check controller was connected]</p> <p>ls /dev/input</p> <p>[and see if js0 is listed.]</p> <p>[Lets test the joystick in a linux machine]</p> <p>sudo apt-get update</p> <p>sudo apt-get install -y jstest-gtk</p> <p>jstest /dev/input/js0</p> <p>To remove a device, let\\'s say another JoyStick that you don't use anymore</p> <p>bluetoothctl</p> <p>paired-devices</p> <p>remove THE_CONTROLLER_MAC_ADDRESS</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#ld06-lidar","title":"LD06 Lidar","text":"<p>Datasheet for LD06 lidar: [datasheet]{.underline}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#laser-map","title":"Laser Map","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#mechanical-drawing","title":"Mechanical drawing","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#donkeycar-ai-framework","title":"DonkeyCar AI Framework","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#setting-up-the-donkeycar-ai-framework","title":"Setting up the DonkeyCar AI Framework","text":"<p>Reference [http://docs.donkeycar.com]{.underline}</p> <p>Make sure that the OpenCV you want to use supporting CUDA is already available as a system</p> <p>wide package.</p> <p>Remember that when you are compiling and building software from source, it may take a few hours ...</p> <p>SSH into the Single Board Computer (SBC) e.g., RPI, JTN, JNX, etc.</p> <p>\\ Install some packaged, some may be already installed</p> <pre><code>sudo apt update -y\n\nsudo apt upgrade -y\n\nsudo usermod -aG dialout jetson\n</code></pre> <p>#If packages are being held back</p> <pre><code> sudo apt-get \\--with-new-pkgs upgrade\n</code></pre> <pre><code>sudo apt-get install -y build-essential python3 python3-dev python3-pip\nlibhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev\nliblapack-dev libblas-dev gfortran libxslt1-dev libxml2-dev libffi-dev\nlibcurl4-openssl-dev libssl-dev libpng-dev libopenblas-dev openmpi-doc\nopenmpi-bin libopenmpi-dev libopenblas-dev git nano\n</code></pre> <p>Install RPi.GPIO clone for Jetson Nano</p> <p>[https://github.com/NVIDIA/jetson-gpio]{.underline}</p> <p>pip3 install Jetson.GPIO</p> <p>If the pip install complains about ownership of the directory*</p> <p>then execute the following command</p> <p>sudo chown -R jetson:jetson /home/jetson/.cache/pip</p> <p>ex:</p> <p>WARNING: The directory \\'/home/jetson/.cache/pip/http\\' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo\\'s -H flag.</p> <p>WARNING: The directory \\'/home/jetson/.cache/pip\\' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of</p> <p>that directory. If executing pip with sudo, you may want sudo\\'s -H flag.</p> <p>If pip breaks for some reason, you can reinstall it with the following lines</p> <p>python3 -m pip uninstall pip</p> <p>sudo apt install python3-pip --reinstall</p> <p>If the install request elevated privileges, execute the following command</p> <p>sudo pip3 install Jetson.GPIO</p> <p>if pip has a new version</p> <p>pip3 install --upgrade pip</p> <p>Let's make sure the user jetson can use gpio</p> <p>sudo groupadd -f -r gpio</p> <p>sudo usermod -a -G gpio jetson</p> <p>sudo cp /opt/nvidia/jetson-gpio/etc/99-gpio.rules /etc/udev/rules.d/</p> <p>28Jan20 - did not work with JetPack3.4</p> <p>15May21- did not work with JetPack4.5</p> <p>19Oct21 - did not work with JetPack4.6</p> <p>18Sep22 - did not work with JetPack4.6.2</p> <p>Will get back to it later if the jetson user can not access GPIO</p> <p>sudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger</p> <p>We want to have control over the versions of each software library to minimize the framework from</p> <p>breaking after system-wide upgrades. Therefore, lets create a virtual environment for the</p> <p>DonkeyCar.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#create-a-virtual-environment-for-the-donkeycar-ai-framework","title":"Create a virtual environment for the DonkeyCar AI Framework.","text":"<p>If you want the virtual environment to be under the user's home directory, make sure to be on the</p> <p>home directory for user jetson</p> <p>If you have not done so, lets create a directory to store our projects and one subdirectory</p> <p>to store virtual environments</p> <p>cd \\~</p> <p>mkdir projects</p> <p>cd projects</p> <p>mkdir envs</p> <p>cd envs</p> <p>pip3 install virtualenv</p> <p>if complains about user permission</p> <p>pip3 install virtualenv --user</p> <p>We will create a virtual environment called donkey since our AI framework is</p> <p>based on the Donkey Car project</p> <p>Since your SBC will be initially dedicated to the class AI framework (Donkey), at least until</p> <p>your custom project starts, let's activate the donkey virtual env automatically every time the user</p> <p>Jetson logs into the SBC. We can remove this settings later if needed when using ROS2</p> <p>echo \\\"source \\~/projects/envs/donkey/bin/activate\\\" &gt;&gt; \\~/.bashrc</p> <p>source \\~/.bashrc</p> <p>When a virtual environment is active, you should see (name_of_virtual_enviroment) in front of the terminal prompt.</p> <p>ex:</p> <p>[(donkey)]{.mark} jetson@ucsdrobocar-xxx-yy:\\~\\$</p> <p>At this point, using pip and pip3 should be the same as using pip3 by default in this virtual environment.</p> <p>https://docs.donkeycar.com/guide/robot_sbc/setup_jetson_nano/ 46</p> <p>cd \\~/projects/envs/donkey/lib/python3.6/site-packages/</p> <p>ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so cv2.so</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#it-is-necessary-to-create-a-link-to-it","title":"it is necessary to create a link to it","text":"<p># Go to the folder where OpenCV\\'s native library is built</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#cd-usrlocallibpython36site-packagescv2python-36","title":"cd /usr/local/lib/python3.6/site-packages/cv2/python-3.6","text":"<p># Rename</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#mv-cv2cpython-36m-xxx-linux-gnuso-cv2so","title":"mv cv2.cpython-36m-xxx-linux-gnu.so cv2.so","text":"<p># Go to your virtual environments site-packages folder if previously set</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#cd-envlibpython36site-packages","title":"cd \\~/env/lib/python3.6/site-packages/","text":"<p># Or just go to your home folder if not set a venv site-packages folder</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#cd","title":"cd \\~","text":"<p># Symlink the native library</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#ln-s-usrlocallibpython36site-packagescv2python-36cv2so","title":"ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so","text":"<p>cv2.so</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#note-that-it-is-almost-mandatory-to-create-a-virtual-environment-in","title":"NOTE that it is almost mandatory to create a virtual environment in","text":"<p>order to properly install</p> <p># tensorflow, scipy and keras, and always a best practice.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#_3","title":"DonkeyCar Installation","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#confirm-that-opencv-built-from-previous-steps-is-working-on-the-virtual-environment-donkey","title":"Confirm that OpenCV built from previous steps is working on the virtual environment Donkey","text":"<p># Testing to see if OpenCV is installed in the virtual env.</p> <p>python3 -c \\'import cv2 as cv; print(cv.__version__)\\'</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:\\~/projects/envs/donkey\\$ python3 -c \\'import cv2 as cv; print(cv.__version__)\\'</p> <p>4.6.0</p> <p># We won\\'t use Python2, but just in case one will need it for some reason</p> <p>python2 -c \\'import cv2 as cv; print(cv.__version__)\\'</p> <p>We are not done installing software yet. We need to install more dependencies..</p> <p>Make sure you have the donkey virtual environment activated</p> <p>Remember some of these installs may take a while. It does not mean that the SBC is frozen, you</p> <p>can see that the CPU is busy with top, htop, or jtop</p> <p>source \\~/projects/envs/donkey/bin/activate</p> <p>pip3 install -U pip testresources setuptools</p> <p>pip3 install -U futures==3.1.1 protobuf==3.12.2 pybind11==2.5.0</p> <p>pip3 install -U cython==0.29.21 pyserial</p> <p>pip3 install -U future==0.18.2 mock==4.0.2 h5py==2.10.0 keras_preprocessing==1.1.2 keras_applications==1.0.8 gast==0.3.3</p> <p>pip3 install -U absl-py==0.9.0 py-cpuinfo==7.0.0 psutil==5.7.2 portpicker==1.3.1 six requests==2.24.0 astor==0.8.1 termcolor==1.1.0 wrapt==1.12.1 google-pasta==0.2.0</p> <p>pip3 install -U gdown</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#tensorflow","title":"Tensorflow","text":"<p>Now let\\'s install [Tensorflow]{.underline} (Artificial Neural Network software).</p> <p>\"TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.\"</p> <p>Lets install Tensorflow enabled for GPU acceleration</p> <p>Another chance for you to study while software is being installed...</p> <p>Background information here</p> <p>[https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html]{.underline}</p> <p>Remember you are using a low power SBC, depending on the size of the software it takes a while</p> <p>We are installing Tensorflow outside the virtual environment so it is available for other uses</p> <p>Here is another chance for you to study while software is being installed...</p> <p>[Background information here]{.underline}</p> <p>We are using JetPack 4.5 because the new DonkeyCar release was breaking the install with JetPack4.6.2. It requires Python 7 and newer.</p> <p>Let\\'s stick with JetPack4.5 for now</p> <p>[https://developer.download.nvidia.com/compute/redist/jp/v45/tensorflow/]{.underline}</p> <p>As of 18Sep22</p> <p>tensorflow-1.15.4+nv20.12-cp36-cp36m-linux_aarch64.whl 218MB 2020-12-18 14:54:04</p> <p>tensorflow-2.3.1+nv20.12-cp36-cp36m-linux_aarch64.whl 264MB 2020-12-18 14:54:06</p> <p>tensorflow-1.15.5+nv21.2-cp36-cp36m-linux_aarch64.whl 218MB 2021-02-26 16:10:00</p> <p>tensorflow-2.4.0+nv21.2-cp36-cp36m-linux_aarch64.whl 273MB 2021-02-26 16:10:14</p> <p>tensorflow-1.15.5+nv21.3-cp36-cp36m-linux_aarch64.whl 218MB 2021-03-25 18:14:17</p> <p>tensorflow-2.4.0+nv21.3-cp36-cp36m-linux_aarch64.whl 273MB 2021-03-25 18:14:48</p> <p>tensorflow-1.15.5+nv21.4-cp36-cp36m-linux_aarch64.whl 218MB 2021-04-26 20:36:59</p> <p>tensorflow-2.4.0+nv21.4-cp36-cp36m-linux_aarch64.whl 273MB 2021-04-26 20:38:13</p> <p>tensorflow-1.15.5+nv21.5-cp36-cp36m-linux_aarch64.whl 218MB 2021-05-20 20:19:08</p> <p>tensorflow-2.4.0+nv21.5-cp36-cp36m-linux_aarch64.whl 274MB 2021-05-20 20:19:20</p> <p>tensorflow-1.15.5+nv21.6-cp36-cp36m-linux_aarch64.whl 220MB 2021-06-29 18:18:14</p> <p>tensorflow-2.5.0+nv21.6-cp36-cp36m-linux_aarch64.whl 293MB 2021-06-29 18:18:43</p> <p># This will install the latest tensorflow compatible with the Jet Pack as a system package</p> <p>Alternatively if you want to chose a particular version:</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#pip3-install-pre-extra-index-url","title":"pip3 install --pre --extra-index-url","text":"<p>https://developer.download.nvidia.com/compute/redist/jp/v45 tensorflow==2.3.1</p> <p>Previous versions install</p> <p>pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v45 tensorflow==2.3.1</p> <p>pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v45 tensorflow==2.4.0</p> <p>Remember you are using a low power SBC, depending on the size of the software it takes a while</p> <p>Lets verify that Tensorflow installed correctly</p> <p>python3</p> <p>import tensorflow</p> <p>exit()</p> <p>No errors should be reported</p> <p>If you get errors importing Tensorflow 2.5.0, try these</p> <p>pip install numpy==1.19.2</p> <p>ex:</p> <p>If you see info with libcuda it means, Tensorflow will be accelerated using the CUDA</p> <p>cores of the SBC's GPU</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:\\~/projects\\$ python3</p> <p>Python 3.6.9 (default, Jun 29 2022, 11:45:57)</p> <p>[GCC 8.4.0] on linux</p> <p>Type \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more information.</p> <p>&gt;&gt;&gt; import tensorflow</p> <p>2022-08-09 12:19:25.285765: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library [libcudart.]{.mark}so.10.2</p> <p>&gt;&gt;&gt; exit()</p> <p>Verifying that TensorRT was installed</p> <p>sudo apt-get update</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:\\~\\$ sudo apt-get install tensorrt</p> <p>Reading package lists... Done</p> <p>Building dependency tree</p> <p>Reading state information... Done</p> <p>tensorrt is already the newest version (7.1.3.0-1+cuda10.2).</p> <p>0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.</p> <p>dpkg -l | grep TensorRT</p> <p>arm64 Meta package of TensorRT</p> <p>ii uff-converter-tf 7.1.3-1+cuda10.2 arm64 UFF converter for TensorRT package</p> <p># Periodically the versions of tensorflow, cuDNN / CUDA give us conflict. Here is a list of compatibility</p> <p>[https://www.tensorflow.org/install/sourcegpu]{.underline}</p> <p>Installing pycuda - it will take a while again...</p> <p>pip3 install pycuda</p> <p>If you are having errors installing pycuda use the following command:</p> <p>pip3 install pycuda==2020.1</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#pytorch","title":"PyTorch","text":"<p>Lets install [PyTorch]{.underline} too</p> <p>\"An open source machine learning framework that accelerates the path from research prototyping to production deployment\"</p> <p>Again, these steps will take some time. Use your time wisely</p> <p>cd \\~/projects</p> <p>wget https://nvidia.box.com/shared/static/p57jwntv436lfrd78inwl7iml6p13fzh.whl</p> <p>cp p57jwntv436lfrd78inwl7iml6p13fzh.whl torch-1.8.0-cp36-cp36m-linux_aarch64.whl</p> <p>pip3 install torch-1.8.0-cp36-cp36m-linux_aarch64.whl</p> <p>sudo apt-get install libjpeg-dev zlib1g-dev libpython3-dev libavcodec-dev libavformat-dev libswscale-dev</p> <p>git clone -b v0.9.0 https://github.com/pytorch/vision torchvision</p> <p>cd torchvision</p> <p>python setup.py install</p> <p>cd ../</p> <p># it will take a good while again. Keep studying other things...</p> <p>Testing Pythorch install</p> <p>python</p> <p>import torch</p> <p>print(torch.__version__)</p> <p>exit()</p> <p>ex:</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:\\~/projects\\$ python</p> <p>Python 3.6.9 (default, Jan 26 2021, 15:33:00)</p> <p>[GCC 8.4.0] on linux</p> <p>Type \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more information.</p> <p>&gt;&gt;&gt; import torch</p> <p>&gt;&gt;&gt; print(torch.__version__)</p> <p>1.8.0</p> <p>&gt;&gt;&gt; exit()</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:\\~/projects\\$</p> <p>One more test</p> <p>pip3 show torch</p> <p>ex:</p> <p>Name: torch</p> <p>Version: 1.8.0</p> <p>Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration</p> <p>Home-page: https://pytorch.org/</p> <p>Author: PyTorch Team</p> <p>Author-email: packages@pytorch.org</p> <p>License: BSD-3</p> <p>Location: /home/jetson/projects/envs/donkey/lib/python3.6/site-packages</p> <p>Requires: dataclasses, typing-extensions, numpy</p> <p>Required-by: torchvision</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#_4","title":"DonkeyCar Installation","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#as-of-summer-ii-2022-we-are-using-a-new-stereo-camera-from-luxonis","title":"As of Summer II 2022, we are using a new Stereo Camera from Luxonis","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#configuring-oakd-lite","title":"Configuring OAKD Lite","text":"<p>Open a terminal window and run the following commands:</p> <p>sudo apt update &amp;&amp; sudo apt upgrade</p> <p># after upgrades</p> <p>sudo reboot now</p> <p>If you have not added the extra swap space while building OpenCV, please add it</p> <p>You can use jtop to add more swap space using the left and right keys and clicking the plus button</p> <p>jtop</p> <p>4</p> <p>{width=\"4.005208880139983in\" height=\"1.776890857392826in\"}</p> <p>Add 4G of swap and press \\&lt;S&gt; to enable it.</p> <p>{width=\"3.0156255468066493in\" height=\"2.161738845144357in\"}</p> <p>Alternatively you can use the command line</p> <p># Disable ZRAM:</p> <p>sudo systemctl disable nvzramconfig</p> <p># Create 4GB swap file</p> <p>sudo fallocate -l 4G /mnt/4GB.swap</p> <p>sudo chmod 600 /mnt/4GB.swap</p> <p>sudo mkswap /mnt/4GB.swap</p> <p>If you have an issue with the final command, you can try the following:</p> <p>sudo nano /etc/fstab</p> <p># Add this line at the bottom of the file</p> <p>/mnt/4GB.swap swap swap defaults 0 0</p> <p># Reboot</p> <p>sudo reboot now</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#installing-dependencies","title":"Installing dependencies","text":"<p>Navigate to the directory where you will be installing the luxonis libraries using cd \\~/projects</p> <p>sudo nano install_dependencies.sh</p> <p>Copy the entire contents of the file: [install_dependencies.sh]{.underline}</p> <p>bash install_dependencies.sh</p> <p>echo \\\"export OPENBLAS_CORETYPE=ARMV8\\\" &gt;&gt; \\~/.bashrc</p> <p>echo \\'SUBSYSTEM==\\\"usb\\\", ATTRS{idVendor}==\\\"03e7\\\", MODE=\\\"0666\\\"\\' | sudo tee /etc/udev/rules.d/80-movidius.rules</p> <p>sudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#navigate-using-cd-to-the-folder-where-you-would-like-to-install-the","title":"Navigate using cd to the folder where you would like to install the","text":"<p>camera example files and requirements</p> <p>cd \\~/projects</p> <p>git clone [https://github.com/luxonis/depthai-python.git]{.underline}</p> <p>cd depthai-python/examples</p> <p>python3 install_requirements.py</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#if-you-want-to-test-the-camera-and-you-have-remote-desktop","title":"If you want to test the camera and you have remote desktop","text":"<p>[NoMachine]{.underline} already installed and the OAKD Lite is connected to JTN , run the following in the terminal on a NoMachine session</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#navigate-to-the-examples-folder-in-depthai-python-first-and-then","title":"Navigate to the examples folder in depthai-python first and then","text":"<p>cd ColorCamera</p> <p>python3 rgb_preview.py</p> <p>You should be able to see preview video on the No machine desktop</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#_5","title":"DonkeyCar Installation","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#installing-donkeycar-ai-framework","title":"Installing Donkeycar AI Framework","text":"<p>Lets Install the Donkeycar AI Framework</p> <p>If you are upgrading from Donkey3 then save the values from your calibration that</p> <p>you had on</p> <p>myconfig.py</p> <p>Then let\\'s remove the old donkeycar and d3 directories</p> <p>cd \\~/projects</p> <p>rm -rf donkeycar</p> <p>rm -rf d3</p> <p>If the projects directory was not created yet, mkdir projects</p> <p>Install more dependencies</p> <p>sudo apt-get install python3-dev python3-numpy python-dev libsdl-dev libsdl1.2-dev libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev libsdl1.2-dev libsmpeg-dev python-numpy subversion libportmidi-dev ffmpeg libswscale-dev libavformat-dev libavcodec-dev libfreetype6-dev libswscale-dev libjpeg-dev libfreetype6-dev</p> <p>pip install pygame</p> <p>Lets enable the use of synchronization of files with remote computers using rsync</p> <p>sudo apt-get install rsync</p> <p>This part will take a bit of time. Be patient, please keep in mind that you are using a low power</p> <p>single board computer (SBC).</p> <p>If you are curious if your SBC is really working, you can open another tab in the terminal window</p> <p>or a complete new terminal window, ssh to the JTN then execute the command top or htop</p> <p>look at the CPU utilization...</p> <p>Note I had problems installing Donkey with the latest version of pip (20.0.2). I had to revert</p> <p>to an earlier version of pip. See versions of pip here [https://pip.pypa.io/en/stable/news/]{.underline}</p> <p>On 28 May20, it worked. Keeping the line below for reference in case the problem happens again</p> <p># pip install --upgrade pip==18.1</p> <p>Install Donkey with</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#cd-projects","title":"cd \\~/projects","text":"<p>Get donkeycar from Github</p> <p>git clone https://github.com/autorope/donkeycar</p> <p>cd donkeycar</p> <p>cd \\~/projects</p> <p>git clone https://github.com/autorope/donkeycar</p> <p>cd donkeycar</p> <p>git fetch --all --tags -f</p> <p>git checkout 4.5.1</p> <p>pip install -e .[[nano]{.mark}]</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#pip3-install-e-nano","title":"pip3 install -e .[nano]","text":"<p>Proceed to [Create a Car]{.underline}</p> <p># 1/30/24</p> <p># [mlopezme@ucsd.edu]{.underline} []{.mark}</p> <p># Moises, did you change the instructions to have nano45? It was giving problems in WI24. Does it install Donkey 4.5.1? That is what we need</p> <p># For ECE MAE 148 when we are ready for the latest version of Donkey, let\\'s say using a docker container</p> <p>cd \\~/projects</p> <p>git clone https://github.com/autorope/donkeycar</p> <p>cd donkeycar</p> <p>git fetch --all --tags -f</p> <p>latestTag=\\$(git describe --tags `git rev-list --tags --max-count=1`)</p> <p>git checkout \\$latestTag</p> <p>pip install -e .[[nano45]{.mark}]</p> <p>It may take a while. You may not see progress on the terminal. You can ssh to the SBC</p> <p>and run the command top or htop or jtop from another terminal/tab</p> <p>Grab a coffee, go study something ...</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#_6","title":"DonkeyCar Installation","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#create-a-car","title":"Create a Car","text":"<p>Let's create a car on the path \\~/project/d4</p> <p>cd \\~/projects/donkeycar</p> <p>donkey createcar --path \\~/projects/d4</p> <p>If complains about old version of numpy and the install fails</p> <p>pip install numpy --upgrade</p> <p>using donkey v4.3.22 ...</p> <p>Creating car folder: /home/jetson/projects/d4</p> <p>making dir /home/jetson/projects/d4</p> <p>The version of the Donkey car may be newer than the one above...</p> <p># For Winter 2024</p> <p>Make sure the DonkeyCar is version 4.5.1. The latest version of the DonkeyCar (5.x) does not work at the Jetson Nano yet.</p> <p>You spent several hours on this configuration right?! Please make a backup of your uSD card - \"Backup of the uSD Card\"</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#modifying-camera","title":"Modifying Camera","text":"<p>Change the camera type to MOCK to enable us to test the system without a camera</p> <p>nano myconfig.py</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#camera","title":"CAMERA","text":"<p>CAMERA_TYPE = \\\"MOCK\\\" (PICAM|WEBCAM|CVCAM|CSIC|V4L|MOCK)</p> <p># if you have USB camera connected to the JTN , use WEBCAM</p> <p># And change this line so the Donkey can run using the web interface</p> <p>USE_JOYSTICK_AS_DEFAULT = False</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#_7","title":"DonkeyCar Installation","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#quick-test","title":"Quick Test","text":"<p>Lets test the Donkey AI framework install</p> <p>python manage.py drive</p> <p>Lets connect to the JTN by using a web browser from your PC</p> <p>[http://ucsdrobocar-xxx-yy:8887]{.underline}</p> <p>You should see a screen like this</p> <p>{width=\"4.6631517935258096in\" height=\"3.119792213473316in\"}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#we-stop-the-donkey-with-ctrl-c","title":"We stop the Donkey with Ctrl-C","text":"<p>Ctrl-C</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#modifying-joystick","title":"Modifying Joystick","text":"<p>Now let\\'s change the type of joystick we use with Donkey</p> <p>nano myconfig.py</p> <p>JOYSTICK</p> <p>[USE_JOYSTICK_AS_DEFAULT = True]{.mark} when starting the manage.py, when True, wil\\$</p> <p>JOYSTICK_MAX_THROTTLE = 0.5 this scalar is multiplied with the -1 to\\$</p> <p>JOYSTICK_STEERING_SCALE = 1.0 some people want a steering that is less\\$</p> <p>AUTO_RECORD_ON_THROTTLE = True if true, we will record whenever throttle\\$</p> <p>[CONTROLLER_TYPE=\\'F710\\']{.mark} (ps3|ps4|xbox|nimbus|wiiu|F710)</p> <p>python manage.py drive</p> <p>ex</p> <p>Starting vehicle...</p> <p>Opening /dev/input/js0...</p> <p>Device name: Logitech Gamepad F710</p> <p>recorded 10 records</p> <p>recorded 20 records</p> <p>recorded 30 records</p> <p>recorded 40 records</p> <p>erased last 100 records.</p> <p>E-Stop!!!</p> <p>recorded 10 records</p> <p>recorded 20 records</p> <p>recorded 30 records</p> <p>recorded 40 records</p> <p>recorded 50 records</p> <p>The Right Joystick is the Throttle, the Left Joystick is the Steering</p> <p>The Y Button deletes 5s of driving at the default configuration =100 records at 20 Hz</p> <p>The A Button is the emergency break</p> <p>Joystick Controls:</p> <p>+------------------+---------------------------+</p> <p>| control | action |</p> <p>+------------------+---------------------------+</p> <p>| start | toggle_mode |</p> <p>| B | toggle_manual_recording |</p> <p>| Y | erase_last_N_records |</p> <p>| A | emergency_stop |</p> <p>| back | toggle_constant_throttle |</p> <p>| R1 | chaos_monkey_on_right |</p> <p>| L1 | chaos_monkey_on_left |</p> <p>| circle | show_record_acount_status |</p> <p>| R2 | enable_ai_launch |</p> <p>| left_stick_horz | set_steering |</p> <p>| right_stick_vert | set_throttle |</p> <p>+------------------+---------------------------+</p> <p>If your joystick is not returning to neutral</p> <p>You can add a deadzone value</p> <p>on myconfig.py</p> <p>ex:</p> <p>NETWORK_JS_SERVER_IP = \\\"192.168.0.1\\\"when listening for network joystick cont\\$</p> <p>JOYSTICK_DEADZONE = 0.01 when non zero, this is the smallest throt\\$</p> <p>JOYSTICK_THROTTLE_DIR = -1.0 use -1.0 to flip forward/backward, use \\$</p> <p>Lets Integrate the JTN and PWM Controller into the RC Chassis</p> <p>Charge your LiPo Battery</p> <p>After you charge your Lithium Polymer (LiPo) battery(ries) - [some info here]{.underline}</p> <p>Connect the battery(ries) and batteries monitor/alarm</p> <p>Please do not use the batteries without the batteries monitor/alarms</p> <p>If you discharge a LiPO batteries below a threshold lets say 3.0</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#_8","title":"DonkeyCar Installation","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#vesc_1","title":"VESC","text":"<p>Ensure that you have already configured the VESC device using the VESC Tool Software</p> <p>Edit the myconfig.py to have these values</p> <p>DRIVE_TRAIN_TYPE = \\\"VESC\\\"</p> <p>VESC_MAX_SPEED_PERCENT =.2 ## Max speed as a percent of actual max speed</p> <p>VESC_SERIAL_PORT= \\\"/dev/ttyACM0\\\" ## check this val with ls /dev/tty*</p> <p>VESC_HAS_SENSOR= True</p> <p>VESC_START_HEARTBEAT= True</p> <p>VESC_BAUDRATE= 115200</p> <p>VESC_TIMEOUT= 0.05</p> <p>VESC_STEERING_SCALE = .5</p> <p>VESC_STEERING_OFFSET = .5</p> <p>DONKEY_GYM = False</p> <p>(we will leave the CAMERA_TYPE = \"MOCK\" for now to make sure we can drive the car with the VESC)</p> <p>Download the following files</p> <p>~~[https://drive.google.com/drive/folders/1SBzChXK2ebzPHgZBP_AIhVXJOekVc0r3?usp=sharing]{.underline}~~</p> <p>[https://drive.google.com/drive/folders/19TS3VyNXQPBSr41yiPaxpI1lnxClI2c8?usp=sharing]{.underline}</p> <p>And replace them on the Jetson in the locations shown in the images below. Note - to get the files on the jetson you can use SFTP (secure file transfer protocol):</p> <p>Examples on how to use SFTP:\\ sftp jetson@ucsdrobocar-148-xx.local</p> <p>cd To the directory you want to go to</p> <p>put /File/Path/On/Your/Computer</p> <p>alternatively</p> <p>get filename /File/Location/You/Want/Them/Go/On/Your/Computer</p> <p>To get a directory</p> <p>get -rf filename /File/Location/You/Want/Them/Go/On/Your/Computer</p> <p>type \\\"exit\\\" to disconnect</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#width4351609798775153in-height3276042213473316inwidth4307292213473316in-height3131517935258093in","title":"{width=\"4.351609798775153in\" height=\"3.276042213473316in\"}{width=\"4.307292213473316in\" height=\"3.131517935258093in\"}","text":"<p>Once these have been replaced, you should run</p> <p>python manage.py drive</p> <p>It should first throw a pyvesc import error. Follow the description in the terminal to install the needed libraries</p> <p>Then run it again. It should throw a permissions error. Follow the advice on how to fix the error with chmod</p> <p>Int(10) error bug fix (Credit Saimai Lau and Albert Chang):\\ When running python manage.py drive, the intermittent \\\"invalid literal for int() with base 10: \\'None\\' error is from the VESC package checking whether the version of the VESC is below 3.0, so we can comment out that part since we\\'re using 6.0 just do</p> <p>nano /home/jetson/projects/envs/donkey/lib/python3.6/site-packages/pyvesc/VESC/VESC.py</p> <p>and put # at the beginning of lines 38-40 Then \\^S to save and \\^X to exit</p> <p>{width=\"7.5in\" height=\"2.0833333333333335in\"}</p> <p>Error explanation: The self.get_firmware_version() get thes version by requesting it from the VESC and reading the replied bytes, but sometimes the data is lost or incomplete so the function returns \\'None\\' as the version. We already know the version is 6.0 so we don't need this function.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10doc/#oakd","title":"OAKD","text":"<p>Once you have the VESC driving the car, you will need to make sure you have set up the document by following the instructions labeled Configuring OAKD Lite found [earlier in this document]{.underline}</p> <p>Edit the myconfig.py camera type to OAKD</p> <p>CAMERA_TYPE = \\\"OAKD\\\"</p> <p>Make sure the camera is working by checking the images that are being saved to the /data/images directory.</p> <p>The easiest way to do this is to go to</p> <p>[http://localhost:8887]{.underline} while running donkeysim, and you should be able to see a livestream from the camera. Note - if several people are running donkeysim at the same time on the same wifi this interface may get buggy</p> <p>You can also do this by either:</p> <p>transferring the files to you laptop or virtual machine</p> <p>with scp, rsync, winscp (windows) or filezilla (mac)</p> <p>Or</p> <p>Using NoMachine by following the instructions found [here]{.underline} in this document</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/","title":"10docfinal","text":"<p>UCSD RoboCar ECE &amp; MAE 148</p> <p>Version 20.7 - 23Oct2022</p> <p>Prepared by</p> <p>Dr. Jack Silberman</p> <p>Department of Electrical and Computer Engineering</p> <p>and</p> <p>Dominic Nightingale</p> <p>Department of Mechanical and Aerospace Engineering</p> <p>University of California, San Diego</p> <p>9500 Gilman Dr, La Jolla, CA 92093</p> <p></p> <p></p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#table-of-contents","title":"Table of Contents","text":"<p>Table of Contents 2</p> <p>Introduction 4</p> <p>Single Board Computer (SBC) Basic Setup 5</p> <p>Jetson Nano (JTN) Configuration 5</p> <p>Jetson Xavier NX Configuration 5</p> <p>Hardware Setup 6</p> <p>Jetson Nano GPIO Header PINOUT 6</p> <p>VESC 7</p> <p>VESC Hardware V6.x 8</p> <p>Motor Detection 10</p> <p>Sensor Detection 14</p> <p>Enable Servo Control For Steering 16</p> <p>VESC Hardware V4.12 (if you have V6, skip this) 17</p> <p>PWM Controller 18</p> <p>Wiring adafruit board 19</p> <p>Detecting the PWM controller 20</p> <p>Emergency stop - Relay 21</p> <p>Logitech F710 controller 23</p> <p>Other JoyStick Controllers 23</p> <p>LD06 Lidar 25</p> <p>Laser Map 25</p> <p>Mechanical drawing 25</p> <p>Install OpenCV from Source 26</p> <p>DonkeyCar AI Framework 26</p> <p>Setting up the DonkeyCar AI Framework 26</p> <p>Create a virtual environment for the DonkeyCar AI Framework. 27</p> <p>Confirm openCV build from previous steps 28</p> <p>Tensorflow 29</p> <p>PyTorch 32</p> <p>Installing Donkeycar AI Framework 35</p> <p>Create a Car 36</p> <p>Donkeycar manage.py 37</p> <p>Modifying PWM board configuration 37</p> <p>Modifying Camera 37</p> <p>Quick Test 38</p> <p>Modifying Joystick 39</p> <p>Calibration of the Throttle and Steering 41</p> <p>Begin Calibration 42</p> <p>Saving car configuration file 44</p> <p>Driving the Robot to Collect data 46</p> <p>Backup of the uSD Card 50</p> <p>If needed, we have an uSD Card Image Ready for Plug and Play 52</p> <p>ROS with Docker 53</p> <p>Supporting material 54</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_1","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#introduction","title":"Introduction","text":"<p>This document was derived from the DIY RoboCar - Donkey Car Framework</p> <p>Reference information can be found at http://docs.donkeycar.com</p> <p>At UC San Diego\u2019s Introduction to Autonomous Vehicles class (ECE MAE148), we use an AI Framework called Donkey Car which is based on Deep Learning / Human behavior cloning as well as we do traditional programming using Robot Operating System (ROS2).</p> <p>DonkeyCar can be seen as the \u201chello world\u201d of affordable scaled autonomous cars</p> <p>We have added other features into our UCSD scale robot cars that are not found</p> <p>at the Default Donkey car build such as a wireless emergency off switch. Therefore, please follow</p> <p>the instructions found in this document vs. the default Donkey built.</p> <p>Another framework we use called UCSD Robocar is primarily maintained and developed by Dominic Nightingale right here at UC San Diego. UCSD Robocar uses ROS and ROS2 for controlling our scaled robot cars which can vary from traditional programming or machine learning to achieve an objective. The framework works with a vast selection of sensors and actuation methods in our inventory making it a robust framework to use across various platforms. Has been tested on 1/16, 1/10, 1/5 scaled robot cars and soon our go-karts.</p> <p>As August 2019 we transitioned from the single board computer (SBC) called Raspberry PI to the</p> <p>Nvidia Jetson Nano. If you are using a Raspberry PI, then search in this document for</p> <p>Raspberry PI (RPI or PI) Configuration.</p> <p>On 28Aug19, we updated the instructions to include Raspberry PI 4B</p> <p>Depending on your Single Board Computer, Jetson Xavier NX, Jetson Nano,</p> <p>then follow the related instructions.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_2","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#single-board-computer-sbc-basic-setup","title":"Single Board Computer (SBC) Basic Setup","text":"<p>We will be using the Ubuntu Linux distribution. In the class you have access to a virtual</p> <p>machine image file with Ubuntu.</p> <p>We won\u2019t install ROS2 directly into the SBC, we will be using Docker images and containers.</p> <p>You will install OpenCV from source as part of your learning on compiling and building</p> <p>software from source.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#jetson-nano-jtn-configuration","title":"Jetson Nano (JTN) Configuration","text":"<p>Instructions to configure the Jetson Nano</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#jetson-xavier-nx-jnx-configuration","title":"Jetson Xavier NX (JNX) Configuration","text":"<p>Instructions to Configure the Jetson Xaviver NX</p> <p>Archive location to previous JetPack versions</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#editing-remotely-with-jupyter-notebooks","title":"Editing Remotely with Jupyter Notebooks","text":"<p>Install Jupyter notebook on your Jetson:</p> sudo apt install jupyter-notebook <p>https://ljvmiranda921.github.io/notebook/2018/01/31/running-a-jupyter-notebook/</p> <p>Help document for editing using Jupyter notebook:</p> <p>Conifguring Jupyter Notebook on SSH</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#hardware-setup","title":"Hardware Setup","text":"<p>You should consider breaking the work on building the robots per team member:</p> <p>1)  Someone could start to build OpenCV GPU accelerated in parallel     while you build the robot. It will take several hours building     OpenCV from     source.     Try to divide the work by team members \u2026</p> <p>2)  Start designing, 3D Printing, Laser Cutting the parts</p> <p>As of Spring 2022, we upgraded all the ECE MAE 148 robots to use VESCs.</p> <p>If you are using a regular Electronic Speed Controller (ESC) vs. a VESC you may need to use a I2C PWM board to generate reliable PWM to the ESC and Steering Servo, look for the PWM Controller text below.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#jetson-nano-gpio-header-pinout","title":"Jetson Nano GPIO Header PINOUT","text":"<p>I2C and UART pins are connected to hardware and should not be reassigned. By default, all other pins</p> <p>(except power) are assigned as GPIO. Pins labeled with other functions are recommended functions if</p> <p>using a different device tree. Here\u2019s a spreadsheet map to RPi to help.</p> <p></p> <p>sudo usermod -aG i2c jetson</p> <p>sudo reboot now</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#vesc","title":"VESC","text":"<p>VESC is a super cool Electronic Speed Controller (ESC) that runs open source code with</p> <p>significantly more capabilities than a regular RC Car ESC.</p> <p>VESCs are very popular for electrical skateboards, DIY electrical scooters, and robotics.</p> <p>For robotics, one of the capabilities we will use the most is Odometry (speed and position)</p> <p>based on the sensors on brushless motors (sensored) or to some extent, specially using the latest VESCs and firmware, it is also available with brushless motors without sensors (sensorless).</p> <p>https://vesc-project.com/</p> <p>VESC Setup Instructions</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#logitech-f710-controller","title":"Logitech F710 controller","text":"<p>Place your Logitech F710 controller on the x mode</p> <p>(look for small switch in one of the controller face)</p> <p>Connect the USB JoyStick Dongle into the JTN and then list the input devices again</p> <p>ls /dev/input</p> <p>(env) jetson@ucsdrobocar00:~/projects/d3\\$ ls /dev/input</p> <p>by-id event0 event2 event4 mice mouse1</p> <p>by-path event1 event3 js0 mouse0</p> <p>We are looking for a js0</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#other-joystick-controllers","title":"Other JoyStick Controllers","text":"<p>JoyStick Controllers - either the Logitech F-10 or PS4</p> <p>Make sure your myconfig.py on your car directory reflects your controller</p> <p>At the SBC</p> <p>Connecting the LogiTech Controller is as easy as plugging in the USB dongle at the SBC.</p> <p>Or if you have a PS4 controller</p> <p>Connecting a PS4 Controller - Bluetooth</p> <p>Deactivate the virtual environment if you are using one</p> <p>deactivate</p> <p>sudo apt-get install bluetooth libbluetooth3 libusb-dev</p> <p>sudo systemctl enable bluetooth.service</p> <p>Need these for XBox Controller, skip for PS4</p> <p>sudo apt install sysfsutils</p> <p>sudo nano /etc/sysfs.conf</p> <p>add this line at the end of the file:</p> <p>/module/bluetooth/parameters/disable_ertm=1</p> <p>Reboot your SBC and see if ertm was disabled.</p> <p>cat /sys/module/bluetooth/parameters/disable_ertm</p> <p>The result should print Y</p> <p>At the PS4 controller</p> <p>Press the Share and PS buttons at the same time.</p> <p>The controller light will flash like a strobe light. That means it is in the pairing mode</p> <p>If the SBC is not seeing the PS4, you should try a mUSB cable between the SBC and the</p> <p>PS4 controller.</p> <p>At the SBC</p> <p>sudo bluetoothctl</p> <p>agent on</p> <p>default-agent</p> <p>scan on</p> <p>If your controller is off, Press Share/PlayStation</p> <p>example of a PS4 controller mac address</p> <p>Device A6:15:66:D1:46:1B Alias: Wireless Controller</p> <p>connect YOUR_MAC_ADDRESS</p> <p>trust YOUR_MAC_ADDRESS</p> <p>quit</p> <p>If your ps4 turns off, turn it on again by pressing PS</p> <p>If you want to check controller was connected</p> <p>ls /dev/input</p> <p>and see if js0 is listed.</p> <p>Lets test the joystick in a linux machine</p> <p>sudo apt-get update</p> <p>sudo apt-get install -y jstest-gtk</p> <p>jstest /dev/input/js0</p> <p>Turn off your PS4 controller by pressing and holding PS</p> <p>To remove a device, let's say another JoyStick that you don\u2019t use anymore</p> <p>bluetoothctl</p> <p>paired-devices</p> <p>remove THE_CONTROLLER_MAC_ADDRESS</p> <p>I had to try the method above twice, I rebooted the SBC in between</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_3","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#ld06-lidar","title":"LD06 Lidar","text":"<p>Datasheet for LD06 lidar: datasheet</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#laser-map","title":"Laser Map","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#mechanical-drawing","title":"Mechanical drawing","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#previous-versions-of-hardware","title":"Previous Versions of Hardware","text":"<p>Skip the PWM Controller and EMO starting in 2022 Summer II. Left here for people using these as low cost alternative robot.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#pwm-controller","title":"PWM Controller","text":"<p>If you are using a VESC, we don\u2019t use the PWM board, you can skip this part.</p> <p>These PWM controllers are used only if one has a regular ESC (not the cool VESC).</p> <p>We are following the standard from RC CAR world, Channel 1 for Steering, Channel 2 for Throttle</p> <p>Note:The default DonkeyCar build uses Channels 0 and 1</p> <p>The UCSDRoboCar has at least two actuators. A steering servo and the DC motor connected to an Electronics Speed Controller (ESC). These are controlled by PWM (Pulse Width Modulation).</p> <p>We use PWM Controller to generate the PWM signals, a device similar to the one in the picture below</p> <p></p> <p>Shutdown the JTN if it is on by typing this command</p> <p>sudo shutdown -h now</p> <p>Connect the Steering Servo to the Channel 1</p> <p>Connect the Throttle (Electronic Speed Controller ESC) to Channel 2</p> <p>Observe the orientation of the 3 wires connector coming from the Steering Servo and ESC. Look for a the black or brown wire, that is the GND (-).</p> <p>Lets install the PWM Controller</p> <p>https://www.jetsonhacks.com/nvidia-jetson-nano-j41-header-pinout/</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#wiring-adafruit-board","title":"Wiring adafruit board","text":"<p>You need to connect the following pins between the JTN and the PWM board:</p> <p>Disconnect the power to the JTN</p> <p>+3.3v, the two I2C pins (SDA and SCL) and ground</p> <p>The 3.3V from the JTN goes to the VCC at the PWM board</p> <p>Note: for the ground connection you need to skip one pin (skip pin 7)</p> <ul> <li> <p>3.3V - pin 1</p> </li> <li> <p>SDA - pin 3</p> </li> <li> <p>SCL- pin 5</p> </li> <li> <p>No_Connect (Skip) - pin 7</p> </li> <li> <p>Ground - pin 9</p> </li> </ul> <p></p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_4","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#detecting-the-pwm-controller","title":"Detecting the PWM controller","text":"<p>Lets detect the PWM controller</p> <p>sudo i2cdetect -r -y 1</p> <p>0 1 2 3 4 5 6 7 8 9 a b c d e f</p> <p>00: -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>70: 70 -- -- -- -- -- -- --</p> <p>If you see the 40 and 70 above, the JTN is communicating with the PWM board</p> <p>If you want to save some typing everytime you log into the JTN, add it to the end of .bashrc</p> <p>nano ~/.bashrc</p> <p>source /projects/envs/donkey/bin/activate</p> <p>cd ~/projects/d3</p> <p>Test is with a reboot</p> <p>sudo reboot now</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_5","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#emergency-stop-relay","title":"Emergency stop - Relay","text":"<p>If using an ESC vs. VESC follow this, with VESC skip</p> <p>Connecting the Emergency Stop Circuit and Batteries into the Robot</p> <p>For this part of your robot, you will have to do some hacking. That is part of the class.</p> <p>The instructor will discuss the principle of the circuit and how to implement it with the component in your robot kit.</p> <p>Long story short, the PWM Controller we use has a disable pin. If the correct logic is applied to it for example Logic 1 or 0 (3.3V or 0V) it will disable the PWM signals and the UCSDRoboCar will stop.</p> <p>Think why one needs a separate EMO circuit vs. relying on the software, operating system and computer communicating with the PWM controller that then send PWM pulses to the actuators (steering servo and driving DC motor by the electronics speed controller)</p> <p>First search on the web and read the datasheet of the emergency stop switch (EMO) components provided with the robot kit and discuss with your teammates how the EMO will work. You got two main components to build the WireLess EMO switch:</p> <p>1)  A Wireless Relay with wireless remote controls</p> <p>2)  A Red/Blue high power LED. This is to help the user know if the car     is disable (Red) or Enabled (Blue).</p> <p>Do some discussion with your Team and pay attention to the Lecture explanations:</p> <ul> <li> <p>What is the disable pin of the PWM controller?</p> </li> <li> <p>Does it disable logic 1 or 0?</p> </li> <li> <p>How to wire the wireless relay to provide the logic you need to   disable the PWM controller? (1 or 0)</p> </li> <li> <p>What pin at the Single Board Computer (SBC) can provide the logic you   need to disable the PWM controller</p> </li> <li> <p>How to connect the LEDs (Blue and Red) to indicate (BLUE - enabled),   (RED - power is on / robot is disabled).</p> </li> <li> <p>What is the fail safe situation, using the normally closed or normally   open pins of the wireless relay to disable the PWM controller?</p> </li> <li> <p>Note: The power to the PWM controller, that powers the LEDs, comes   from ESC (Electronics Speed Controller). Therefore, you have to   connect the robot batteries to the ESC and the ESC to the PWM   controller. We are using channel 2 for the ESC. Channel 1 for   the Steering. Then you need to power the ESC for the circuit to work   ...</p> </li> </ul> <p>After you see that the EMO is working, i.e. wireless remote control disables the PWM, and the LEDs light up as planned, then you need to document your work. Please use a schematic software such as Fritzing (http://fritzing.org/home/) to document your electrical schematic.</p> <p>It may seem we did the opposite way; document after your build. In our case, you learned by looking at components information, thinking about the logic, and experimenting. Since you are an engineer you need to document even if it was a hack / test try first\u2026</p> <p>Working in a company you may do fast prototyping first then document your work when the risk is low. On larger projects you design, make schematics, diagrams, drawings, work instructions, then build it. Keep that in mind!</p> <p>Now you can go drive your robot to collect data. Make sure to keep the</p> <p>EMO handy and used when needed!</p> <p>Also keep in mind that \\&lt;X&gt; on your controller is like an emergency break.</p> <p>When you run Donkey it will display the functions associated with the joystick buttons.</p> <p>Read and remember them</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_6","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_7","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#install-opencv-from-source","title":"Install OpenCV from Source","text":"<p># Installing an Open Source Computer Vision (OpenCV) package with CUDA Support</p> <p># As of Jan 2020, NVIDIA is not providing OpenCV optimized to use CUDA (GPU acceleration).</p> <p># Search the web if you are curious why.</p> <p># https://forums.developer.nvidia.com/t/opencv-cuda-python-with-jetson-nano/72902</p> <p>#Here are the instructions to build OpenCV from source</p> <p>It will take an approximate 4 hrs to install opencv</p> <p>1. jtop</p> <p>2. 4</p> <p>3. Add 6 GB of swap space and enable</p> <p>4. cd ~</p> <p>5. nano install_opencv.sh</p> <p>6. Copy the entire contents of the attached file</p> <p>7. bash install_opencv.sh</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#donkeycar-ai-framework","title":"DonkeyCar AI Framework","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#setting-up-the-donkeycar-ai-framework","title":"Setting up the DonkeyCar AI Framework","text":"<p>Reference http://docs.donkeycar.com</p> <p>Make sure that the OpenCV you want to use supporting CUDA is already available as a system</p> <p>wide package.</p> <p>Remember that when you are compiling and building software from source, it may take a few</p> <p>hours ...</p> <p>SSH into the Single Board Computer (SBC) e.g., RPI, JTN, JNX, etc.</p> <p># Install some packaged, some may be already installed</p> <p>sudo apt update -y</p> <p>sudo apt upgrade -y</p> <p>sudo usermod -aG dialout jetson</p> <p>#If packages are being held back</p> <p>sudo apt-get --with-new-pkgs upgrade</p> <p>sudo apt-get install -y build-essential python3 python3-dev python3-pip libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran libxslt1-dev libxml2-dev libffi-dev libcurl4-openssl-dev libssl-dev libpng-dev libopenblas-dev openmpi-doc openmpi-bin libopenmpi-dev libopenblas-dev git nano</p> <p>Install RPi.GPIO clone for Jetson Nano</p> <p>https://github.com/NVIDIA/jetson-gpio</p> <p>pip3 install Jetson.GPIO</p> <p>If the pip install complains about ownership of the directory*</p> <p>then execute the following command</p> <p>sudo chown -R jetson:jetson /home/jetson/.cache/pip</p> <p>ex:</p> <p>WARNING: The directory '/home/jetson/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.</p> <p>WARNING: The directory '/home/jetson/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of</p> <p>that directory. If executing pip with sudo, you may want sudo's -H flag.</p> <p>If pip breaks for some reason, you can reinstall it with the following lines</p> <p>python3 -m pip uninstall pip</p> <p>sudo apt install python3-pip --reinstall</p> <p>If the install request elevated privileges, execute the following command</p> <p>sudo pip3 install Jetson.GPIO</p> <p>if pip has a new version</p> <p>pip3 install --upgrade pip</p> <p>Let\u2019s make sure the user jetson can use gpio</p> <p>sudo groupadd -f -r gpio</p> <p>sudo usermod -a -G gpio jetson</p> <p>sudo cp /opt/nvidia/jetson-gpio/etc/99-gpio.rules /etc/udev/rules.d/</p> <p>28Jan20 - did not work with JetPack3.4</p> <p>15May21- did not work with JetPack4.5</p> <p>19Oct21 - did not work with JetPack4.6</p> <p>18Sep22 - did not work with JetPack4.6.2</p> <p>Will get back to it later if the jetson user can not access GPIO</p> <p>sudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger</p> <p>We want to have control over the versions of each software library to minimize the framework from</p> <p>breaking after system-wide upgrades. Therefore, lets create a virtual environment for the</p> <p>DonkeyCar.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#create-a-virtual-environment-for-the-donkeycar-ai-framework","title":"Create a virtual environment for the DonkeyCar AI Framework.","text":"<p>If you want the virtual environment to be under the user\u2019s home directory, make sure to be on the</p> <p>home directory for user jetson</p> <p>If you have not done so, lets create a directory to store our projects and one subdirectory</p> <p>to store virtual environments</p> <p>cd ~</p> <p>mkdir projects</p> <p>cd projects</p> <p>mkdir envs</p> <p>cd envs</p> <p>pip3 install virtualenv</p> <p>if complains about user permission</p> <p>pip3 install virtualenv --user</p> <p>We will create a virtual environment called donkey since our AI framework is</p> <p>based on the Donkey Car project</p> <p>Since your SBC will be initially dedicated to the class AI framework (Donkey), at least until</p> <p>your custom project starts, let\u2019s activate the donkey virtual env automatically every time the user</p> <p>Jetson logs into the SBC. We can remove this settings later if needed when using ROS2</p> <p>echo \"source ~/projects/envs/donkey/bin/activate\" &gt;&gt; ~/.bashrc</p> <p>source ~/.bashrc</p> <p>When a virtual environment is active, you should see (name_of_virtual_enviroment) in front of the terminal prompt.</p> <p>ex:</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:~\\$</p> <p>At this point, using pip and pip3 should be the same as using pip3 by default in this virtual environment.</p> <p>https://docs.donkeycar.com/guide/robot_sbc/setup_jetson_nano/ 46</p> <p>#it is necessary to create a link to it</p> <p># Go to the folder where OpenCV's native library is built</p> <p>#cd /usr/local/lib/python3.6/site-packages/cv2/python-3.6</p> <p># Rename</p> <p>#mv cv2.cpython-36m-xxx-linux-gnu.so cv2.so</p> <p># Go to your virtual environments site-packages folder if previously set</p> <p>#cd ~/env/lib/python3.6/site-packages/</p> <p># Or just go to your home folder if not set a venv site-packages folder</p> <p>#cd ~</p> <p># Symlink the native library</p> <p>#ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so cv2.so</p> <p>#NOTE that it is almost mandatory to create a virtual environment in order to properly install</p> <p># tensorflow, scipy and keras, and always a best practice.</p> <p>cd ~/projects/envs/donkey/lib/python3.6/site-packages/</p> <p>ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so cv2.so</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_8","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#confirm-that-opencv-built-from-previous-steps-is-working-on-the-virtual-environment-donkey","title":"Confirm that OpenCV built from previous steps is working on the virtual environment Donkey","text":"<p># Testing to see if OpenCV is installed in the virtual env.</p> <p>python3 -c 'import cv2 as cv; print(cv.__version__)'</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:~/projects/envs/donkey\\$ python3 -c 'import cv2 as cv; print(cv.__version__)'</p> <p>4.6.0</p> <p># We won't use Python2, but just in case one will need it for some reason</p> <p>python2 -c 'import cv2 as cv; print(cv.__version__)'</p> <p>We are not done installing software yet. We need to install more dependencies..</p> <p>Make sure you have the donkey virtual environment activated</p> <p>Remember some of these installs may take a while. It does not mean that the SBC is frozen, you</p> <p>can see that the CPU is busy with top, htop, or jtop</p> <p>source ~/projects/envs/donkey/bin/activate</p> <p>pip3 install -U pip testresources setuptools</p> <p>pip3 install -U futures==3.1.1 protobuf==3.12.2 pybind11==2.5.0</p> <p>pip3 install -U cython==0.29.21 pyserial</p> <p>pip3 install -U future==0.18.2 mock==4.0.2 h5py==2.10.0 keras_preprocessing==1.1.2 keras_applications==1.0.8 gast==0.3.3</p> <p>pip3 install -U absl-py==0.9.0 py-cpuinfo==7.0.0 psutil==5.7.2 portpicker==1.3.1 six requests==2.24.0 astor==0.8.1 termcolor==1.1.0 wrapt==1.12.1 google-pasta==0.2.0</p> <p>pip3 install -U gdown</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#tensorflow","title":"Tensorflow","text":"<p>Now let's install Tensorflow (Artificial Neural Network software).</p> <p>\u201cTensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.\u201d</p> <p>Lets install Tensorflow enabled for GPU acceleration</p> <p>Another chance for you to study while software is being installed\u2026</p> <p>Background information here</p> <p>https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html</p> <p>Remember you are using a low power SBC, depending on the size of the software it takes a while</p> <p>We are installing Tensorflow outside the virtual environment so it is available for other uses</p> <p>Here is another chance for you to study while software is being installed\u2026</p> <p>Background information here</p> <p>We are using JetPack 4.5 because the new DonkeyCar release was breaking the install with JetPack4.6.2. It requires Python 7 and newer.</p> <p>Let's stick with JetPack4.5 for now</p> <p>https://developer.download.nvidia.com/compute/redist/jp/v45/tensorflow/</p> <p>As of 18Sep22</p> <p>tensorflow-1.15.4+nv20.12-cp36-cp36m-linux_aarch64.whl 218MB 2020-12-18 14:54:04</p> <p>tensorflow-2.3.1+nv20.12-cp36-cp36m-linux_aarch64.whl 264MB 2020-12-18 14:54:06</p> <p>tensorflow-1.15.5+nv21.2-cp36-cp36m-linux_aarch64.whl 218MB 2021-02-26 16:10:00</p> <p>tensorflow-2.4.0+nv21.2-cp36-cp36m-linux_aarch64.whl 273MB 2021-02-26 16:10:14</p> <p>tensorflow-1.15.5+nv21.3-cp36-cp36m-linux_aarch64.whl 218MB 2021-03-25 18:14:17</p> <p>tensorflow-2.4.0+nv21.3-cp36-cp36m-linux_aarch64.whl 273MB 2021-03-25 18:14:48</p> <p>tensorflow-1.15.5+nv21.4-cp36-cp36m-linux_aarch64.whl 218MB 2021-04-26 20:36:59</p> <p>tensorflow-2.4.0+nv21.4-cp36-cp36m-linux_aarch64.whl 273MB 2021-04-26 20:38:13</p> <p>tensorflow-1.15.5+nv21.5-cp36-cp36m-linux_aarch64.whl 218MB 2021-05-20 20:19:08</p> <p>tensorflow-2.4.0+nv21.5-cp36-cp36m-linux_aarch64.whl 274MB 2021-05-20 20:19:20</p> <p>tensorflow-1.15.5+nv21.6-cp36-cp36m-linux_aarch64.whl 220MB 2021-06-29 18:18:14</p> <p>tensorflow-2.5.0+nv21.6-cp36-cp36m-linux_aarch64.whl 293MB 2021-06-29 18:18:43</p> <p># This will install the latest tensorflow compatible with the Jet Pack as a system package</p> <p>Alternatively if you want to chose a particular version:</p> <p>#pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v45 tensorflow==2.3.1</p> <p>Previous versions install</p> <p>pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v45 tensorflow==2.3.1</p> <p>pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v45 tensorflow==2.4.0</p> <p>Remember you are using a low power SBC, depending on the size of the software it takes a while</p> <p>Lets verify that Tensorflow installed correctly</p> <p>python3</p> <p>import tensorflow</p> <p>exit()</p> <p>No errors should be reported</p> <p>If you get errors importing Tensorflow 2.5.0, try these</p> <p>pip install numpy==1.19.2</p> <p>ex:</p> <p>If you see info with libcuda it means, Tensorflow will be accelerated using the CUDA</p> <p>cores of the SBC\u2019s GPU</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:~/projects\\$ python3</p> <p>Python 3.6.9 (default, Jun 29 2022, 11:45:57)</p> <p>[GCC 8.4.0] on linux</p> <p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p> <p>&gt;&gt;&gt; import tensorflow</p> <p>2022-08-09 12:19:25.285765: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2</p> <p>&gt;&gt;&gt; exit()</p> <p>Verifying that TensorRT was installed</p> <p>sudo apt-get update</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:~\\$ sudo apt-get install tensorrt</p> <p>Reading package lists... Done</p> <p>Building dependency tree</p> <p>Reading state information... Done</p> <p>tensorrt is already the newest version (7.1.3.0-1+cuda10.2).</p> <p>0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.</p> <p>dpkg -l | grep TensorRT</p> <p>arm64 Meta package of TensorRT</p> <p>ii uff-converter-tf 7.1.3-1+cuda10.2 arm64 UFF converter for TensorRT package</p> <p># Periodically the versions of tensorflow, cuDNN / CUDA give us conflict. Here is a list of compatibility</p> <p>https://www.tensorflow.org/install/sourcegpu</p> <p>Installing pycuda - it will take a while again\u2026</p> <p>pip3 install pycuda</p> <p>If you are having errors installing pycuda use the following command:</p> <p>pip3 install pycuda==2020.1</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#pytorch","title":"PyTorch","text":"<p>Lets install PyTorch too</p> <p>\u201cAn open source machine learning framework that accelerates the path from research prototyping to production deployment\u201c</p> <p>Again, these steps will take some time. Use your time wisely</p> <p>cd ~/projects</p> <p>wget https://nvidia.box.com/shared/static/p57jwntv436lfrd78inwl7iml6p13fzh.whl</p> <p>cp p57jwntv436lfrd78inwl7iml6p13fzh.whl torch-1.8.0-cp36-cp36m-linux_aarch64.whl</p> <p>pip3 install torch-1.8.0-cp36-cp36m-linux_aarch64.whl</p> <p>sudo apt-get install libjpeg-dev zlib1g-dev libpython3-dev libavcodec-dev libavformat-dev libswscale-dev</p> <p>git clone -b v0.9.0 https://github.com/pytorch/vision torchvision</p> <p>cd torchvision</p> <p>python setup.py install</p> <p>cd ../</p> <p># it will take a good while again. Keep studying other things\u2026</p> <p>Testing Pythorch install</p> <p>python</p> <p>import torch</p> <p>print(torch.__version__)</p> <p>exit()</p> <p>ex:</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:~/projects\\$ python</p> <p>Python 3.6.9 (default, Jan 26 2021, 15:33:00)</p> <p>[GCC 8.4.0] on linux</p> <p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p> <p>&gt;&gt;&gt; import torch</p> <p>&gt;&gt;&gt; print(torch.__version__)</p> <p>1.8.0</p> <p>&gt;&gt;&gt; exit()</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:~/projects\\$</p> <p>One more test</p> <p>pip3 show torch</p> <p>ex:</p> <p>Name: torch</p> <p>Version: 1.8.0</p> <p>Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration</p> <p>Home-page: https://pytorch.org/</p> <p>Author: PyTorch Team</p> <p>Author-email: packages@pytorch.org</p> <p>License: BSD-3</p> <p>Location: /home/jetson/projects/envs/donkey/lib/python3.6/site-packages</p> <p>Requires: dataclasses, typing-extensions, numpy</p> <p>Required-by: torchvision</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_9","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#as-of-summer-ii-2022-we-are-using-a-new-stereo-camera-from-luxonis","title":"As of Summer II 2022, we are using a new Stereo Camera from Luxonis","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#configuring-oakd-lite","title":"Configuring OAKD Lite","text":"<p>Open a terminal window and run the following commands:</p> <p>sudo apt update &amp;&amp; sudo apt upgrade</p> <p># after upgrades</p> <p>sudo reboot now</p> <p>If you have not added the extra swap space while building OpenCV, please add it</p> <p>You can use jtop to add more swap space using the left and right keys and clicking the plus button</p> <p>jtop</p> <p>4</p> <p></p> <p>Add 4G of swap and press \\&lt;S&gt; to enable it.</p> <p></p> <p>Alternatively you can use the command line</p> <p># Disable ZRAM:</p> <p>sudo systemctl disable nvzramconfig</p> <p># Create 4GB swap file</p> <p>sudo fallocate -l 4G /mnt/4GB.swap</p> <p>sudo chmod 600 /mnt/4GB.swap</p> <p>sudo mkswap /mnt/4GB.swap</p> <p>If you have an issue with the final command, you can try the following:</p> <p>sudo nano /etc/fstab</p> <p># Add this line at the bottom of the file</p> <p>/mnt/4GB.swap swap swap defaults 0 0</p> <p># Reboot</p> <p>sudo reboot now</p> <p>#Installing dependencies</p> <p>Navigate to the directory where you will be installing the luxonis libraries using cd ~/projects</p> <p>sudo nano install_dependencies.sh</p> <p>Copy the entire contents of the file: install_dependencies.sh</p> <p>bash install_dependencies.sh</p> <p>echo \"export OPENBLAS_CORETYPE=ARMV8\" &gt;&gt; ~/.bashrc</p> <p>echo 'SUBSYSTEM==\"usb\", ATTRS{idVendor}==\"03e7\", MODE=\"0666\"' | sudo tee /etc/udev/rules.d/80-movidius.rules</p> <p>sudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger</p> <p>#Navigate using cd to the folder where you would like to install the camera example files and requirements</p> <p>cd ~/projects</p> <p>git clone https://github.com/luxonis/depthai-python.git</p> <p>cd depthai-python/examples</p> <p>python3 install_requirements.py</p> <p>#If you want to test the camera and you have remote desktop NoMachine already installed and the OAKD Lite is connected to JTN , run the following in the terminal on a NoMachine session</p> <p>#Navigate to the examples folder in depthai-python first and then</p> <p>cd ColorCamera</p> <p>python3 rgb_preview.py</p> <p>You should be able to see preview video on the No machine desktop</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_10","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#installing-donkeycar-ai-framework","title":"Installing Donkeycar AI Framework","text":"<p>Lets Install the Donkeycar AI Framework</p> <p>If you are upgrading from Donkey3 then save the values from your calibration that</p> <p>you had on</p> <p>myconfig.py</p> <p>Then let's remove the old donkeycar and d3 directories</p> <p>cd ~/projects</p> <p>rm -rf donkeycar</p> <p>rm -rf d3</p> <p>If the projects directory was not created yet, mkdir projects</p> <p>#cd ~/projects</p> <p>Get donkeycar from Github</p> <p>git clone https://github.com/autorope/donkeycar</p> <p>cd donkeycar</p> <p>cd ~/projects</p> <p>git clone https://github.com/autorope/donkeycar</p> <p>cd donkeycar</p> <p>git fetch --all --tags -f</p> <p>git checkout 4.5.1</p> <p>pip install -e .[nano]</p> <p>Install more dependencies</p> <p>sudo apt-get install python3-dev python3-numpy python-dev libsdl-dev libsdl1.2-dev libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev libsdl1.2-dev libsmpeg-dev python-numpy subversion libportmidi-dev ffmpeg libswscale-dev libavformat-dev libavcodec-dev libfreetype6-dev libswscale-dev libjpeg-dev libfreetype6-dev</p> <p>pip install pygame</p> <p>Lets enable the use of synchronization of files with remote computers using rsync</p> <p>sudo apt-get install rsync</p> <p>This part will take a bit of time. Be patient, please keep in mind that you are using a low power</p> <p>single board computer (SBC).</p> <p>If you are curious if your SBC is really working, you can open another tab in the terminal window</p> <p>or a complete new terminal window, ssh to the JTN then execute the command top or htop</p> <p>look at the CPU utilization\u2026</p> <p>Note I had problems installing Donkey with the latest version of pip (20.0.2). I had to revert</p> <p>to an earlier version of pip. See versions of pip here https://pip.pypa.io/en/stable/news/</p> <p>On 28 May20, it worked. Keeping the line below for reference in case the problem happens again</p> <p># pip install --upgrade pip==18.1</p> <p>Install Donkey with</p> <p>#pip3 install -e .[nano]</p> <p>Proceed to Create a Car</p> <p># 1/30/24</p> <p># mlopezme@ucsd.edu </p> <p># Moises, did you change the instructions to have nano45? It was giving problems in WI24. Does it install Donkey 4.5.1? That is what we need</p> <p># For ECE MAE 148 when we are ready for the latest version of Donkey, let's say using a docker container</p> <p>cd ~/projects</p> <p>git clone https://github.com/autorope/donkeycar</p> <p>cd donkeycar</p> <p>git fetch --all --tags -f</p> <p>latestTag=\\$(git describe --tags `git rev-list --tags --max-count=1`)</p> <p>git checkout \\$latestTag</p> <p>pip install -e .[nano45]</p> <p>It may take a while. You may not see progress on the terminal. You can ssh to the SBC</p> <p>and run the command top or htop or jtop from another terminal/tab</p> <p>Grab a coffee, go study something ...</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_11","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#create-a-car","title":"Create a Car","text":"<p>Let\u2019s create a car on the path ~/project/d4</p> <p>cd ~/projects/donkeycar</p> <p>donkey createcar --path ~/projects/d4</p> <p>If complains about old version of numpy and the install fails</p> <p>pip install numpy --upgrade</p> <p>________ ______ _________</p> <p>___ __ \\______________ /___________ __ __ ____/_____ ________</p> <p>__ / / / __ \\ __ \\ //_/ _ \\ / / / _ / _ __ `/_ ___/</p> <p>_ /_/ // /_/ / / / / ,\\&lt; / __/ /_/ / / /___ / /_/ /_ /</p> <p>/_____/ \\___//_/ /_//_/|_| \\__/_\\_, / \\___/ \\_,_/ /_/</p> <p>/____/</p> <p>using donkey v4.3.22 ...</p> <p>Creating car folder: /home/jetson/projects/d4</p> <p>making dir /home/jetson/projects/d4</p> <p>The version of the Donkey car may be newer than the one above\u2026</p> <p># For Winter 2024</p> <p>Make sure the DonkeyCar is version 4.5.1. The latest version of the DonkeyCar (5.x) does not work at the Jetson Nano yet.</p> <p>You spent several hours on this configuration right?! Please make a backup of your uSD card - \u201cBackup of the uSD Card\u201d</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_12","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#if-you-are-using-a-pwm-board-with-a-esc-vs-a-vesc","title":"If you are using a PWM board with a ESC vs. a VESC","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#starting-on-fall22-we-use-a-vesc-for-controlling-the-bldc-motor-skip-setting-the-pwm-board-we-left-here-for-people-that-may-want-to-use-it-on-their-own-robot","title":"Starting on FALL\u201922, we use a VESC for controlling the BLDC motor. Skip setting the PWM board. We left here for people that may want to use it on their own robot","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#again-if-you-using-an-esc-skip-the-pwm-board-setup","title":"Again, if you using an ESC skip the PWM board setup","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#modifying-pwm-board-configuration","title":"#Modifying PWM board configuration","text":"<p>#Now we need to edit the myconfig.py to change the default bus number for the PWM board</p> <p>#(PCA9685)</p> <p>#nano myconfig.py</p> <p>#Jetson Nano: set PCA9685_I2C_BUSNUM = 1</p> <p>#Remove the comment from the line; \u201c\u201d and add \u201c1\u201d to the BUSNUM</p> <p>#PCA9685_I2C_BUSNUM = 1 None \u2026</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#modifying-camera","title":"Modifying Camera","text":"<p>Change the camera type to MOCK to enable us to test the system without a camera</p> <p>nano myconfig.py</p> <p>#CAMERA</p> <p>CAMERA_TYPE = \"MOCK\" (PICAM|WEBCAM|CVCAM|CSIC|V4L|MOCK)</p> <p># if you have USB camera connected to the JTN , use WEBCAM</p> <p># And change this line so the Donkey can run using the web interface</p> <p>USE_JOYSTICK_AS_DEFAULT = False</p> <p>In summary you change these 3 lines in the myconfig.py to be able to test your Donkey installation</p> <p># if using the PWM board the PWM board and ESC vs. VESC</p> <p>PCA9685_I2C_BUSNUM = 1</p> <p>CAMERA_TYPE = \"MOCK\"</p> <p>USE_JOYSTICK_AS_DEFAULT = False</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_13","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#quick-test","title":"Quick Test","text":"<p>Lets test the Donkey AI framework install</p> <p>python manage.py drive</p> <p>Adding part PWMSteering.</p> <p>Adding part PWMThrottle.</p> <p>Tub does NOT exist. Creating a new tub...</p> <p>New tub created at: /home/jetson/projects/d3/data/tub_1_19-08-05</p> <p>Adding part TubWriter.</p> <p>You can now go to \\&lt;your pi ip address&gt;:8887 to drive your car.</p> <p>Starting vehicle...</p> <p>8887</p> <p>Lets connect to the JTN by using a web browser from your PC</p> <p>http://ucsdrobocar-xxx-yy:8887</p> <p>You should see a screen like this</p> <p></p> <p>#We stop the Donkey with Ctrl-C</p> <p>Ctrl-C</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#modifying-joystick","title":"Modifying Joystick","text":"<p>Now let's change the type of joystick we use with Donkey</p> <p>nano myconfig.py</p> <p>JOYSTICK</p> <p>USE_JOYSTICK_AS_DEFAULT = True when starting the manage.py, when True, wil\\$</p> <p>JOYSTICK_MAX_THROTTLE = 0.5 this scalar is multiplied with the -1 to\\$</p> <p>JOYSTICK_STEERING_SCALE = 1.0 some people want a steering that is less\\$</p> <p>AUTO_RECORD_ON_THROTTLE = True if true, we will record whenever throttle\\$</p> <p>CONTROLLER_TYPE='F710' (ps3|ps4|xbox|nimbus|wiiu|F710)</p> <p>python manage.py drive</p> <p>ex</p> <p>Starting vehicle...</p> <p>Opening /dev/input/js0...</p> <p>Device name: Logitech Gamepad F710</p> <p>recorded 10 records</p> <p>recorded 20 records</p> <p>recorded 30 records</p> <p>recorded 40 records</p> <p>erased last 100 records.</p> <p>E-Stop!!!</p> <p>recorded 10 records</p> <p>recorded 20 records</p> <p>recorded 30 records</p> <p>recorded 40 records</p> <p>recorded 50 records</p> <p>The Right Joystick is the Throttle, the Left Joystick is the Steering</p> <p>The Y Button deletes 5s of driving at the default configuration =100 records at 20 Hz</p> <p>The A Button is the emergency break</p> <p>Joystick Controls:</p> <p>+------------------+---------------------------+</p> <p>| control | action |</p> <p>+------------------+---------------------------+</p> <p>| start | toggle_mode |</p> <p>| B | toggle_manual_recording |</p> <p>| Y | erase_last_N_records |</p> <p>| A | emergency_stop |</p> <p>| back | toggle_constant_throttle |</p> <p>| R1 | chaos_monkey_on_right |</p> <p>| L1 | chaos_monkey_on_left |</p> <p>| circle | show_record_acount_status |</p> <p>| R2 | enable_ai_launch |</p> <p>| left_stick_horz | set_steering |</p> <p>| right_stick_vert | set_throttle |</p> <p>+------------------+---------------------------+</p> <p>If your joystick is not returning to neutral</p> <p>You can add a deadzone value</p> <p>on myconfig.py</p> <p>ex:</p> <p>NETWORK_JS_SERVER_IP = \"192.168.0.1\"when listening for network joystick cont\\$</p> <p>JOYSTICK_DEADZONE = 0.01 when non zero, this is the smallest throt\\$</p> <p>JOYSTICK_THROTTLE_DIR = -1.0 use -1.0 to flip forward/backward, use \\$</p> <p>Lets Integrate the JTN and PWM Controller into the RC Chassis</p> <p>Charge your LiPo Battery</p> <p>After you charge your Lithium Polymer (LiPo) battery(ries) - some info here</p> <p>Connect the battery(ries) and batteries monitor/alarm</p> <p>Please do not use the batteries without the batteries monitor/alarms</p> <p>If you discharge a LiPO batteries below a threshold lets say 3.0</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_14","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#calibration-of-the-throttle-and-steering","title":"Calibration of the Throttle and Steering","text":"<p>Before you develop code or you can use someone's code to control a robot, we need to calibrate the actuator/mechanism to find out its range of motion compared to the control / command a computer will send to the controller of the actuator/mechanism.</p> <p>The calibration is car specific. If you use the same platform, the numbers should be really close. Otherwise, you will need to calibrate your car.</p> <p>MAKE SURE THE ROBOT IS ON THE RC CAR STAND</p> <p>MAKE SURE THE ROBOT IS ON THE RC CAR STAND</p> <p>MAKE SURE THE ROBOT IS ON THE RC CAR STAND</p> <p>MAKE SURE THE ROBOT IS ON THE RC CAR STAND</p> <p>MAKE SURE THE ROBOT IS ON THE RC CAR STAND</p> <p>Follow the safety guidelines provided in person in the class.</p> <p>If something does not sound right, don\u2019t do it. Safety first.</p> <p>Power the JTN</p> <p>Power the Electronic Speed Controller (ESC)</p> <p>Lets run a Python command to calibrate Steering and Throttle</p> <p>The donkey commands need to be run from the directory you created for your car, i.e., ~/projects/d4</p> <p>If you have not done so, SSH into the JTN</p> <p>If needed, change directory to d4</p> <p>cd ~/projects/d4</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_15","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#begin-calibration","title":"Begin Calibration","text":"<p>donkey calibrate --channel 1 --bus 1</p> <p>Please make sure that when you are trying the steering values you do not keep the steering</p> <p>servo max out.</p> <p>Please reduce go back 10 or 15 values when you notice the servo motor is not moving or making a</p> <p>constant noise. If you leave the servo motor max-out you will most likely burn it.</p> <p>If by any chance your software locks-up, please turn off the ESC immediately.</p> <p>Then once you run the calibrate again, issue the center value before turning in the ESC again</p> <p>Note that after 10 values or so the calibration may time out. Just run it again.</p> <p>Try some values around 390 to center the steering</p> <p>Enter a PWM setting to test(100-600)370</p> <p>Enter a PWM setting to test(100-600)380</p> <p>Enter a PWM setting to test(100-600)390</p> <p>Enter a PWM setting to test(100-600)400</p> <p>In my case, 390 seems to center the steering</p> <p>Take note of the left max and right max value. We will be able to adjust these after we try driving the</p> <p>car so it goes straight.</p> <p>ex:</p> <p>390 - Center</p> <p>290 - Steering left max</p> <p>490 - Steering right max</p> <p>Note: If the donkey calibration times-out, just run it again.</p> <p>You can end the calibration by typing CTRL-C</p> <p>MAKE SURE THE ROBOT IS ON THE RC CAR STAND</p> <p>MAKE SURE THE ROBOT IS ON THE RC CAR STAND</p> <p>MAKE SURE THE ROBOT IS ON THE RC CAR STAND</p> <p>MAKE SURE THE ROBOT IS ON THE RC CAR STAND</p> <p>MAKE SURE THE ROBOT IS ON THE RC CAR STAND</p> <p>Now to calibrate the Throttle Electronic Speed Controller ESC (THROTTLE)</p> <p>Following the standard for R/C cars. Throttle goes on channel 2</p> <p>donkey calibrate --channel 2 --bus 1</p> <p>Enter a PWM setting to test(100-600)370</p> <p>Enter a PWM setting to test(100-600)380 (neutral)</p> <p>Enter a PWM setting to test(100-600)390</p> <p>On my case, 380 when I power up the ESC seems to be the middle point (neutral),</p> <p>I will use 380. Your case may vaires. 370 seems a common value</p> <p>Neutral when power the ESC - 380</p> <p>Make sure the car is balanced over the car stand</p> <p>Then go in increments of 10 until you can no longer hear an increase on the speed</p> <p>of the car. Don\u2019t worry much about the max speed since we won\u2019t drive that fast autonomously</p> <p>and during training the speed will be limited.</p> <p>Max speed forward - 490</p> <p>Reverse on RC cars is a little tricky because the ESC needs to receive</p> <p>a reverse pulse, zero pulse, and again reverse pulse to start to go backwards.</p> <p>Use the same technique as above set the PWM setting to your zero throttle</p> <p>(lets say 380 or 370).</p> <p>Enter the reverse value, then the zero throttle (e.g., 370) value, then a reverse value again.</p> <p>Enter values +/- 10 of the reverse value to find a reasonable reverse speed. Remember this reverse PWM value.</p> <p>(dk)pi@jackrpi02:~/projects/d3 \\$</p> <p>donkey calibrate --channel 2 --bus 1</p> <p>Enter a PWM setting to test(100-600)360</p> <p>Enter a PWM setting to test(100-600)370</p> <p>Enter a PWM setting to test(100-600)360</p> <p>Enter a PWM setting to test(100-600)350</p> <p>Enter a PWM setting to test(100-600)340</p> <p>Enter a PWM setting to test(100-600)330</p> <p>Enter a PWM setting to test(100-600)320</p> <p>I got for Throttle</p> <p>490 - Max speed forward</p> <p>380 - Neutral</p> <p>300 - Max speed backwards</p> <p>For my robocar I have:</p> <p>Steering</p> <p>290 - Steering left max</p> <p>490 - Steering right max</p> <p>Throttle</p> <p>490 - Max speed forward</p> <p>380 - Neutral</p> <p>300 - Max speed backwards</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#saving-car-configuration-file","title":"Saving car configuration file","text":"<p>Now let's write these values into the car configuration file</p> <p>Edit the file my config.py</p> <p>nano myconfig.py</p> <p>Change these values according to YOUR calibration values and where you have the Steering Servo and ESC connected</p> <p>...</p> <p>STEERING</p> <p>STEERING_CHANNEL = 1 channel on the 9685 pwm board 0-15</p> <p>STEERING_LEFT_PWM = 290 pwm value for full left steering</p> <p>STEERING_RIGHT_PWM = 490 pwm value for full right steering</p> <p>THROTTLE</p> <p>THROTTLE_CHANNEL = 2 channel on the 9685 pwm board 0-15</p> <p>THROTTLE_FORWARD_PWM = 490 pwm value for max forward throttle</p> <p>THROTTLE_STOPPED_PWM = 380 pwm value for no movement</p> <p>THROTTLE_REVERSE_PWM = 300 pwm value for max reverse throttle</p> <p>Also, change these</p> <p>CAMERA</p> <p>CAMERA_TYPE = \"WEBCAM\" (PICAM|WEBCAM|CVCAM|CSIC|V4L|MOCK)</p> <p>9865, overrides only if needed, ie. TX2..</p> <p>PCA9685_I2C_ADDR = 0x40 I2C address, use i2cdetect to validate this numb\\$</p> <p>PCA9685_I2C_BUSNUM = 1 None will auto detect, which is fine on the pi. \\$</p> <p>JOYSTICK</p> <p>USE_JOYSTICK_AS_DEFAULT = True when starting the manage.py, when True, wil\\$</p> <p>and if needed to zero the joystick</p> <p>JOYSTICK_DEADZONE = 0.01 when non zero, this is the smallest thro\\$</p> <p>Note: When driving the robot, if you robot has too much power or not enough power</p> <p>you can adjust the max_throttle</p> <p>JOYSTICK_MAX_THROTTLE = 0.5</p> <p>This also will be your starting power setting when using the constant throttle autopilot.</p> <p>You can test driving your robot by issuing</p> <p>python manage.py drive</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#if-you-are-using-the-vesc-and-oakd-camera-on-the-physical-car","title":"If you are using the VESC and OAKD camera on the physical car","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#vesc_1","title":"VESC","text":"<p>Ensure that you have already configured the VESC device using the VESC Tool Software</p> <p>Edit the myconfig.py to have these values</p> <p>DRIVE_TRAIN_TYPE = \"VESC\"</p> <p>VESC_MAX_SPEED_PERCENT =.2 ## Max speed as a percent of actual max speed</p> <p>VESC_SERIAL_PORT= \"/dev/ttyACM0\" ## check this val with ls /dev/tty*</p> <p>VESC_HAS_SENSOR= True</p> <p>VESC_START_HEARTBEAT= True</p> <p>VESC_BAUDRATE= 115200</p> <p>VESC_TIMEOUT= 0.05</p> <p>VESC_STEERING_SCALE = .5</p> <p>VESC_STEERING_OFFSET = .5</p> <p>DONKEY_GYM = False</p> <p>(we will leave the CAMERA_TYPE = \u201cMOCK\u201d for now to make sure we can drive the car with the VESC)</p> <p>Download the following files</p> <p>~~https://drive.google.com/drive/folders/1SBzChXK2ebzPHgZBP_AIhVXJOekVc0r3?usp=sharing~~</p> <p>https://drive.google.com/drive/folders/19TS3VyNXQPBSr41yiPaxpI1lnxClI2c8?usp=sharing</p> <p>And replace them on the Jetson in the locations shown in the images below. Note - to get the files on the jetson you can use SFTP (secure file transfer protocol):</p> <p>Examples on how to use SFTP: sftp jetson@ucsdrobocar-148-xx.local</p> <p>cd To the directory you want to go to</p> <p>put /File/Path/On/Your/Computer</p> <p>alternatively</p> <p>get filename /File/Location/You/Want/Them/Go/On/Your/Computer</p> <p>To get a directory</p> <p>get -rf filename /File/Location/You/Want/Them/Go/On/Your/Computer</p> <p>type \"exit\" to disconnect</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#img-src10imagesfinalmediaimage18png","title":"&lt;img src=\"./10imagesfinal/media/image18.png\"","text":"<p>style=\"width:4.35161in;height:3.27604in\" /&gt;</p> <p>Once these have been replaced, you should run</p> <p>python manage.py drive</p> <p>It should first throw a pyvesc import error. Follow the description in the terminal to install the needed libraries</p> <p>Then run it again. It should throw a permissions error. Follow the advice on how to fix the error with chmod</p> <p>Int(10) error bug fix (Credit Saimai Lau and Albert Chang): When running python manage.py drive, the intermittent \"invalid literal for int() with base 10: 'None' error is from the VESC package checking whether the version of the VESC is below 3.0, so we can comment out that part since we're using 6.0 just do</p> <p>nano /home/jetson/projects/envs/donkey/lib/python3.6/site-packages/pyvesc/VESC/VESC.py</p> <p>and put # at the beginning of lines 38-40 Then ^S to save and ^X to exit</p> <p></p> <p>Error explanation: The self.get_firmware_version() get thes version by requesting it from the VESC and reading the replied bytes, but sometimes the data is lost or incomplete so the function returns 'None' as the version. We already know the version is 6.0 so we don\u2019t need this function.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#oakd","title":"OAKD","text":"<p>Once you have the VESC driving the car, you will need to make sure you have set up the document by following the instructions labeled Configuring OAKD Lite found earlier in this document</p> <p>Edit the myconfig.py camera type to OAKD</p> <p>CAMERA_TYPE = \"OAKD\"</p> <p>Make sure the camera is working by checking the images that are being saved to the /data/images directory.</p> <p>The easiest way to do this is to go to</p> <p>http://localhost:8887 while running donkeysim, and you should be able to see a livestream from the camera. Note - if several people are running donkeysim at the same time on the same wifi this interface may get buggy</p> <p>You can also do this by either:</p> <p>transferring the files to you laptop or virtual machine</p> <p>with scp, rsync, winscp (windows) or filezilla (mac)</p> <p>Or</p> <p>Using NoMachine by following the instructions found here in this document</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#gnss-configuration","title":"GNSS Configuration:","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#how-to-plug-pointonenav-to-donkeycar","title":"How to Plug PointOneNav to Donkeycar","text":"<ol> <li> <p>Make sure your user is added to the dialout group. If not</p> <ol> <li> <p>sudo adduser jetson dialout</p> </li> <li> <p>sudo reboot now</p> </li> </ol> </li> <li> <p>Download     https://drive.google.com/file/d/1BK_UjH-He9d_D4eObWMHzpHHCqmtq75h/view?usp=share_link     (Note that this zip file cannot be shared outside of the class. It     is still proprietary as of now)</p> </li> <li> <p>Unzip.</p> </li> <li> <p>Run</p> <ol> <li> <p>cd quectel-lg69t-am.0.15.0/p1_runner</p> </li> <li> <p>deactivate (This should get you out of the current environment)</p> </li> <li> <p>wget     https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-pypy3-Linux-aarch64.sh     .</p> </li> <li> <p>bash Mambaforge-pypy3-Linux-aarch64.sh</p> </li> <li> <p>Reboot the jetson</p> </li> <li> <p>mamba create --name py37 -c conda-forge python=3.7 pip</p> </li> <li> <p>mamba activate py37</p> </li> <li> <p>pip3 install -e .</p> <ol> <li> <p>%If this fails, you can try just going to the p1_runner     directory and running the python3 bin/config_tool.py     command, and then doing \u201cpip install ____\u201d for all the     missing things, you may just need</p> <ol> <li> <p>pip install pyserial</p> </li> <li> <p>pip install fusion_engine_client</p> </li> <li> <p>pip install pynmea</p> </li> <li> <p>pip install ntripstreams</p> </li> <li> <p>pip install websockets</p> </li> </ol> </li> </ol> </li> <li> <p>python3 bin/config_tool.py reset factory</p> </li> <li> <p>python3 bin/config_tool.py apply uart2_message_rate nmea gga on</p> </li> <li> <p>python3 bin/config_tool.py save</p> </li> <li> <p>python3 bin/runner.py --device-id \\&lt;polaris_username&gt; --polaris     \\&lt;polaris_password&gt; --device-port /dev/ttyUSB1</p> </li> </ol> </li> </ol> <p>(if not getting any data including Nans try USB0)</p> <p>Note: The GPS corrections will only happen when you are actively running runner.py. I recommend making a bashrc command that you can run to start up the runner.py program easily in a 2nd terminal while using the GPS for anything.</p> <ol> <li> <p>Create a project with the DonkeyCar path follow template</p> <ol> <li> <p>Open a new terminal window</p> </li> <li> <p>Make sure that the donkey car environment is running</p> <ol> <li>source ~/projects/envs/donkey/bin/activate</li> </ol> </li> <li> <p>cd ~/projects</p> </li> <li> <p>donkey createcar --path ./mycar --template path_follow</p> </li> </ol> </li> <li> <p>Set the following in the myconfig.py</p> <ol> <li> <p>GPS_SERIAL = \u201c/dev/ttyUSB2\u201d (USB1 if USB0 used above)</p> </li> <li> <p>GPS_SERIAL_BAUDRATE = 460800</p> </li> <li> <p>GPS_DEBUG = True</p> </li> <li> <p>HAVE_GPS = True</p> </li> <li> <p>GPS_NMEA_PATH = None</p> </li> </ol> </li> <li> <p>Also set things like the VESC parameters in myconfig.py. You can     copy these over from the donkeycar you created earlier.</p> </li> <li> <p>Run</p> <ol> <li>python3 manage.py drive</li> </ol> </li> <li> <p>You should see GPS positions being outputted after you run     Donkeycar. If you don\u2019t want to output set GPS_DEBUG to False</p> </li> <li> <p>Configure button actions</p> <ol> <li> <p>SAVE_PATH_BTN is the button to save the in-memory path to a     file.</p> </li> <li> <p>LOAD_PATH_BTN is the button to (re)load path from the csv file     into memory.</p> </li> <li> <p>RESET_ORIGIN_BTN is the button to set the current position as     the origin.</p> </li> <li> <p>ERASE_PATH_BTN is the button to erase path from memory and reset     the origin.</p> </li> <li> <p>TOGGLE_RECORDING_BTN is the button to toggle recording mode on     or off. Note that there is a pre-assigned button in the web ui,     so there is not need to assign this button to one of the web/w*     buttons if you are using the web ui.</p> </li> <li> <p>INC_PID_D_BTN is the button to change PID 'D' constant by     PID_D_DELTA.</p> </li> <li> <p>DEC_PID_D_BTN is the button to change PID 'D' constant by     -PID_D_DELTA</p> </li> <li> <p>INC_PID_P_BTN is the button to change PID 'P' constant by     PID_P_DELTA</p> </li> <li> <p>DEC_PID_P_BTN is the button to change PID 'P' constant by     -PID_P_DELTA</p> </li> </ol> </li> </ol> <p>The logitech buttons are named stuff like \u201cX\u201d or \u201cR1\u201d See the example config below. SAVE_PATH_BTN = \"R1\" # button to save path</p> <p>LOAD_PATH_BTN = \"X\" # button (re)load path</p> <p>RESET_ORIGIN_BTN = \"B\" # button to press to move car back to origin</p> <p>ERASE_PATH_BTN = \"Y\" # button to erase path</p> <p>TOGGLE_RECORDING_BTN = \"L1\" # button to toggle recording mode</p> <p>INC_PID_D_BTN = None # button to change PID 'D' constant by PID_D_DELTA</p> <p>DEC_PID_D_BTN = None # button to change PID 'D' constant by -PID_D_DELTA</p> <p>INC_PID_P_BTN = \"None\" # button to change PID 'P' constant by PID_P_DELTA</p> <p>DEC_PID_P_BTN = \"None\" # button to change PID 'P' constant by -PID_P_DELTA</p> <p>#</p> <ol> <li> <p>Recording a path</p> <ol> <li> <p>The algorithm assumes we will be driving in a continuous     connected path such that the start and end are the same. You can     adjust the space between recorded waypoints by editing the     PATH_MIN_DIST value in myconfig.py You can change the name and     location of the saved file by editing the PATH_FILENAME value.</p> </li> <li> <p>Enter User driving mode using either the web controller or a     game controller.</p> </li> <li> <p>Move the car to the desired starting point</p> </li> <li> <p>Erase the path in memory (which will also reset the origin).</p> <ol> <li>Make sure to reset the origin!!! If you didn\u2019t need to erase     the path in memory you can just</li> </ol> </li> <li> <p>Toggle recording on.</p> </li> <li> <p>Drive the car manually around the track until you reach the     desired starting point again.</p> </li> <li> <p>Toggle recording off.</p> </li> <li> <p>If desired, save the path.</p> </li> </ol> </li> <li> <p>Following a path</p> <ol> <li> <p>Enter User driving mode using either the web controller or a     game controller.</p> </li> <li> <p>Move the car to the desired starting point - make sure it\u2019s the     same one from when you recorded the path</p> </li> <li> <p>Reset the origin (be careful; don't erase the path, just reset     the origin).</p> </li> <li> <p>Load the path</p> </li> <li> <p>Enter Autosteering or Autopilot driving mode. This is normally     done by pressing the start button either once or twice If you     are in Autosteering mode you will need to manually provide     throttle for the car to move. If you are in Autopilot mode the     car should drive itself completely.</p> </li> </ol> </li> <li> <p>Configuring Path Follow Parameters</p> <ol> <li> <p>So the algorithm uses the cross-track error between a desired     line and the vehicle's measured position to decide how much and     which way to steer. But the path we recorded is not a simple     line; it is a lot of points that is typically some kind of     circuit. As described above, we use the vehicle's current     position to choose a short segment of the path that we use as     our desired track. That short segment is recalculated every time     we get a new measured car position. There are a few     configuration parameters that determine exactly which two points     on the path that we use to calculate the desired track line.</p> <ol> <li> <p>PATH_SEARCH_LENGTH = None # number of points to search for     closest point, None to search entire path</p> </li> <li> <p>PATH_LOOK_AHEAD = 1 # number of points ahead of the closest     point to include in cte track</p> </li> <li> <p>PATH_LOOK_BEHIND = 1 # number of points behind the closest     point to include in cte track</p> </li> </ol> </li> <li> <p>Generally, if you are driving very fast you might want the look     ahead to be larger than if driving slowly so that your steering     can anticipate upcoming curves. Increasing the length of the     resulting track line, by increasing the look behind and/or look     ahead, also acts as a noise filter; it smooths out the track.     This reduces the amount of jitter in the controller. However,     this must be balanced with the true curves in the path; longer     track segments effectively 'flatten' curves and so can result in     understeer; not steering enough when on a curve.</p> </li> </ol> </li> <li> <p>Determining PID Coefficients</p> <ol> <li> <p>The PID coefficients are the most important (and time consuming)     parameters to configure. If they are not correct for your car     then it will not follow the path. The coefficients can be     changed by editing their values in the myconfig.py file.</p> </li> <li> <p>PID_P is the proportional coefficient; it is multiplied with the     cross-track error. This is the most important parameter; it     contributes the most to the output steering value and in some     cases may be all that is needed to follow the line. If this is     too small then car will not turn enough when it reaches a curve.     If this to too large then it will over-react to small changes in     the path and may start turning in circles; especially when it     gets to a curve.</p> </li> <li> <p>PID_D is the differential coefficient; it is multiplied with the     change in the cross-track error. This parameter can be useful in     reducing oscillations and overshoot.</p> </li> <li> <p>PID_I is the integral coefficient; it is multiplied with the     total accumulated cross-track error. This may be useful in     reducing offsets caused by accumulated error; such as if one     wheel is slightly smaller in diameter than another.</p> </li> <li> <p>Determining PID Coefficients can be difficult. One approach is:</p> <ol> <li> <p>First determine the P coefficient.</p> </li> <li> <p>zero out the D and the I coefficients.</p> </li> <li> <p>Use a kind of 'binary' search to find a value where the     vehicle will roughly follow a recorded straight line;     probably oscillating around it. It will be weaving</p> </li> <li> <p>Next find a D coefficient that reduces the weaving     (oscillations) on a straight line. Then record a path with a     tight turn. Find a D coefficient that reduces the overshoot     when turning.</p> </li> <li> <p>You may not even need the I value. If the car becomes     unstable after driving for a while then you may want to     start to set this value. It will likely be much smaller than     the other values.</p> </li> <li> <p>Be patient. Start with a reasonably slow speed. Change one     thing at a time and test the change; don't make many changes     at once. Write down what is working.</p> </li> <li> <p>Once you have a stable PID controller, then you can figure     out just how fast you can go with it before autopilot     becomes unstable. If you want to go faster then set the     desired speed and start tweaking the values again using the     method suggested above.</p> </li> </ol> </li> </ol> </li> </ol>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_16","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_17","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_18","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_19","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_20","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_21","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_22","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_23","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#driving-the-robot-to-collect-data","title":"Driving the Robot to Collect data","text":"<p>Remember you are driving at max of x (0.x) Throttle power based on the</p> <p>myconfig.py that you edited.</p> <p>The robot is not controlling speed but power given the motor using PWM values</p> <p>Transmitted from the Single Board Computer (SBC) like a Jetson Nano (JTN) to the</p> <p>Electronic Speed Controller (ESC).</p> <p>To reverse you may have to reverse, stop, reverse. This is a feature of some</p> <p>ESCs used in RC cars to prevent damaging gears when changing from forward to reverse.</p> <p>On the JTN</p> <p>If you are not changing directory automatically when the user logs in</p> <p>cd ~/projects/d3</p> <p>(env) jetson@ucsdrobocar00:~/projects/d3 \\$</p> <p>python manage.py drive</p> <p>Note: CTRL-C stop the manage.py drive</p> <p>If you get an error on the joystick or Donkey stops loading the JoyStick</p> <p>it is because game controller is off</p> <p>or not connected/paired with the JTN</p> <p>ls</p> <p>config.py data logs manage.py models</p> <p>ls data</p> <p>tub_1_17-10-13</p> <p>If you want to wipe clean the data collected</p> <p>Remove the content of the ~/projects/d3/data directory. It should be tub_\u2026\u2026</p> <p>You can delete the entire directory then create it again.</p> <p>~/projects/d3 \\$</p> <p>rm -rf data</p> <p>mkdir data</p> <p>Follow the Donkey Docs to install the Donkey AI framework into your PC http://docs.donkeycar.com</p> <p>On the PC</p> <p>Activate the virtual environment. I am assuming you have your virtual environment under</p> <p>~/projects/envs/donkey</p> <p>source ~/projects/envs/donkey/bin/activate</p> <p>cd projects</p> <p>cd d3</p> <p>SSH into the JTN</p> <p>ssh jetson@ucsdrobocar00.local</p> <p>On the JTN</p> <p>If you are not changing directory automatically when the user logs in</p> <p>cd ~/projects/d3</p> <p>(env) jetson@ucsdrobocar00:~/projects/d3 \\$</p> <p>python manage.py drive</p> <p>Note: CTRL-C stop the manage.py drive</p> <p>Drive the robot to collect data</p> <p>On the PC</p> <p>Get data from JTN</p> <p>Transfer all data from the JTN to the PC and delete data that was deleted from the JTN</p> <p>rsync -a --progress --delete jetson@ucsdrobocar00.local:~/d3/data ~/projects/d3</p> <p>ls data</p> <p>tub_1_17-10-12</p> <p>Train model on all data (Tubes)</p> <p>python train.py --model=models/date_name.h5</p> <p>To train using a particular tube</p> <p>python train.py --tub ~/projects/d3/data/tub_1_18-01-07 --model=models/model_name.h5</p> <p>To make an incremental training using a previous model</p> <p>python train.py --tub ~/projects/d3/data/NAME_OF_NEW_TUBE --transfer=models/NAME_OF_PREVIOUS_MODEL.h5 --model=models/NAME_OF_NEW_MODEL.h5</p> <p>On your personal PC (Not required, only if you installed on your personal computer)</p> <p>clean-up tubs removing possible bad data</p> <p>~/projects/d3</p> <p>donkey tubclean data</p> <p>using donkey ...</p> <p>Listening on 8886...</p> <p>Open a browser and type</p> <p>http://localhost:8886</p> <p></p> <p>You can clean-up your tub directories. Please make a backup of your data before you start</p> <p>to clean it up.</p> <p>On the mac if the training complains</p> <p>rm ~/projects/d2t/data/.DS_Store</p> <p>If it complains about docopt, install it again. And I did not change anything</p> <p>from the previous day. Go figure\u2026</p> <p>(env) jack@lnxmbp01:~/projects/d2\\$ pip list</p> <p>(env) jack@lnxmbp01:~/projects/d2\\$ pip install docopt</p> <p>See the models here</p> <p>~/projects/d3</p> <p>ls models</p> <p>ucsd_12oct17.h5</p> <p>Place Autopilot into RPI</p> <p>rsync -a --progress ~/projects/d3/models/ jetson@ucsdrobocar00:~/projects/d3/models/</p> <p>At JTN</p> <p>ls models</p> <p>Ucsd_12oct17.h5</p> <p>On the JTN</p> <p>Run AutoPilot at the RPI</p> <p>python manage.py drive --model=./models/ucsd_12oct17.h5</p> <p>...</p> <p>Using TensorFlow backend.</p> <p>loading config file: /home/pi/d2/config.py</p> <p>config loaded</p> <p>PiCamera loaded.. .warming camera</p> <p>Starting vehicle...</p> <p>/home/pi/env/lib/python3.4/site-packages/picamera/encoders.py:544: PiCameraResolutionRounded: frame size rounded up from 160x120 to 160x128</p> <p>width, height, fwidth, fheight)))</p> <p>END of DonkeyCar AI Framework Instructions</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#remote-desktop-installation","title":"Remote Desktop Installation","text":"<p>Remote Access to the SBC Graphical User Interface (GUI)</p> <p>Lets install a remote desktop server on the SBC and a client on your computer</p> <p>We will be using NoMachine</p> <p>Here is a link to the instructions to install NOMACHINE on a Single Board Computers base</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#backup-of-the-usd-card","title":"Backup of the uSD Card","text":"<p>Once you finished the configuration of your SBC, why not making another backup of the uSD card</p> <p>(or first one if you have not done it yet)</p> <p>It can save you lots of time during a recovery process. In case of a crash, you will only need to</p> <p>restore the image vs. install all over again.</p> <p>Backup the uSD card following these steps in a Linux machine</p> <p>Eject the uSD card from your SBC, plug it into a linux PC using a uSD adapter</p> <p>sudo fdisk -l</p> <p>Disk /dev/sda: 59.6 GiB, 64021856256 bytes, 125042688 sectors</p> <p>Units: sectors of 1 * 512 = 512 bytes</p> <p>Sector size (logical/physical): 512 bytes / 512 bytes</p> <p>I/O size (minimum/optimal): 512 bytes / 512 bytes</p> <p>Disklabel type: gpt</p> <p>Disk identifier: D048AD43-24FD-4DED-B06E-7BB8ED98158C</p> <p>Device Start End Sectors Size Type</p> <p>/dev/sda1 24576 125042654 125018079 59.6G Linux filesystem</p> <p>/dev/sda2 2048 2303 256 128K Linux filesystem</p> <p>/dev/sda3 4096 4991 896 448K Linux filesystem</p> <p>/dev/sda4 6144 7295 1152 576K Linux filesystem</p> <p>/dev/sda5 8192 8319 128 64K Linux filesystem</p> <p>/dev/sda6 10240 10623 384 192K Linux filesystem</p> <p>/dev/sda7 12288 13439 1152 576K Linux filesystem</p> <p>/dev/sda8 14336 14463 128 64K Linux filesystem</p> <p>/dev/sda9 16384 17663 1280 640K Linux filesystem</p> <p>/dev/sda10 18432 19327 896 448K Linux filesystem</p> <p>/dev/sda11 20480 20735 256 128K Linux filesystem</p> <p>/dev/sda12 22528 22687 160 80K Linux filesystem</p> <p>On my case, the 64G uSD was mounted on /dev/sda</p> <p>example on the command line for making an image of the uSD card mounted</p> <p>on a Linux machines as /dev/sda</p> <p>sudo dd bs=4M if=/dev/sda of=ucsd_robocar_image_25sep19.img status=progress</p> <p>Lets compress the image using Zip</p> <p>zip ucsd_robocar_image_25sep19.zip ucsd_robocar_image_25sep19.img</p> <p>Example using MacOS</p> <p>diskutil list</p> <p>/dev/disk6 (external, physical):</p> <p>: TYPE NAME SIZE IDENTIFIER</p> <p>0: GUID_partition_scheme *128.0 GB disk6</p> <p>1: Linux Filesystem 127.6 GB disk6s1</p> <p>2: Linux Filesystem 67.1 MB disk6s2</p> <p>3: Linux Filesystem 67.1 MB disk6s3</p> <p>4: Linux Filesystem 458.8 KB disk6s4</p> <p>5: Linux Filesystem 458.8 KB disk6s5</p> <p>6: Linux Filesystem 66.1 MB disk6s6</p> <p>7: Linux Filesystem 524.3 KB disk6s7</p> <p>8: Linux Filesystem 262.1 KB disk6s8</p> <p>9: Linux Filesystem 262.1 KB disk6s9</p> <p>10: Linux Filesystem 104.9 MB disk6s10</p> <p>11: Linux Filesystem 134.2 MB disk6s11</p> <p>sudo dd if=/dev/disk6 of=ucsdrobocar-xxx-yy-v1.0.img</p> <p>Alternatively you can use MS Windows, use a software called Win32. Search the web</p> <p>for instructions on using Win32</p> <p>\u201c</p> <p>Using Windows</p> <p>Once you open Win32 Disk Imager, use the blue folder icon to choose the location and the name of the backup you want to take, and then choose the drive letter for your SD card. Click on the Read button. The card will then be backed up to your PC.Mar 18, 2015</p> <p>Backing up and Restoring your Raspberry Pi's SD Card\u2013 The \u2026</p> <p>\u201c</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#if-needed-we-have-an-jtn-usd-card-image-ready-for-plug-and-play","title":"If needed, we have an JTN uSD Card Image Ready for Plug and Play","text":"<p>If you run out of time trying to make the compilation and build OpenCV, and install the Donkey AI</p> <p>framework, here is a link for a 64G Bytes uSDcard that is ready to go. Get recovery image here. </p> <p>The recovery image already has all the software installed.</p> <p>Connect a uUSB cable between the PC and the JTN, or connect the JTN to the access point</p> <p>using a network cable.</p> <p>Boot the JTN, wait 1~2 minutes for the software to finish loading</p> <p>You can ssh to the JTN</p> <p>ssh jetson@192.168.55.1</p> <p>or</p> <p>ssh jetson@ucsdrobocar-xxx-yy.local</p> <p>enter password</p> <p>jetsonucsd</p> <p>You need to change the host name and change the default password</p> <p>Connect the JTN to a WiFi network</p> <p>see steps earlier in this document</p> <p>ex:</p> <p>sudo nmcli device wifi connect UCSDRoboCar5GHz password UCSDrobocars2018</p> <p>shutdown the JTN</p> <p>sudo shutdown now</p> <p>If you are using the uUSB cable, remove it from the JTN</p> <p>Power on the JTN with the provided 5V power supply</p> <p>Wait 1~2 minutes for the JTN to complete the boot process</p> <p>Then use SSH to connect to the JTN</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#_24","title":"10docfinal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#ros-with-docker","title":"ROS with Docker","text":"<p>Here is the link for getting your robot set up with ROS using our docker images</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#supporting-material","title":"Supporting material","text":"<p>How to show what Linux we have installed</p> <p>dmesg | head -1</p> <p>How to show the distribution we are running</p> <p>lsb_release -a</p> <p>This session is in-process, just a placeholder for now</p> <p>Jetson Nano ROS based on NVIDIA</p> <p>https://github.com/dusty-nv/jetbot_ros</p> <p>Here is what I changed on myconfig.py</p> <p>CAMERA_TYPE = \"WEBCAM\" (PICAM|WEBCAM|CVCAM|CSIC|V4L|MOCK)</p> <p>PCA9685_I2C_BUSNUM = 1 None will auto detect, which is fine on the pi. But other platforms should specify the bus number.</p> <p>STEERING_CHANNEL = 1 channel on the 9685 pwm board 0-15</p> <p>STEERING_LEFT_PWM = 290 pwm value for full left steering</p> <p>STEERING_RIGHT_PWM = 490 pwm value for full right steering</p> <p>THROTTLE_CHANNEL = 2 channel on the 9685 pwm board 0-15</p> <p>THROTTLE_FORWARD_PWM = 490 pwm value for max forward throttle</p> <p>THROTTLE_STOPPED_PWM = 380 pwm value for no movement</p> <p>THROTTLE_REVERSE_PWM = 300 pwm value for max reverse throttle</p> <p>USE_JOYSTICK_AS_DEFAULT = True when starting the manage.py, when True, will not require a --js option to use the joystick</p> <p>CONTROLLER_TYPE='F710' (ps3|ps4|xbox|nimbus|wiiu|F710)</p> <p>JOYSTICK_DEADZONE = 0.01 when non zero, this is the smallest throttle before recording triggered.</p> <p>--</p> <p>--</p> <p>To use TensorflowRT on the JetsonNano</p> <p>http://docs.donkeycar.com/guide/robot_sbc/tensorrt_jetson_nano/</p> <p>Updating the Donkey AI framework and or using the master release fork</p> <p>\u201c</p> <p>Tawn 2:47 PM</p> <p>3.1.0 released now. TensorRT support on Nano! TFlite support (w TF 2.0+). Better support for cropping across more tools. Please update like:</p> <p>cd projects/donkeycar</p> <p>git checkout master</p> <p>git pull</p> <p>pip install -e .[pi] or .[nano] or .[pc] depending where you are installing it</p> <p>cd ~/mycar</p> <p>donkey update</p> <p>Important Note: Your old models will not work with the new code. We've changed how cropping and normalization work to support TensorRT/TFLite. So please, RE-TRAIN your models after updating to  </p> <p>docs to help get you started w TensorRT: https://docs.donkeycar.com/guide/robot_sbc/tensorrt_jetson_nano/</p> <p>Installing TensorRT on Ubuntu18.04</p> <p>After installing Tensorflow on your virtual environment</p> <p>ex: conda install tensorflow-gpu==1.13.1</p> <p>Testing the Tensorflow Install</p> <p>python</p> <p>Enter the following txt, you can cut and paste</p> <p>Python</p> <p>import tensorflow as tf</p> <p>hello = tf.constant('Hello, TensorFlow!')</p> <p>sess = tf.Session()</p> <p>print(sess.run(hello))</p> <p>https://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.htmlinstalling-tar</p> <p>https://developer.nvidia.com/nvidia-tensorrt-5x-download</p> <p></p> <p></p> <p>Open a terminal window and navigate to the directory where you saved the file</p> <p>Now let\u2019s expand the file</p> <p></p> <p>Note that the file above expects to work with cudnn7.5</p> <p>The Cuda install will need to match that version, cudnn7.5</p> <p>e.g. ~/projects/TensorRT-5.1.5.0/</p> <p>cd ~/projects/TensorRT-5.1.5.0</p> <p>tar xzvf TensorRT-5.1.5.0.Ubuntu-18.04.2.x86_64-gnu.cuda-10.1.cudnn7.5.tar.gz</p> <p>export LD_LIBRARY_PATH=\\$LD_LIBRARY_PATH:~/projects/TensorRT-5.1.5.0/lib</p> <p>Activate the virtual environment you want TensorflowRT installed</p> <p>e.g. source ~/projects/env/bin/activate</p> <p>cd python</p> <p>If using python 3.6</p> <p>pip install tensorrt-5.1.5.0-cp36-none-linux_x86_64.whl</p> <p>or if using python3.7</p> <p>pip install tensorrt-5.1.5.0-cp37-none-linux_x86_64.whl</p> <p>cd ..</p> <p>cd uff</p> <p>pip install uff-0.6.3-py2.py3-none-any.whl</p> <p>which convert-to-uff</p> <p>/home/jack/projects/env/bin/convert-to-uff</p> <p>or</p> <p>which convert-to-uff</p> <p>/home/jack/miniconda3/envs/donkey/bin/convert-to-uff</p> <p>cd ..</p> <p>cd graphsurgeon</p> <p>pip install graphsurgeon-0.4.1-py2.py3-none-any.whl</p> <p>cd..</p> <p>apt-get install tree</p> <p>tree -d</p> <p>dpkg -l | grep nvinfer</p> <p>Testing TensorRT</p> <p>python</p> <p>import tensorrt as trt</p> <p>exit()</p> <p>Example after training a model</p> <p>You end up with a Linear.h5 in the models folder</p> <p>python manage.py train --model=./models/Linear.h5 --tub=./data/tub_1_19-06-29,...</p> <p>Freeze model using freeze_model.py in donkeycar/scripts</p> <p>The frozen model is stored as protocol buffers.</p> <p>This command also exports some metadata about the model which is saved in ./models/Linear.metadata</p> <p>python freeze_model.py --model=./models/Linear.h5 --output=./models/Linear.pb</p> <p>Convert the frozen model to UFF. The command below creates a file ./models/Linear.uff</p> <p>convert-to-uff ./models/Linear.pb</p> <p>cd ~/projects/d3_trax_rally</p> <p>python ~/projects/donkeycar/scripts/freeze_model.py --model=./models/24aug19_ucsd_booker_linear_1_320_240.h5 --output=./models/24aug19_ucsd_booker_linear_1_320_240.pb</p> <p>convert-to-uff ./models/24aug19_ucsd_booker_linear_1_320_240.pb</p> <p>For the RPI</p> <p>If you would like to try tflite support, you will need a newer version of Tensorflow on the pi. You can download and install this version:</p> <p>wget https://tawn-train.s3.amazonaws.com/tf/tensorflow-2.0.0a0-cp35-cp35m-linux_armv7l.whl</p> <p>pip install tensorflow-2.0.0a0-cp35-cp35m-linux_armv7l.whl</p> <p>Your host PC can stay at TF 1.13.1</p> <p>to train a TensorRT model, during training add the arg</p> <p>--type=tensorrt_linear</p> <p>And for TFlite support add the arg</p> <p>--type=tflite_linear</p> <p>Also use this flag when running your model on the car. (edited)</p> <p>\u201c</p> <p>PS3 Controller Modes</p> <p>This is similar for the Logitech wireless controllers</p> <p>The default mode will be that User is in Control. That is, the user controls Steering and Throttle.</p> <p>To switch to Local Angle (software controls the Steering and uses the Throttle), you need to press the \\&lt;Select&gt; button in the Joystick.</p> <p>If you give Throttle the Robocar should drive around semi-autonomously.</p> <p>After few laps that you see that your model is good,</p> <p>Please hold your robot with the wheels out of the floor</p> <p>you can press the \\&lt;Start&gt; button and immediately press the \\&lt;left_DOWN_arrow&gt; button a few times to decrease the Throttle as needed. This is important so you slow down the Robocar for a constant Throttle.</p> <p>Press the \\&lt;left_UP_arrow&gt; to give it more Throttle as needed.</p> <p>Pressing \\&lt;X&gt; will stop the robocar and go back to User mode (user is in control)</p> <p>You can change the driving modes by pressing the \\&lt;Select&gt; button. You should be able to see a message on your computer terminal that is SSH connected to the RoboCar RPI.</p> <p>The Local &amp; Angle mode (fully autonomous) is to be used after you see that you can do few laps with local angle</p> <p>Hit the Select button to toggle between three modes - User, Local Angle, and Local Throttle &amp; Angle.</p> <ul> <li> <p>User - User controls both steering and throttle with joystick</p> </li> <li> <p>Local Angle - Ai controls steering. User controls the throttle.</p> </li> <li> <p>Local Throttle &amp; Angle - Ai controls both steering and throttle</p> </li> </ul> <p>When the car is in Local Angle mode, the NN will steer. You must provide throttle...</p> <p>Ideally you will have ~ 60 laps</p> <p>If you don\u2019t have a good working Auto-Pilot, get more data in 10 laps increments.</p> <p>In summary, you may want to start with 60 laps and then do 10~20 laps more to see if the model gets better.</p> <p>I would not worry much about a few bad spots when collecting data. Drive the car back to the track,</p> <p>then press Green_Triangle to delete the last 5s of data.</p> <p>Keep driving, you will develop good skills, you will get good data and better models. If you leave the track, just drive the RoboCar back to track. It may even learn how to get back to track.</p> <p>If you keep the data from the same track (ex: UCSD Track) in the d2t/data directory, as you add more files to it (e.g., tub_5_17-10-13) it will help your model. At the same time it will take more time to train since your model will read all the data sets in the directory. You can use the transfer model to add new data to a current model.</p> <p>Incremental training using a previous model</p> <p>python train.py --tub ~/projects/d2t/data/NAME_OF_NEW_TUBE_DATA --transfer=models/NAME_OF_PREVIOUS_MODEL.h5 --model=models/NAME_OF_NEW_MODEL.h5</p> <p>Some Advanced Tools</p> <p>The visualization tool is to be used on your PC. Please even if you can, please do not use the GPU Cluster Resources for this.</p> <p>https://docs.google.com/presentation/d/1oOF9qHh6qPwF-ocOwGRzmLIRXcIcguDFwa7x9EicCo8/editslide=id.g629a9e24fa_0_1149</p> <p></p> <p>Visualizing the model driving the car vs. human driver</p> <p>Install OpenCV</p> <p>sudo apt-get install python-opencv</p> <p>pip3 install opencv-python</p> <p>donkey makemovie --tub=data\\tub_file --model=models\\model_name.h5 --limit=100 --salient --scale=2</p> <p>example</p> <p>donkey makemovie --tub=data/tub_9_19-01-19 --model=models/19jan19_oakland_5.h5 --start 1 --end 1000 --salient --scale=2</p> <p>tub_3_20-03-07</p> <p>07mar20_circuit_320x180_3.h5</p> <p>donkey makemovie --tub=data/tub_3_20-03-07 --model=models/07mar20_circuit_320x180_3.h5 --type=linear --start 1 --end 1000 --salient --scale=2</p> <p>https://devtalk.nvidia.com/default/topic/1051265/jetson-nano/remote-access-to-jetson-nano/</p> <p>If you're on the same wired local network, ssh -X works, but you should be aware that the graphics are being rendered on your local X server in such a case, so if you launch an OpenGL game for example, it'll use your local virtual graphics hardware, which means your local *CPU* in most cases. Cuda, however, will be done on the nano.</p> <p>To launch a program remotely from a linux computer:</p> <p>ssh -X some_user@test-jetson -C gedit</p> <p>ssh -X ubuntu@tegra-ubuntu nautilus</p> <p>Where `gedit` is the program you wish to run. You can omit -C get straight to a ssh prompt with X support. Any graphical applications you launch will pop up on your screen automatically.</p> <p>Please note that X does not need to be running on the Nano, so if you want to save a whole bunch of memory while working remotely you can run `sudo systemctl isolate multi-user.target` to temporarily shut down the graphical environment on the Nano itself.</p> <p>To remotely access from windows, here are instructions on how to set up an X server on windows and connect it to Putty, but please note those instructions have an old download link. A new one is here.</p> <p>Raspberry PI (RPI or PI) Configuration</p> <p>04Nov19 - Added a link to a plug and play image</p> <p>03Nov19 - Updated instruction for Tensorflow 2.0</p> <p>28Aug19 - updated the instructions to include Raspberry PI 4B</p> <p>08Aug19 - Since we are using the Nvidia Jetson Nano, I am no longer maintaining the RPI</p> <p>instructions. If you got to this point and is using Raspberry PIs, please check the</p> <p>Donkey Docs for the latest updates\u2026 http://docs.donkeycar.com</p> <p>In general if you are using a Linux distribution like Ubuntu in this course it will make</p> <p>your life is much easier. Initially, you will need access to a Linux to modify some files from</p> <p>a uSD card. You can ask a colleague, TA, or course instructor for help.</p> <p>Moreover, if you use Linux it will be another entry into your resume. Give it a try. You can</p> <p>make a dual boot in your computer or have a virtual machine, VirtualBox is free.</p> <p>-------------</p> <p>We will start with an Operating System Image called Raspbian (Raspberry Debian \u2026)</p> <p>You can use your favorite disk image writer to have the disk image written to the uSD.</p> <p>The uSD card to be used on the RPI. Note, this is not a regular file copy operation.</p> <p>You can use Etcher https://etcher.io/</p> <p>From your PC let's prepare the Raspberry PI (RPI) uSD card</p> <p>Make sure your computer can access the Internet</p> <p>If you are using one of our WiFi Access Points in one of the labs or at one of the tracks, the first PC</p> <p>that connects to the WiFi Access point will need to accept the UCSD Wireless Visitor Agreement,</p> <p>just like when connecting directly to UCSD\u2019s Visitor WiFi.</p> <p>Get the Raspbian Lite uSD image here.</p> <p>Etcher can use Zipped files, you don\u2019t need to Unzip the image file. If you are using Linux command</p> <p>lines to write the disk image to a uSD card you may need to extract the file first.</p> <p>This OS (disk) image is based on the Raspbian headless (no GUI). We will use command line on</p> <p>the RPI, to get to the RPI we will be using SSH (secure shell). Don\u2019t worry, these are just command</p> <p>line names. Mastering these will be good skills to have.</p> <p>If you don\u2019t know about SSH and the command line in Linux, you will learn enough in this course.</p> <p>Let's write the disk image into the uSD Card</p> <p>Connect the provided uSD adapter into your PC</p> <p>Insert the provided uSD card (64 Gbytes) into the uSD adapter</p> <p>Install and run Etcher https://etcher.io/</p> <p>Start Etcher, chose the Zipped file with the Disk Image you downloaded,</p> <p>pay attention when choosing the drive with the uSD card on it (e.g., 64 Gbytes)</p> <p>write the image to uSD card.</p> <p>If you are using Linux, after you finish writing the disk image to the uSD card</p> <p>you should see two partitions in your computer file system</p> <p>boot and rootfs</p> <p>If you are not seeing these partitions in your computer, try removing the uSD card from</p> <p>your computer then insert it again. If that does not work, try using a</p> <p>computer running Linux.</p> <p>Prepare the network configuration file.</p> <p>We will create a text file with the WiFi configurations.</p> <p>You can use the nano or gedit on Linux (e.g.,Ubuntu).</p> <p>On other OSes make sure the file you are creating is a plain text file.</p> <p>Don\u2019t use MS Word or other Apps that may save the file with different file formats</p> <p>and hidden text format characters.</p> <p>Note: If you are using a MAC with the SD to uSD adapter, MAC OS may not be able to</p> <p>write the Donkey Image disk \u201cboot\u201d. Use the USB uSD card adapter provider for each</p> <p>Team.</p> <p>At the root partition of the uSD card, will name the file as wpa_supplicant.conf</p> <p>Using Linux or a Mac terminal, the command line you will use is</p> <p>sudo nano wpa_supplicant.conf</p> <p>Navigate to the drive that you created the RPI image.</p> <p>Look for a boot partition</p> <p>Edit and save the file with this name wpa_supplicant.conf</p> <p>Open a terminal ex:</p> <p>cd /media/UserID/boot</p> <p>ex:</p> <p>cd /media/jack/boot</p> <p>sudo nano wpa_supplicant.conf</p> <p>or</p> <p>sudo nano /media/UserID/boot/wpa_supplicant.conf</p> <p>You will need to enter your password</p> <p>ex: nano /media/jack/boot/wpa_supplicant.conf</p> <p>on a mac</p> <p>cd /Volumes/boot</p> <p>nano wpa_supplicant.conf</p> <p>Here is the content of wpa_supplicant.conf</p> <p>You can copy and paste it</p> <p>ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev</p> <p>update_config=1</p> <p>country=US</p> <p>network={</p> <p>ssid=\"SD-DIYRoboCar5GHz\"</p> <p>key_mgmt=WPA-PSK</p> <p>psk=\"SDrobocars2017\"</p> <p>priority=100</p> <p>id_str=\"SD-DIYRoboCars5GHz\"</p> <p>}</p> <p>network={</p> <p>ssid=\"UCSDRoboCar5GHz\"</p> <p>key_mgmt=WPA-PSK</p> <p>psk=\"UCSDrobocars2018\"</p> <p>priority=90</p> <p>id_str=\"ucsdrobocar5G\"</p> <p>}</p> <p>network={</p> <p>ssid=\"SD-DIYRoboCar\"</p> <p>key_mgmt=WPA-PSK</p> <p>psk=\"SDrobocars2017\"</p> <p>priority=80</p> <p>id_str=\"SD-DIYRoboCars2.4GHz\"</p> <p>}</p> <p>network={</p> <p>ssid=\"UCSDRoboCar\"</p> <p>key_mgmt=WPA-PSK</p> <p>psk=\"UCSDrobocars2018\"</p> <p>priority=70</p> <p>id_str=\"ucsdrobocar2.4GHz\"</p> <p>}</p> <p>If you are using a PC running Linux to edit the file in a later time,</p> <p>place the uSD card into the PC</p> <p>sudo nano /media/user_id/rootfs/etc/wpa_supplicant/wpa_supplicant.conf</p> <p>On first boot, this file will be moved to the path below where it may be edited later</p> <p>as needed.</p> <p>To edit current WiFi connections or to add WiFi Connections</p> <p>You will need to ssh to to RPI and edit the following file</p> <p>/etc/wpa_supplicant/wpa_supplicant.conf</p> <p>From a terminal where you SSH to the RPI</p> <p>sudo nano /etc/wpa_supplicant/wpa_supplicant.conf</p> <p>Please change the hostname and the password of your RPI so other teams don't connect to your RPI by mistake.</p> <p>Change the Hostname (HostID)</p> <p>Using Linux you will be able to edit two files that are in the following directory</p> <p>When using MacOS you may not see ext3/ext4 partitions that Linux (Raspbian) uses</p> <p>hostname</p> <p>and</p> <p>hosts</p> <p>Replace raspberrypi with ucsdrobocar-xxx-yy</p> <p>The hostnames should follow ucsdrobocar-xxx-yy</p> <p>xx= Team number</p> <p>For team 07, replace raspberrypi by ucsdrobocar07</p> <p>For COMOS</p> <p>For team 07, replace raspberrypi by ucsdcosmos11t07</p> <p>sudo nano /media/UserID/rootfs/etc/hostname sudo nano /media/UserID/rootfs/etc/hosts </p> <p>ex:</p> <p>sudo nano /media/jack/rootfs/etc/hostname</p> <p>sudo nano /media/jack/rootfs/etc/hosts </p> <p>When editing from the RPI</p> <p>sudo nano /etc/hostname</p> <p>sudo nano /etc/hosts</p> <p>If you have not changed the RPI host name, you need to make sure that only your RPI is</p> <p>connected to the WiFi router until you can change its hostname and password.</p> <p>Your computer and the RPI need to be on the same network so you can connect to it.</p> <p>Said so, in our case we have 5GHz WiFi too, you should connect your computer to</p> <p>the 5GHz Access Point. There is a bridge between</p> <p>the 5GHz and 2.4GHz networks in the Access Points we use.</p> <p>Therefore you computer will be able to connect to the</p> <p>RPI.</p> <p>Enable SSH on boot Create a text file and name it ssh in the boot partition</p> <p>On Linux</p> <p>sudo nano /media/UserID/boot/ssh</p> <p>ex: sudo nano /media/jack/boot/ssh</p> <p>On MacOS</p> <p>/Volumes/boot</p> <p>You can add some characters into the file to force nano to write it.</p> <p>Type any short text such as test</p> <p>ex: 123</p> <p>Close any terminal that may be using the uSD card</p> <p>Use the eject equivalent function in your computer to eject the boot and roofs uSD</p> <p>partitions</p> <p>Remove the uSD card from your computer, carefully insert it into your RPI</p> <p>Power-up your RPI. You can use a uUSB power adapter or plug it into a PC USB port.</p> <p>Connecting to your RPI using SSH</p> <p>SSH is installed by default on Linux, MacOS, and Windows 10.</p> <p>On MS Windows 7 or older you will need to get software/app that supports SSH.</p> <p>During the first boot it may take a few minutes for the RPI to be ready.</p> <p>Watch for the RPI green LED flashing, it indicates uSD access.</p> <p>During the first boot Raspbian extend the file system. It takes more time than the next following OS boot.</p> <p>To connect to your RPI you will need to run a computer terminal and issue a ssh command</p> <p>ssh pi@rpiname.local</p> <p>ex: ssh pi@ucsdrobocar07.local</p> <p>password: raspberry</p> <p>If you have changed the hostname of your RPI or you can not see it in the network,</p> <p>maybe because you have incorrect WiFi info, coordinate with other teams</p> <p>that you are the only one doing the initial configuration while connected to the WiFi or direct</p> <p>to the router using a network cable.</p> <p>Otherwise you will be connecting to another Team\u2019s RPI.</p> <p>Connect a network cable to your RPI and the WiFi access point. Boot it...</p> <p>Alternatively, remove the uSD card and put it back in your PC. Read on the net about</p> <p>Raspbian / Linux (Debian) on changing hostname at</p> <p>the OS level at the uSD card on your PC before placing it in RPI.</p> <p>Hint</p> <p>sudo nano</p> <p>/etc/hostname /etc/hosts </p> <p>Please change the hostname and the password so other</p> <p>teams don't work on your RPI.</p> <p>Again</p> <p>If you are having difficulties connecting to you RPI using WiFi. Connect the RPI to the</p> <p>router using a network cable (e.g., CAT5).</p> <p>We should have at least one network cable in lab.</p> <p>SSH into the RPI</p> <p>ssh pi@ucsdrobocar-xxx-yy.local</p> <p>Lets update the repository content and upgrade the raspbian</p> <p>(RPI Linux OS based on Debian)</p> <p>If you did not configure the RPI WiFI correctly it will not connect to</p> <p>your WiFi Access Point (AP)</p> <p>If needed</p> <p>edit the wpa_supplicant.conf file</p> <p>sudo nano /etc/wpa_supplicant/wpa_supplicant.conf</p> <p>SSH into the RPI</p> <p>ssh pi@ucsdrobocar-xxx-yy.local</p> <p>Check that the date and time is correct at the RPI and instal ntp to auto-update date and time</p> <p>date</p> <p>Hint. In the case the clock on the RPI is not updating automatically by getting information</p> <p>from the Internet, you need to set the date and time manually or updates and install may fail</p> <p>Here is how you can do it</p> <p>ex: sudo date -s '2018-10-04 09:30:00'</p> <p>Once you set the country/location, and have an active Internet access,</p> <p>these should force a clock synchronization on boot/reboot</p> <p>Initially it may be off because you have not set the time zone yet. e.g. Pacific Time</p> <p>sudo apt-get update</p> <p>sudo apt-get install ntp</p> <p>sudo /etc/init.d/ntp stop</p> <p>sudo ntpd -q -g sudo /etc/init.d/ntp start</p> <p>Assuming you RPI is connected to the Internet, you can always use</p> <p>sudo ntpd -q -g</p> <p>to get the time from a server in the Internet</p> <p>Reboot to check if the date / times are updating correctly</p> <p>sudo reboot now</p> <p>SSH into the RPI</p> <p>Check that the date and time are correct</p> <p>date</p> <p>Be patient, this may take a while depending on how many updates were released</p> <p>after the uSD card image that you are using</p> <p>sudo apt-get update</p> <p>sudo apt-get upgrade</p> <p>then reboot the RPI</p> <p>sudo reboot now</p> <p>Connect to the RPI As applicable, once you connect to your RPI run the following command</p> <p>sudo raspi-config</p> <p>to change the host name and other things.</p> <p>On the RPI use this command</p> <p>ex: For team 07, use hostname ucsdrobocar07</p> <p>Change the password</p> <p>sudo raspi-config</p> <p></p> <p>User Password, Hostname (if needed), enable camera, enable I2C,</p> <p>locale-time zone to America/ US Pacific / Los Angeles</p> <p>WiFi country US, then expand the file system just in case</p> <p>it was not automatically done at boot.</p> <p>If changed the host name, your RPI will have the new hostname on</p> <p>the next boot</p> <p>sudo reboot now</p> <p>After you configure your RPI WiFi and Hostname correctly and reboot,</p> <p>Your RPI should be accessible using SSH from Linux, Mac, Windows</p> <p>Open a terminal</p> <p>ssh pi@ucsdrobocar-xxx-yy.local</p> <p>enter your password</p> <p>try this just in case you can not connect to the RPI on another WiFi Access point</p> <p>such as your Phone acting as a hotspot</p> <p>ssh pi@ucsdrobocar-xxx-yy (without .local)</p> <p>If you have not done so, change the default password</p> <p>on the RPI</p> <p>passwd</p> <p>To disable downloading translations, to save time,</p> <p>create a file named 99translations</p> <p>sudo nano /etc/apt/apt.conf.d/99translations</p> <p>Place the following line in the 99translations</p> <p>Acquire::Languages \"none\";</p> <p>reboot the RPI</p> <p>sudo reboot now</p> <p>Lets install some dependencies, libraries, and utilities</p> <p>sudo apt-get update</p> <p>This step may take some time, be patient</p> <p>sudo apt-get install build-essential python3 python3-dev python3-pip python3-virtualenv python3-numpy python3-picamera python3-pandas python3-rpi.gpio i2c-tools avahi-utils joystick libopenjp2-7-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev git ntp</p> <p>And install these to have the dependencies for OpenCV</p> <p>sudo apt-get install libilmbase-dev libopenexr-dev libgstreamer1.0-dev libjasper-dev libwebp-dev libatlas-base-dev libavcodec-dev libavformat-dev libswscale-dev libqtgui4 libqt4-test</p> <p>Installing OpenCV</p> <p>sudo apt install python3-opencv</p> <p>reboot the RPI</p> <p>sudo reboot now</p> <p>In case you want to use the (PCA9685) PWM controller from outside a virtual environment</p> <p>sudo apt-get install python3-pip</p> <p>pip3 install Adafruit_PCA9685</p> <p>and this if the Donkey framework complains about the RPI camera</p> <p>pip install \"picamera[array]\"</p> <p>Lets set a virtual environment and name it donkey</p> <p>We will create virtual environments to enable using different software library configurations without having to install them at the root and user level (higher level)</p> <p>If you have not done so, lets create a directory to store our projects and one subdirectory</p> <p>to store virtual environments</p> <p>cd ~</p> <p>mkdir projects</p> <p>cd projects</p> <p>mkdir envs</p> <p>cd envs</p> <p>pip3 install virtualenv</p> <p>python3 -m virtualenv -p python3 ~/projects/envs/donkey --system-site-packages</p> <p>this line will activate the virtual environment called donkey every time the</p> <p>user pi logs in a terminal</p> <p>echo \"source ~/projects/envs/donkey/bin/activate\" &gt;&gt; ~/.bashrc</p> <p>source ~/.bashrc</p> <p>When a virtual environment is active, you should see (name_of_virtual_enviroment)</p> <p>in front of the terminal prompt</p> <p>ex:</p> <p>(donkey) pi@ucsdrobocar-xxx-yy:~\\$</p> <p>Testing to see if OpenCV is installed in the virtual env</p> <p>python</p> <p>import cv2</p> <p>cv2.__version__</p> <p>exit ()</p> <p>[GCC 8.2.0] on linux</p> <p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p> <p>&gt;&gt;&gt; import cv2</p> <p>&gt;&gt;&gt; cv2.__version__</p> <p>'3.2.0'</p> <p>&gt;&gt;&gt; exit ()</p> <p>Lets install the Donkeycar AI framework</p> <p>if not done, create a directory called projects</p> <p>mkdir projects</p> <p>cd projects</p> <p>Get the latest donkeycar from Github.</p> <p>git clone https://github.com/autorope/donkeycar</p> <p>cd donkeycar</p> <p>git checkout master</p> <p>This step may take some time, be patient</p> <p>pip install -e .[pi]</p> <p>Lets install Tensorflow</p> <p>single board computer (SBC) ...</p> <p>cd ~/projects</p> <p>Type the command below all in one line</p> <p>wget https://github.com/PINTO0309/Tensorflow-bin/raw/master/tensorflow-2.0.0-cp37-cp37m-linux_armv7l.whl</p> <p>Lets install tensorflow. This step may take some time to install. You are using a low power</p> <p>pip install --upgrade tensorflow-2.0.0-cp37-cp37m-linux_armv7l.whl</p> <p>Quick test of the Tensorflow install</p> <p>python -c \"import tensorflow\"</p> <p>If no errors, you should be good.</p> <p>If you want to know what is running in your RPI while it is busy,</p> <p>open another terminal, or tab in the same terminal, SSH into the RPI</p> <p>then run the command top</p> <p>top</p> <p>ex: you can see that the CPU(s) is/are busy</p> <p>wait for the install to finish\u2026</p> <p></p> <p>Try the Tensorflow install again few times if it fails</p> <p>If needed, upgrade pip</p> <p>pip install --upgrade pip</p> <p>Let\u2019s create a car on the path ~/projects/d3</p> <p>donkey createcar --path ~/projects/d3</p> <p>Your RPI directory should look like this</p> <p>list the content of a directory with ls</p> <p>d3 donkeycar envs tensorflow-2.0.0-cp37-cp37m-linux_armv7l.whl</p> <p>cd ~/projects/d3</p> <p>ls</p> <p>The output should be something similar to</p> <p>config.py data logs manage.py models myconfig.py train.py</p> <p>Research what you have to do so when you log into your RPI you are working from</p> <p>the car directory like ~/projects/d3 vs. having to issue the command cd /projects/d3</p> <p>all the time after you log into the RPI.</p> <p>What happens when the RPI boots in relation to the file .bashrc ( ~/.bashrc) ?</p> <p>To enable the use of a WebCam (USB cameras)</p> <p>sudo apt-get install python3-dev python3-numpy libsdl-dev libsdl-image1.2-dev \\</p> <p>libsdl-mixer1.2-dev libsdl-ttf2.0-dev libsmpeg-dev libportmidi-dev \\</p> <p>libavformat-dev libswscale-dev libjpeg-dev libfreetype6-dev</p> <p>pip install pygame</p> <p>Here is a link to a plug an play image with necessary software installed</p> <p>If you are at UCSD connected to one of the classes WiFi access points you can</p> <p>connect to the RPI with</p> <p>ssh pi@ucsdrobocar-xxx-yy.local</p> <p>raspberryucsd</p> <p>Then make sure to change the hostname and password. See instructions in this document.</p> <p>Installing the Pulse Width Modulation (PWM)</p> <p>https://docs.google.com/document/d/11nu6_ReReoIxA1KVq-sCy7Tczbk6io0izcItucrw7hI/edit</p> <p></p> <p>For reference, below is the Raspberry Pi Pinout.</p> <p>You will notice we connect to +3.3v, the two I2C pins (SDA and SCL) and ground:</p> <p></p> <p>After connecting the I2C PWM Controller lets test the communication</p> <p>Power the RPI, you can use the USB power adapter for this test</p> <p>sudo i2cdetect -y 1</p> <p>The output is something like</p> <p>0 1 2 3 4 5 6 7 8 9 a b c d e f</p> <p>00: -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>40: 40 -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>70: 70 -- -- -- -- -- -- --</p> <p>(dk)pi@jackrpi02:~ \\$</p> <p>Look for a similar result above. 40 ...70 in this case is what we are looking for.</p> <p>Now that the RPI3 is talking to the I2C PWM board we can calibrate the Steering and Throttle.</p> <p>Lets get the EMO circuit working too in case you need to use it.</p> <p>For the UCSD RoboCars, an Emergency Stop Circuit is required.</p> <p>COSMOS RoboCars are not required to have the EMO</p> <p>After you charge your Lithium Polymer (LiPo) battery(ries) - some info here</p> <p>Connect the battery(ries) and batteries monitor/alarm</p> <p>Please do not use the batteries without the batteries monitor/alarms</p> <p>If you discharge a LiPO batteries below a threshold lets say 3.2 volts. It may not recover to be charged. That is why it is critical that you use the LiPo batteries alarm all the time.</p> <p>Connecting the Emergency Stop Circuit and Batteries into the Robot</p> <p>For this part of your robot, you will have to do some hacking. That is part of the class.</p> <p>The instructor will discuss the principle of the circuit and how to implement it with the component in your robot kit.</p> <p>Long story short, the PWM Controller we use has a disable pin. It the correct logic is applied to it for example Logic 1 or 0 (3.3V or 0V) it will disable the PWM signals and the UCSDRoboCar will stop.</p> <p>Think why one needs a separate EMO circuit vs. relying on the software, operating system and computer communicating with the PWM controller that then send PWM pulses to the actuators (steering servo and driving DC motor by the electronics speed controller)</p> <p>First search on the web and read the datasheet of the emergency stop switch (EMO) components provided with the robot kit and discuss them with your teammates how the EMO will work. You got two main components to build the a WireLess EMO switch:</p> <p>3)  A Wireless Relay with wireless remote controls</p> <p>4)  A Red/Blue high power LED. This is to help the user know if the car     is disable (Blue) or Enabled (Red).</p> <ul> <li> <p>What is the disable pin the PWM controller?</p> </li> <li> <p>Does it disable on logic 1 or 0?</p> </li> <li> <p>How to wire the wireless relay to provide the logic you need to   disable PWM controller? (1 or 0)</p> </li> <li> <p>How to connect the LEDs (Blue and Red) to indicate (RED -   hot/enabled), (BLUE - power is on / disabled).</p> </li> <li> <p>Note: The power to the PWM controller, that powers the LEDs, comes   from ESC (Electronics Speed Controller). Therefore, you have to   connect the robot batteries to the ESC and the ESC to the PWM   controller. We are using channel 2 for the ESC. Channel 1 for   the Steering.</p> </li> </ul> <p>After you see that the EMO is working, i.e. wireless remote control disable the PWM, and the LEDs light up as planned, then you need to document your work. Please use a schematic software such as Fritzing (http://fritzing.org/home/) to document your electrical schematic.</p> <p>It may seem we did the opposite way; document after your build. In our case, you learned by looking at components information, thinking about the logic, and experimenting. Since you are an engineer you need to document even it if was a hack\u2026</p> <p>Working in a company you may do fast prototyping first then document. On larger projects you make schematics, diagrams, drawings, work instructions, then build it. Keep that in mind!</p> <p>Calibration of the Throttle and Steering</p> <p>Before you develop code or you can use a someone's code to control a robot, we need to calibrate the actuator/mechanism to find out its range of motion compared to the control / command a computer will send to the controller of the actuator/mechanism.</p> <p>The calibration is car specific. If you use the same platform, the numbers should be really close. Otherwise, you will need to calibrate your car.</p> <p>We are following the standard from RC CAR world, Channel 1 for Steering, Channel 2 for Throttle</p> <p>Note:The default DonkeyCar build uses Channels 0 and 1</p> <p>The UCSDRoboCar has at least two actuators. A steering servo and the DC motor connected to an Electronics Speed Controller (ESC). These are controlled by PWM (Pulse Width Modulation).</p> <p>We use PWM Controller to generate the PWM signals, a device similar to the one in the picture below</p> <p></p> <p>Shutdown the RPI if it is on type this command</p> <p>sudo shutdown -h now</p> <p>Connect the Steering Servo to the Channel 1</p> <p>Connect the Throttle (ESC) to Channel 2</p> <p>Observe the orientation of the 3 wires connector coming from the Steering Servo and ESC. Look for a the black or brown wire, that is the GND (-).</p> <p>MAKE SURE THE ROBOT IS ON THE RC CAR STAND</p> <p>Follow the safety guidelines provided in person in the class. If something does not sound right, don\u2019t do it. Safety first.</p> <p>Power on the RPI</p> <p>Power the Electronic Speed Controller (ESC)</p> <p>Enable the robot using your EMO wireless control - The safety LED should be RED</p> <p>Lets run a Python command to calibrate Steering and Throttle</p> <p>The donkey commands need to be run from the directory you created for your car, i.e., d2</p> <p>If you have not done so, SSH into the RPI</p> <p>If needed, change directory to d2t</p> <p>cd d2t</p> <p>The output</p> <p>(env) pi@jackrpi10:~/projects/d3 \\$</p> <p>donkey calibrate --channel 1</p> <p>Try some values around 400 to center the steering</p> <p>Enter a PWM setting to test(100-600)370</p> <p>Enter a PWM setting to test(100-600)365</p> <p>Enter a PWM setting to test(100-600)360</p> <p>In my case, 365 seems to center the steering</p> <p>Take note of the left max, right max,</p> <p>ex:</p> <p>365 - Center</p> <p>285 - Steering left max</p> <p>440 - Steering right max</p> <p>Note: When donkey calibrate times-out after few tries, just run it again</p> <p>donkey calibrate --channel 1</p> <p>You can interrupt the calibration by typing CTRL-C</p> <p>MAKE SURE THE ROBOT IS ON THE RC CAR STAND</p> <p>Let me say this one more time</p> <p>MAKE SURE THE ROBOT IS ON THE RC CAR STAND</p> <p>Now to calibrate the Throttle Electronic Speed Controller ESC (THROTTLE)</p> <p>Following the standard for R/C cars. Throttle goes on channel 2</p> <p>donkey calibrate --channel 2</p> <p>Enter a PWM setting to test(100-600)370 (neutral)</p> <p>Enter a PWM setting to test(100-600)380</p> <p>Enter a PWM setting to test(100-600)390</p> <p>370 when power up the ESC seems to be the middle point (neutral),</p> <p>lets use 370 also to be compatible with Donkey.</p> <p>Neutral when power the ESC - 370</p> <p>Then go in increments of 10~20 until you can no longer hear increase on the speed</p> <p>of the car. Don\u2019t worry much about the max speed since we won\u2019t drive that fast autonomously</p> <p>and during training the speed will be limited.</p> <p>Max speed forward - 460</p> <p>Reverse on RC cars is a little tricky because the ESC needs to receive</p> <p>a reverse pulse, zero pulse, and again reverse pulse to start to go backwards.</p> <p>Use the same technique as above set the PWM setting to your zero throttle (lets say 370).</p> <p>Enter the reverse value, then the zero throttle (e.g., 370) value, then the reverse value again.</p> <p>Enter values +/- 10 of the reverse value to find a reasonable reverse speed. Remember this reverse PWM value.</p> <p>(dk)pi@jackrpi02:~/projects/d3 \\$</p> <p>donkey calibrate --channel 2</p> <p>\u2026</p> <p>Enter a PWM setting to test(100-600)360</p> <p>Enter a PWM setting to test(100-600)370</p> <p>Enter a PWM setting to test(100-600)360</p> <p>Enter a PWM setting to test(100-600)350</p> <p>Enter a PWM setting to test(100-600)340</p> <p>Enter a PWM setting to test(100-600)330</p> <p>Enter a PWM setting to test(100-600)320</p> <p>Enter a PWM setting to test(100-600)310</p> <p>Enter a PWM setting to test(100-600)300</p> <p>Enter a PWM setting to test(100-600)290</p> <p>Neutral when power the ESC - 370</p> <p>Max speed forward - 460</p> <p>Max speed backwards - 280</p> <p>and from the steering calibration</p> <p>Center - 365</p> <p>Steering left max - 285</p> <p>Steering right max - 440</p> <p>Now let's write these values into the car configuration file</p> <p>(dk)pi@jackrpi02:~/projects/d3 \\$ ls</p> <p>config.py data logs manage.py models</p> <p>Edit the file config.py</p> <p>(dk)pi@jackrpi02:~/projects/d3 \\$</p> <p>nano config.py</p> <p>Change these values according to YOUR calibration values and where you have the Steering Servo and ESC connected</p> <p>...</p> <p>STEERING</p> <p>STEERING_CHANNEL = 1</p> <p>STEERING_LEFT_PWM = 285</p> <p>STEERING_RIGHT_PWM = 440</p> <p>THROTTLE</p> <p>THROTTLE_CHANNEL = 2</p> <p>THROTTLE_FORWARD_PWM = 460</p> <p>THROTTLE_STOPPED_PWM = 370</p> <p>THROTTLE_REVERSE_PWM = 280</p> <p>Also, if needed change these</p> <p>JOYSTICK</p> <p>USE_JOYSTICK_AS_DEFAULT = True</p> <p>JOYSTICK_MAX_THROTTLE = 0.4</p> <p>JOYSTICK_STEERING_SCALE = 1.0</p> <p>AUTO_RECORD_ON_THROTTLE = True</p> <p>CONTROLLER_TYPE='ps3' (ps3|ps4)</p> <p>USE_NETWORKED_JS = False</p> <p>NETWORK_JS_SERVER_IP = \"192.168.0.1\"</p> <p>LED</p> <p>HAVE_RGB_LED = True</p> <p>LED_INVERT = False COMMON ANODE?</p> <p>board pin number for pwm outputs</p> <p>LED_PIN_R = 12</p> <p>LED_PIN_G = 10</p> <p>LED_PIN_B = 16</p> <p>LED status color, 0-100</p> <p>LED_R = 0</p> <p>LED_G = 0</p> <p>LED_B = 10</p> <p>Another example of configuration for the config.py, now with a joystick dead zone for neutral</p> <p>donkey calibrate --channel 1</p> <p>donkey calibrate --channel 2</p> <p>config.py settings</p> <p>STEERING</p> <p>STEERING_CHANNEL = 1</p> <p>STEERING_LEFT_PWM = 275</p> <p>STEERING_RIGHT_PWM = 440</p> <p>Center ~ 360</p> <p>THROTTLE</p> <p>THROTTLE_CHANNEL = 2</p> <p>THROTTLE_FORWARD_PWM = 460</p> <p>THROTTLE_STOPPED_PWM = 370</p> <p>THROTTLE_REVERSE_PWM = 220</p> <p>JOYSTICK_MAX_THROTTLE = 0.8</p> <p>At the UCSD Outdoor Track my car was turning too much. I scaled the steering. May need to increase the number to have sharper turns.</p> <p>JOYSTICK_STEERING_SCALE = 0.8</p> <p>Because I have a particular joystick that the Throttle neutral was not centered, I had to</p> <p>use a deadzone value on config.py</p> <p>Try first with deadzone= 0</p> <p>JOYSTICK_DEADZONE = 0.02</p> <p>LED</p> <p>HAVE_RGB_LED = True</p> <p>board pin number for pwm outputs</p> <p>LED_PIN_R = 8</p> <p>LED_PIN_G = 10</p> <p>LED_PIN_B = 12</p> <p>LED status color, 0-100</p> <p>LED_R = 0</p> <p>LED_G = 0</p> <p>LED_B = 60</p> <p>Note: If you robot has too much power or not enough power when you start driving it, adjust the max_throttle</p> <p>JOYSTICK_MAX_THROTTLE = 0.5</p> <p>This also will be your starting power setting when using the constant throttle autopilot.</p> <p>Connecting the PS3 Keypad/Joystick</p> <p>http://docs.donkeycar.com/parts/controllers/</p> <p>Bluetooth Setup</p> <p>https://pythonhosted.org/triangula/sixaxis.html</p> <p>We need to install some bluetooth software in the RPI</p> <p>ssh to the RPI</p> <p>sudo apt-get update</p> <p>sudo apt-get install bluetooth libbluetooth3 libusb-dev</p> <p>sudo systemctl enable bluetooth.service</p> <p>sudo usermod -G bluetooth -a pi</p> <p>shutdown the RPI</p> <p>sudo shutdown now</p> <p>Remove the power from the RPI. Wait ~ 5 s.</p> <p>Power on the RPI,</p> <p>Connect your PS3 Controller to the RPI using a mini USB Cable.</p> <p>SSH to the RPI</p> <p>Lets download, compile, and install some bluetooth configuration software</p> <p>wget http://www.pabr.org/sixlinux/sixpair.c</p> <p>gcc -o sixpair sixpair.c -lusb</p> <p>sudo ./sixpair</p> <p>Output should be something similar to this</p> <p>Current Bluetooth master: b8:27:eb:49:2d:8c</p> <p>Setting master bd_addr to b8:27:eb:49:2d:8c</p> <p>Execute the \u2018bluetoothctl\u2019 command</p> <p>Pay attention to your PS3 controller mac address</p> <p>bluetoothctl</p> <p>the output should be something similar to this</p> <p>(env) pi@jackrpi10:~ \\$ bluetoothctl</p> <p>[NEW] Controller B8:27:EB:7A:12:18 jackrpi10 [default]</p> <p>[NEW] Device 64:D4:BD:03:CD:C9 PLAYSTATION(R)3 Controller</p> <p>If you don\u2019t see the PS3 Controller, unplug and plug it in RPI again</p> <p>Type agent on</p> <p>[bluetooth]agent on</p> <p>Agent is already registered</p> <p>Type default-agent</p> <p>default-agent</p> <p>If you don\u2019t see the PS3 Controller, unplug and plug it in RPI again</p> <p>default-agent64:D4:BD:03:CD:C9</p> <p>[bluetooth]default-agent</p> <p>Default agent request successful</p> <p>[NEW] Device 64:D4:BD:03:CD:C9 Sony PLAYSTATION(R)3 Controller</p> <p>Authorize service</p> <p>[agent] Authorize service 00001124-0000-1000-8000-00805f9b34fb (yes/no):</p> <p>Type yes</p> <p>Yes</p> <p>[CHG] Device 64:D4:BD:03:CD:C9 Trusted: yes</p> <p>[CHG] Device 64:D4:BD:03:CD:C9 UUIDs: 00001124-0000-1000-8000-00805f9b34fb</p> <p>[NEW] Device 64:D4:BD:03:CD:C9 Sony PLAYSTATION(R)3 Controller</p> <p>Authorize service</p> <p>Yes</p> <p>Type trust and the MAC address for the PS3 controller</p> <p>ex: [bluetooth]trust 64:D4:BD:03:CD:C9</p> <p>trust THE_MAC_ADDRESS_PS3_CONTROLLER</p> <p>Changing 64:D4:BD:03:CD:C9 trust succeeded</p> <p>[CHG] Device 64:D4:BD:03:CD:C9 Trusted: yes</p> <p>[bluetooth]quit</p> <p>Agent unregistered</p> <p>Lets check what input devices are available</p> <p>ls /dev/input</p> <p>Output</p> <p>by-id by-path event0 event1 js0 mice</p> <p>you should see a js0</p> <p>If connected by the USB cable, disconnect the PS3 controller from the RPI.</p> <p>Lets list the input devices again</p> <p>ls /dev/input</p> <p>Output</p> <p>mice</p> <p>Now press the \\&lt;PS&gt; button at the PS3 controller,</p> <p>the lights on the front of the controller should flash for a couple of seconds then stop</p> <p>The LED 1 should then stay one or flash then goes off. Since we are not using a PS3 game</p> <p>console, the LEDS on the controller showing the connections are not reliable.</p> <p>Again, we are not connecting the PS3 controller to a PS3 game console...). As long as you see a</p> <p>js0 listed on your input devices you are good to go. See below.</p> <p>ls /dev/input</p> <p>Output</p> <p>event0 event1 js0 mice</p> <p>Please keep your PS3 Controller Off when you are not using it. Press and hold the \\&lt;PS&gt;</p> <p>button for ~10s. The LEDs at the controller will flash then go off.</p> <p>If the PS3 controller connection is intermittent, try</p> <p>sudo rpi-update</p> <p>To remove a device, lets say another JoyStick that you don\u2019t use anymore</p> <p>bluetoothctl</p> <p>paired-devices</p> <p>remove THE_MAC_ADDRESS</p> <p>[NEW] Controller B8:27:EB:72:95:A6 rpimine01 [default]</p> <p>[NEW] Device 00:1E:3D:D8:EA:15 PLAYSTATION(R)3 Controller</p> <p>[NEW] Device 00:16:FE:74:12:B7 PLAYSTATION(R)3 Controller</p> <p>[bluetooth]remove 00:1E:3D:D8:EA:15</p> <p>[DEL] Device 00:1E:3D:D8:EA:15 PLAYSTATION(R)3 Controller</p> <p>Device has been removed</p> <p>[bluetooth]</p> <p>If you are having problems pairing the PS3 controller, please reset it.</p> <p>There is \u201csmall\u201d reset button at the back. Look up at the Internet if you can\u2019t find it.</p> <p>This is another method that worked for me with the the RaspbianBuster (Sep 2019)</p> <p>sudo apt-get install bluetooth libbluetooth3 libusb-dev</p> <p>sudo systemctl enable bluetooth.service</p> <p>sudo bluetoothctl</p> <p>agent on</p> <p>default-agent</p> <p>scan on</p> <p>Plug the PS3 Controller in the RPI using a Mini USB Cable</p> <p>Trust when asked</p> <p>Disconnect the PS3 Controller</p> <p>Press the PS Button on the Controller. See if it will pair</p> <p>Now you can go drive you robot to collect data. Make sure to keep the</p> <p>EMO handy and use when needed!</p> <p>Also the \\&lt;X&gt; on your controller is like an emergency break.</p> <p>Driving the Robot to Collect data</p> <p>Remember you are driving at max of x (0.x) Throttle power based on the</p> <p>config.py that you edited.</p> <p>To reverse you may have to reverse, stop, reverse. This is a feature of the</p> <p>ESC using on RC cars to prevent damaging gears when changing from forward to reverse.</p> <p>On the RPI</p> <p>cd d2t</p> <p>(env) pi@jackrpi10:~/projects/d3 \\$</p> <p>python manage.py drive</p> <p>Note: CTRL-C stop the manage.py drive</p> <p>Output below</p> <p>(env) pi@jackrpi10:~/projects/d3 \\$ python manage.py drive</p> <p>using donkey v2.5.0t ...</p> <p>loading config file: /home/pi/d2t/config.py</p> <p>config loaded</p> <p>cfg.CAMERA_TYPE PICAM</p> <p>PiCamera loaded.. .warming camera</p> <p>Adding part PiCamera.</p> <p>Adding part PS3JoystickController.</p> <p>Adding part ThrottleFilter.</p> <p>Adding part Lambda.</p> <p>Adding part Lambda.</p> <p>Adding part Lambda.</p> <p>Init ESC</p> <p>Adding part PWMSteering.</p> <p>Adding part PWMThrottle.</p> <p>Tub does NOT exist. Creating new tub...</p> <p>New tub created at: /home/pi/d2t/data/tub_2_18-09-25</p> <p>Adding part TubWriter.</p> <p>You can now move your joystick to drive your car.</p> <p>Starting vehicle...</p> <p>Opening /dev/input/js0...</p> <p>Device name: PLAYSTATION(R)3 Controller</p> <p>/home/pi/env/lib/python3.5/site-packages/picamera/encoders.py:544: PiCameraResolutionRounded: frame size rounded up from 160x120 to 160x128</p> <p>width, height, fwidth, fheight)))</p> <p>If you get this error trying to drive it is because the PS3 game controller is off</p> <p>or not connected</p> <p>Output</p> <p>/dev/input/js0 is missing</p> <p>ls</p> <p>config.py data logs manage.py models sixpair sixpair.c</p> <p>ls data</p> <p>tub_1_17-10-13</p> <p>If you want to wipe clean the data collected</p> <p>Remove the content of the ~./d2t/data directory. It should be tub_\u2026\u2026</p> <p>You can delete the entire directory then create it again.</p> <p>(dk)pi@jackrpi02:~/projects/d3 \\$</p> <p>rm -rf data</p> <p>mkdir data</p> <p>Donkey Install Using Linux - Ubuntu</p> <p>It is highly recommended that you use Ubuntu in this class. Ideally you can make your computer</p> <p>dual boot to Ubuntu. Look for instructions at the Internet for that. PLEASE backup your computer</p> <p>first.</p> <p>Alternatively, you can install a Virtual Machine management software. e.g., VirtualBox.</p> <p>Then install Ubuntu.</p> <p>These first instructions wont have GPU support.</p> <p>Nvidia GPU support documentation is provided later in this document.</p> <p>Installation of GPU features won\u2019t be supported in the class.</p> <p>You will have access to a GPU Cluster from the UC San Diego Supercomputer Center.</p> <p>Lets refresh the Ubuntu repositories and installed software</p> <p>sudo apt-get update</p> <p>sudo apt-get upgrade</p> <p>First lets create a directory to store your projects</p> <p>mkdir projects</p> <p>cd projects</p> <p>Install some necessary software and create a virtual environment</p> <p>sudo apt-get install virtualenv build-essential python3-dev gfortran libhdf5-dev libatlas-base-dev</p> <p>virtualenv env -p python3</p> <p>Activate the virtual environment</p> <p>source env/bin/activate</p> <p>Look for the (env) in front of your command line. It is indication that env is active</p> <p>To deactivate an environment type deactivate from inside the environment</p> <p>To activate, make sure you are at the projects directory then type source env/bin/activate</p> <p>To install specific versions of Keras and Tensorflow (non-GPU)</p> <p>Install Keras</p> <p>pip install keras==2.2.2</p> <p>Install tensorflow</p> <p>pip install tensorflow==1.10.0</p> <p>As 03Feb19, the latest version of tensorflow is 1.12</p> <p>Keep the same version of Tensorflow as close as the Tensorflow in the robocar</p> <p>(Raspberry) as feasible.</p> <p>pip install tensorflow==1.12</p> <p>if needed, install keras 2.2.4</p> <p>pip install keras==2.2.4</p> <p>Run a short TensorFlow program to test it</p> <p>Invoke python from your shell as follows:</p> <p>\\$ python</p> <p>Enter the following short program inside the python interactive shell:</p> <p>Python</p> <p>import tensorflow as tf</p> <p>hello = tf.constant('Hello, TensorFlow!')</p> <p>sess = tf.Session()</p> <p>print(sess.run(hello))</p> <p>Ctr-D gets out of the Python Tensorflow test.</p> <p>Install Donkey on your Ubuntu Linux Computer</p> <p>cd projects</p> <p>If you want to create another virtual environment or env was not created yet</p> <p>Install some necessary software and create a virtual environment</p> <p>sudo apt-get install virtualenv build-essential python3-dev gfortran libhdf5-dev libatlas-base-dev</p> <p>virtualenv env -p python3</p> <p>Activate the virtual environment</p> <p>source env/bin/activate</p> <p>Look for the (env) in front of your command line. It is indication that env is active</p> <p>To deactivate an environment type deactivate from inside the environment</p> <p>To activate, make sure you are at the projects directory then type source env/bin/activate</p> <p>This is like you did on RPI but I had my car under the ~/projects directory</p> <p>Lets get the latest Donkey Framework from Tawn Kramer</p> <p>git clone https://github.com/tawnkramer/donkey pip install -e donkey[pc]</p> <p>Create a car</p> <p>donkey createcar --path ~/projects/d2t</p> <p>from here you can transfer data from your RPI,and then train</p> <p>create a model (autopilot), transfer the model to the RPI, test it.</p> <p>Using the GPU Cluster to Train the Auto Pilots</p> <p>27Aug18 - Under development - Please provide feedback</p> <p>We are following the instruction provide by out IT Team</p> <p>https://docs.google.com/document/d/e/2PACX-1vTe9sehl7izNJJNypsDNABD4wg-F-AClAi0cYV3pIIRGpCknD7SEWQPEGqy_5DBRmFQtkulLkHkLxEm/pub</p> <p>We will be using the UCSD Supercomputer GPU Cluster. There are few steps that you need to follow. Pay attention to not leave machines running on the cluster because it is a share resource.</p> <p>You should be able to log with you student credentials.</p> <p>Pay attention to not use a GPU machine to do work that does not require GPU...</p> <p>To begin, login to your account using an SSH client. Most students will use their standard email username to sign on</p> <p>Create Car Project</p> <p>ssh YOUR_USER_ID@ieng6.ucsd.edu</p> <p>prep me148f</p> <p>We will launch a container (similar to a virtual machine) without GPU. Tensorflow is not installed on it.</p> <p>Lets launch a machine to enable us to create a robocar using the Donkey Framework</p> <p>No GPU to save resources</p> <p>launch-py3torch.sh</p> <p>Attempting to create job ('pod') with 4 CPU cores, 16 GB RAM, and 0 GPU units.</p> <p>To keep the car name the same as in the RPI I created a d2t car</p> <p>source activate /datasets/conda-envs/me148f-conda/</p> <p>donkey createcar --path projects/d2t/</p> <p>The car was created under projects</p> <p>cd ~/projects/d2t</p> <p>Lets exit the container you used to create the Car</p> <p>exit</p> <p>Lets check that we don\u2019t have any container running under our user</p> <p>kubectl get pods</p> <p>No resources found.</p> <p>If there is nothing still running, lets get out of the ieng6 machine</p> <p>logout</p> <p>If you see a container still running,</p> <p>f \u201ckubectl get pods\u201d shows leftover containers still running, you may kill them as follows:</p> <p>[ee148vzz@ieng6-201]:~:505\\$ kubectl get pods</p> <p>NAME READY STATUS RESTARTS AGE</p> <p>ee148vzz-24313 1/1 Running 0 9m</p> <p>[ee148vzz@ieng6-201]:~:506\\$ kubectl delete pod agt-24313</p> <p>pod \"ee148vzz-24313\" deleted</p> <p>[ee148vzz@ieng6-201]:~:507\\$ kubectl get pods</p> <p>No resources found.</p> <p>[ee148vzz@ieng6-201]:~:508\\$</p> <p>If there is no container running, lets get out of the ieng6 machine</p> <p>logout</p> <p>Now, let\u2019s bring some data in so we can train on it</p> <p>Use rsync command below in one line</p> <p>You can rsync direct from the RPI. SSH to the RPI the issue the rsync command to rsync direct to the GPU cluster. Or you can rsync the data to your computer then rsync to the GPU cluster</p> <p>Example of rsync one particular Tub directory</p> <p>rsync -avr -e ssh --rsync-path=cluster-rsync tub_1_17-11-18 YOUR_USER_ID@ieng6.ucsd.edu:projects/d2/data/</p> <p>Example of rsync the entire data directory Sending data to the UCSD GPU Cluster direct from the RPI rsync -avr -e ssh --rsync-path=cluster-rsync ~/projects/d3/data/* YOUR_USER_ID@ieng6.ucsd.edu:projects/d2/data/  </p> <p>or if you have the car created at d2t at the RPI and GPU Cluster  </p> <p>rsync -avr -e ssh --rsync-path=cluster-rsync ~/projects/d3/data/* YOUR_USER_ID@ieng6.ucsd.edu:projects/d2t/data/</p> <p>To enable constant rsync and train while you collect data (drive) we need to exchange a key between the two computers we will rsync. e.g., GPU Cluster and RPI, your computer and RPI, your computer and the GPU Cluster</p> <p>To Enable Continuous Training - And to not have to type password for SSH and RSYNC</p> <p>At the PC or RPI</p> <p>Generate the Private and Public Keys</p> <p>cd ~</p> <p>sudo apt-get install rsync</p> <p>ssh-keygen -t rsa</p> <p>You may see a warning if you already have a key in your system. Just keep or replace it as you</p> <p>wish. Keep in mind that if you replace the key your previous trust connections will be lost.</p> <p>Linux Ubuntu</p> <p>cat .ssh/id_rsa.pub</p> <p>macos</p> <p>cat ~/.ssh/id_rsa.pub</p> <p>Copy the output of the command into a buffer (e.g., Ctrl-C)</p> <p>You can use two terminals to easy the copy and paste the information</p> <p>Leave a terminal open with the key you generated</p> <p>From your PC to the RPI</p> <p>Try this first. If does not work try the steps below this line</p> <p>cat ~/.ssh/id_rsa.pub | ssh pi@YOUR_PI_NAME.local 'cat &gt;&gt; .ssh/authorized_keys'</p> <p>or</p> <p>From the PC to the RPI or PC to the Cluster</p> <p>At the RPI or GPU Cluster</p> <p>cd ~</p> <p>mkdir .ssh</p> <p>If the directory exists, just ignore this error if you see it</p> <p>&gt; mkdir: cannot create directory \u2018.ssh\u2019: File exists</p> <p>chmod 700 .ssh</p> <p>chown pi:pi .ssh</p> <p>nano .ssh/authorized_keys</p> <p>then copy and paste the information from the host id_rsa.pub you generated earlier at the PC or RPI</p> <p>I was able to ssh to the RPI without asking for the password</p> <p>ex: ssh pi@jackrpi05.local</p> <p>Here is what I have done to ssh and use rsync from my computer to the Cluster</p> <p>without having to type the user password all the time</p> <p>for user me148f</p> <p>SSH to ieng6</p> <p>ssh me148f@ieng6.ucsd.edu</p> <p>[me148f@ieng6-201]:~:44\\$ cd ~</p> <p>[me148f@ieng6-201]:~:45\\$ mkdir .ssh</p> <p>[me148f@ieng6-201]:~:46\\$ chmod 700 .ssh</p> <p>[me148f@ieng6-201]:~:48\\$ chown me148f:me148f .ssh</p> <p>Just ignore the error below</p> <p>chown: invalid group: 'me148f:me148f'</p> <p>[me148f@ieng6-201]:~:49\\$ nano .ssh/authorized_keys</p> <p>Now copy and paste the Key you created on your computer.</p> <p>You can use this same process to ssh / rsync from the RPI to the Cluster</p> <p>without having to type the user password all the time.</p> <p>Now you should be able to ssh and rsync to the RPI or Cluster without typing the user password</p> <p>ex: ssh YOUR_USER_ID@ieng6.ucsd.edu</p> <p>If you can not ssh to the RPI or Cluster, fix the problem before continue</p> <p>Constant rsync - sync data while you drive the car</p> <p>Lets make rsync run periodically</p> <p>We will be using some Python Code</p> <p>rsync data continuously to the data directory under where the code was called from</p> <p>ex: ~/projects/d2t</p> <p>From the PC to the RPI</p> <p>At the PC, create a file name continous_data_rsync.py</p> <p>nano continous_data_rsync.py</p> <p>import os</p> <p>import time</p> <p>while True:</p> <p>command = \"rsync -aW --progress %s@%s:%s/data/ ./data/ --delete\" %\\</p> <p>('pi', 'jackrpi04.local', '~/projects/d3')</p> <p>os.system(command)</p> <p>time.sleep(5)</p> <p>call the python code with</p> <p>python continous_data_rsync.py</p> <p>From the RPI to the GPU Cluster</p> <p>Since the GPU cluster can not see your RPI on the network,</p> <p>we need to RSYNC from the RPI to the GPU Cluster</p> <p>At the RPI create a file name continous_data_rsync.py</p> <p>nano continous_data_rsync.py</p> <p>import os</p> <p>import time</p> <p>while True:</p> <p>command = \"rsync -aW --progress %s@%s:%s/data/ ./data/ --delete\" %\\</p> <p>('pi', 'jackrpi04.local', '~/projects/d3')</p> <p>os.system(command)</p> <p>time.sleep(5)</p> <p>call the python code with</p> <p>python continous_data_rsync.py</p> <p>Continuous Train This command fires off the keras training in a mode where it will continuously look for new</p> <p>data at the end of every epoch.  </p> <p>Usage: donkey contrain [--tub=\\&lt;data_path&gt;] [--model=\\&lt;path to model&gt;] [--transfer=\\&lt;path to model&gt;] [--type=\\&lt;linear|categorical|rnn|imu|behavior|3d&gt;] [--aug]</p> <p>example</p> <p>training for a particular tub</p> <p>donkey contrain --tub=~/projects/d2t/data/tub_1_18-08-20 --model=20aug18_just_testing.h5</p> <p>training for all tub files</p> <p>donkey contrain --model=24aug18_just_testing.h5</p> <p>Continuous Training from a PC</p> <p>Youetter have a GPU for training, if not this will take a long time. It wont work.</p> <p>If you don\u2019t have a NVIDIA GPU on your PC with CUDA</p> <p>Here what I have done for continuous training using a PC</p> <p>For continuous training</p> <p>Read instructions and do the security keys exchange first</p> <p>After exchanging the security keys to not have to type the password on ssh and RSYNC</p> <p>Create a file name continous_data_rsync.py</p> <p>nano continous_data_rsync.py</p> <p>import os</p> <p>import time</p> <p>while True:</p> <p>command = \"rsync -aW --progress %s@%s:%s/data/ ./data/ --delete\" %\\</p> <p>('pi', 'jackrpi01.local', '~/projects/d3')</p> <p>os.system(command)</p> <p>time.sleep(10)</p> <p>To start the continuous RSYNC</p> <p>python continous_data_rsync.py</p> <p>Edit the config.py at your computer so the continuous training works</p> <p>Continuous Training sends models to the PI when there is new model created.</p> <p>No need to have an RSYNC for models.</p> <p>nano config.py</p> <p>pi information</p> <p>PI_USERNAME = \"pi\"</p> <p>PI_PASSWD = \"Your_RPI_PassWord_Here\"</p> <p>PI_HOSTNAME = \"jackrpi01.local\"</p> <p>PI_DONKEY_ROOT = \"d2t\"</p> <p>SEND_BEST_MODEL_TO_PI = True</p> <p>save the config.py</p> <p>Continuous Training</p> <p>training for all tub files</p> <p>python train.py --continuous --model=models/date_modelName.h5</p> <p>training for a particular tub</p> <p>python train.py --continuous --tub=~/projects/d2t_pmm01/data/tub_1_18-08-20 --model=20aug18_just_testing.h5</p> <p>If set on config.py continuous Training automatically sends the new model to the RPI.</p> <p>SEND_BEST_MODEL_TO_PI = True</p> <p>When using the UCSD\u2019s GPU Cluster, you will need an rsync for the models</p> <p>since the GPU Cluster can not see the RPI</p> <p>When using the GPU Cluster, rsync works from the RPI to the GPU Cluster</p> <p>rsync models to the RPI continuously from the models directory where the code was called from</p> <p>Create a file name continous_models_rsync.py</p> <p>nano continous_models_rsync.py</p> <p>import os</p> <p>import time</p> <p>while True:</p> <p>command = \"rsync -aW --progress ./models/ %s@%s:%s/models/ --delete\" %\\</p> <p>('pi', 'jackrpi01.local', '~/projects/d3')</p> <p>os.system(command)</p> <p>time.sleep(30)</p> <p>To start the continuous RSYNC</p> <p>python continous_models_rsync.py</p> <p>Issue the drive command with the model name.</p> <p>You need a new model name before issuing the command to drive with a mode.</p> <p>So lets create a blank model file. Use the same name for the model you will create with</p> <p>continuous training</p> <p>touch ~/projects/d3/24nov18_ucsd01.json</p> <p>Load the model_name.json because it loads faster than model_name.h5</p> <p>python manage.py drive --model=./models/24nov18_ucsd01.json</p> <p>Security Keys Exchange</p> <p>It can save you some time by allowing you log to the RPI without asking for the password,</p> <p>same for RSYNC</p> <p>Dont use this on mission critical computers</p> <p>At the PC</p> <p>Generate the Private and Public Keys</p> <p>Exchange security Keys</p> <p>cd ~</p> <p>sudo apt-get install rsync</p> <p>ssh-keygen -t rsa</p> <p>ubuntu</p> <p>cat .ssh/id_rsa.pub</p> <p>mac</p> <p>cat ~/.ssh/id_rsa.pub</p> <p>Copy the output of the command into the buffer (Ctrl-C or Command-C on a Mac)</p> <p>At the RPI</p> <p>cd ~</p> <p>mkdir .ssh</p> <p>Ignore this message if you get it</p> <p>&gt; mkdir: cannot create directory \u2018.ssh\u2019: File exists</p> <p>chmod 700 .ssh</p> <p>chown pi:pi .ssh</p> <p>nano .ssh/authorized_keys</p> <p>I was able to ssh to the RPI without asking for the password</p> <p>ex: ssh pi@jackrpi04.local</p> <p>Summary of commands after the robot built and software Installed</p> <p>http://docs.donkeycar.com/guide/get_driving/</p> <p>You will need and Access Point (AP) to connect to your RoboCar to be able to SSH to it and give the commands:</p> <p>For that we have few options:</p> <ul> <li> <p>Move the WiFi AP to a place close to the track. You will need a power   outlet there to power the AP</p> </li> <li> <p>Use your phone as an AP. You will need to add an entry in the RoboCar   WiFi configuration so it connects to your Phone AP. See instruction on   RPI WiFi configuration. Please make sure it has a lower priority than   the UCSDRoboCar AP. e.g., 30 and below. Moreover, you will need to   connect your computer to the same phone AP so you can SSH to your   RoboCar RPI.</p> </li> <li> <p>You can create a direct connection between your computer and the   RoboCar. Need to search the net for that. Please make sure the RoboCar   still connects to the AP UCSDRoboCar after you are done using the   direct connection with your phone.</p> </li> </ul> <p>If you follow the instructions in this document, the PS3Keypad/Joystick will be used by default to drive the RoboCar.</p> <p>Summary of commands after the installs are complete</p> <p>Jack\u2019s RPI</p> <p>On the PC</p> <p>Activate the virtual environment</p> <p>cd projects</p> <p>source ~/env/bin/activate</p> <p>ssh pi@jackrpi02.local</p> <p>On the RPI</p> <p>(env)pi@jackrpi02:~ \\$</p> <p>cd d2t</p> <p>python manage.py drive</p> <p>Drive the robot to collect data</p> <p>On the PC</p> <p>Get data from RPI</p> <p>(env) jack@virtlnx01:~/projects/d2t\\$</p> <p>rsync -a --progress pi@jackrpi02.local:~/projects/d3/data ~/projects/d2t</p> <p>(env) jack@virtlnx01:~/projects/d2t\\$</p> <p>ls data</p> <p>tub_1_17-10-12</p> <p>Train a model using all tubes in the data directory</p> <p>(env) jack@lnxmbp01:~/projects/d2t\\$</p> <p>python train.py --model=models/date_model_name.h5</p> <p>To use on particular tube</p> <p>python train.py --tub ~/projects/d2t/data/tub_1_18-01-07 --model=models/07jan18_coleman_tube1Test.h5</p> <p>To make an incremental training using a previous model</p> <p>python train.py --tub ~/projects/d2t/data/NAME_OF_NEW_TUBE --transfer=models/NAME_OF_PREVIOUS_MODEL.h5 --model=models/NAME_OF_NEW_MODEL.h5</p> <p>To clean-up tubs</p> <p>(env) jack@LnxWS1:~/projects/d2t\\$</p> <p>donkey tubclean data</p> <p>using donkey v2.2.0 ...</p> <p>Listening on 8886...</p> <p>Open a browser and type</p> <p>http://localhost:8886</p> <p></p> <p>You can clean-up your tub directories. Please make a backup of your data before you start to clean it up.</p> <p>On the mac if the training complains</p> <p>rm ~/projects/d2t/data/.DS_Store</p> <p>If it complains about docopt, install it again. And I did not change anything from the previous day. Go figure\u2026</p> <p>(env) jack@lnxmbp01:~/projects/d2\\$ pip list</p> <p>(env) jack@lnxmbp01:~/projects/d2\\$ pip install docopt</p> <p>Collecting docopt</p> <p>Installing collected packages: docopt</p> <p>Successfully installed docopt-0.6.2</p> <p>(env) jack@virtlnx01:~/projects/d2\\$</p> <p>python train.py --model=models/12oct17_ucsd_day1.h5</p> <p>See the models here</p> <p>(env) jack@virtlnx01:~/projects/d2\\$</p> <p>ls models</p> <p>ucsd_12oct17.h5</p> <p>Place Autopilot into RPI</p> <p>(env) jack@lnxmbp01:~/projects/d2\\$</p> <p>rsync -a --progress ~/projects/d2t/models/ pi@jackrpi02.local:~/projects/d3/models/</p> <p>(dk)pi@jackrpi02:~/d2 \\$</p> <p>ls models</p> <p>ucsd_12oct17.h5</p> <p>On the RPI</p> <p>Run AutoPilot at the RPI</p> <p>(dk)pi@jackrpi02:~/d2 \\$</p> <p>python manage.py drive --model=./models/ucsd_12oct17.h5</p> <p>...</p> <p>Using TensorFlow backend.</p> <p>loading config file: /home/pi/d2/config.py</p> <p>config loaded</p> <p>PiCamera loaded.. .warming camera</p> <p>Starting vehicle...</p> <p>/home/pi/env/lib/python3.4/site-packages/picamera/encoders.py:544: PiCameraResolutionRounded: frame size rounded up from 160x120 to 160x128</p> <p>width, height, fwidth, fheight)))</p> <p>\u2026</p> <p>Check Tub</p> <p>This command allows you to see how many records are contained in any/all tubs. It will also open each record and ensure that the data is readable and intact. If not, it will allow you to remove corrupt records.</p> <p>Usage:</p> <p>donkey tubcheck \\&lt;tub_path&gt; [--fix] </p> <ul> <li> <p>Run on the host computer or the robot</p> </li> <li> <p>It will print summary of record count and channels recorded for each   tub</p> </li> <li> <p>It will print the records that throw an exception while reading</p> </li> <li> <p>The optional --fix will delete records that   have problem</p> </li> </ul> <p>PS3 Controller Modes</p> <p>The default mode will be that User is in Control. That is, the user controls Steering and Throttle.</p> <p>To switch to Local Angle (software controls the Steering and user the Throttle), you need to press the \\&lt;Select&gt; button in the Joystick.</p> <p>If you give Throttle the Robocar should drive around semi-autonomously.</p> <p>After few laps that you see that your model is good,</p> <p>Please hold your robot with the wheels out of the floor</p> <p>you can press the \\&lt;Start&gt; button and immediately press the \\&lt;left_DOWN_arrow&gt; button few times to decrease the Throttle as needed. This is important so you slow down the Robocar for a constant Throttle.</p> <p>Press the \\&lt;left_UP_arrow&gt; to give it more Throttle as needed.</p> <p>Pressing \\&lt;X&gt; will stop the robocar and go back to User mode (user is in control)</p> <p>You can change the driving modes by pressing the \\&lt;Select&gt; button. You should be able to see a message on your computer terminal that is SSH connected to the RoboCar RPI.</p> <p>The Local &amp; Angle mode (fully autonomous) is to be used after you see that you can do few laps with local angle</p> <p>Hit Select button to toggle between three modes - User, Local Angle, and Local Throttle &amp; Angle.</p> <ul> <li> <p>User - User controls both steering and throttle with joystick</p> </li> <li> <p>Local Angle - Ai controls steering. User controls throttle.</p> </li> <li> <p>Local Throttle &amp; Angle - Ai controls both steering and throttle</p> </li> </ul> <p>When the car is in Local Angle mode, the NN will steer. You must provide throttle...</p> <p>Ideally you will have ~ 60 laps</p> <p>If you don\u2019t have a good working Auto-Pilot, get more data in 10 laps increments.</p> <p>In summary, you may want to start with 60 laps and then do 10~20 laps more to see if the model gets better.</p> <p>I would not worry much about few bad spots when collecting data. Drive the car back to the track,</p> <p>then press Green_Triangle to delete the last 5s of data.</p> <p>Keep driving, you will develop good skills, you will get good data and better models. If you leave the track, just drive the RoboCar back to track. It may even learn how to get back to track.</p> <p>If you keep the data from the same track (ex: UCSD Track) in the d2t/data directory, as you add more files to it (e.g., tub_5_17-10-13) it will help your model. At the same time it will take more time to train since your model will read all the data sets in the directory. You can use transfer model to add new data to a current model.</p> <p>Incremental training using a previous model</p> <p>python train.py --tub ~/projects/d2t/data/NAME_OF_NEW_TUBE_DATA --transfer=models/NAME_OF_PREVIOUS_MODEL.h5 --model=models/NAME_OF_NEW_MODEL.h5</p> <p>Some Advanced Tools</p> <p>The visualization tool is to be use on your PC. Please even if you can, please do not use the GPU Cluster Resources for this.</p> <p>Visualizing the model driving the car vs. human driver</p> <p>Install OpenCV</p> <p>sudo apt-get install python-opencv</p> <p>pip3 install opencv-python</p> <p>donkey makemovie --tub=data\\tub_file --model=models\\model_name.h5 --limit=100 --salient --scale=2</p> <p>example</p> <p>donkey makemovie --tub=data/tub_9_19-01-19 --model=models/19jan19_oakland_5.h5 --start 1 --end 1000 --salient --scale=2</p> <p>Here are installs with very limited or no support in this course.</p> <p>There are too many computers and OS variations to support...</p> <p>28Aug20</p> <p>The installation on Linux(ubuntu) is much easier now using Conda</p> <p>See docs.donkeycar.com</p> <p>https://docs.donkeycar.com/guide/host_pc/setup_ubuntu/</p> <p>I will stop maintaining the installing of the donkeycar into the host computers.</p> <p>Ubuntu 18.04 - Tensorflow-GPU - CUDA Install - NVIDIA GPUs with CUDA Cores</p> <p>If your computer has a NVIDIA GPU with CUDA cores, you can take advantage of the GPU to</p> <p>accelerate the AI training. Based on the class experience, results varies between</p> <p>3~10 times faster when comparing to CPU training</p> <p>Installing CUDA related files and Tensorflow GPU</p> <p>If you want to try the easier way but not the latest files, we have a compressed Zip file</p> <p>with the necessary files for Linux Ubuntu 64 bits</p> <p>and Tensorflow 1.12 - GPU (latest version on Feb 2019) - cudnn-10.0-linux-x64-v7.4.1.5,</p> <p>libcudnn7_7.4.1.5-1+cuda10.0_amd64</p> <p>Tensorflow1.12 with GPU CUDA10 cuddnn7.4.2.24 Compute Capabilties_3.0_5.2_6.1_7.0 for Linux</p> <p>The link below has the Tensorflow and CUDA supporting files for Linux Ubuntu</p> <p>/* </p> <p>*/</p> <p>https://drive.google.com/open?id=1xfbn_qy77SjXqpfWQWwti5JFWWtJVjIM</p> <p>Alternatively, you can download the files from the NVIDIA site as long as you use the same versions</p> <p>for cudnn and others libraries</p> <p>Or build Tensorflow from source with the version of the CUDA files you have installed.</p> <p>Building Tensorflow from source can take a few hours even on modern I7s CPU with SSD disk and</p> <p>lots of RAM...</p> <p>Skip these downloads if you are using the files included in the Zipped file listed above.</p> <p>Otherwise you can get the files from the NVIDIA site</p> <p>Download http://developer.nvidia.com/cuda-downloads</p> <p>Download Installer for Linux Ubuntu 18.04 x86_64</p> <p>The base installer is available for download below.</p> <p>As of 17Apr20</p> <p>https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1804&amp;target_type=deblocal</p> <p>At the directory you download files to install</p> <p>mkdir cuda</p> <p>mkdir 10.2</p> <p>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin</p> <p>sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600</p> <p>wget http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb</p> <p>sudo dpkg -i cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb</p> <p>sudo apt-key add /var/cuda-repo-10-2-local-10.2.89-440.33.01/7fa2af80.pub</p> <p>sudo apt-get update</p> <p>sudo apt-get -y install cuda</p> <p>09Aug20</p> <p>https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=2004&amp;target_type=deblocal</p> <p>28Aug20</p> <p>https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=2004&amp;target_type=deblocal</p> <p>CUDA 11</p> <p></p> <p>Base Installer</p> <p>Installation Instructions:</p> <p>cd projects</p> <p>mkdir cuda</p> <p>cd cuda</p> <p>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin</p> <p>sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600</p> <p>wget https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda-repo-ubuntu2004-11-0-local_11.0.3-450.51.06-1_amd64.deb</p> <p>sudo dpkg -i cuda-repo-ubuntu2004-11-0-local_11.0.3-450.51.06-1_amd64.deb</p> <p>sudo apt-key add /var/cuda-repo-ubuntu2004-11-0-local/7fa2af80.pub</p> <p>sudo apt-get update</p> <p>sudo apt-get -y install cuda</p> <p>echo 'export PATH=/usr/local/cuda/bin\\${PATH:+:\\${PATH}}' &gt;&gt; ~/.bashrc</p> <p>reboot the machine</p> <p>As 28aug20 nvidia driver 450.51.06 is the latest</p> <p>After rebooted, nvcc-V and nvidia-smi worked</p> <p>nvidia-smi</p> <p></p> <p></p> <p>https://developer.nvidia.com/rdp/cudnn-download</p> <p></p> <p>download both these files</p> <p>cuDNN Library for Linux (x86_64)</p> <p>Extract cudnn-11.0-linux-x64-v8.0.2.39.tgz</p> <p>Make sure you are in the directory where you downloaded the files</p> <p>tar -xf cudnn-11.0-linux-x64-v8.0.2.39.tgz</p> <p>cd cudnn-11.0-linux-x64-v8.0.2.39</p> <p>We have cuda 11.0 at</p> <p>/usr/local/cuda-11.0</p> <p>sudo cp cuda/include/cudnn.h /usr/local/cuda/include/</p> <p>sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/</p> <p>sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*</p> <p>sudo cp -R cuda/include/* /usr/local/cuda-11.0/include</p> <p>sudo cp -R cuda/lib64/* /usr/local/cuda-11.0/lib64</p> <p>Install libcudnn8_8.0.2.39-1+cuda11.0_amd64.deb</p> <p>sudo dpkg -i libcudnn8_8.0.2.39-1+cuda11.0_amd64.deb</p> <p>Install tensorflow with GPU support</p> <p>For a ubuntu server lets make tensor available without virtual env.</p> <p>For workstations and donkeycar, see below for virtualenv install</p> <p>sudo apt install python3-pip</p> <p>pip3 install --upgrade tensorflow-gpu</p> <p>Lets test the tensorflow-gpu install</p> <p>python3</p> <p>import tensorflow as tf</p> <p>tf.config.list_physical_devices('GPU')</p> <p>exit()</p> <p>Optional</p> <p>Let\u2019s install NCCL. NCCL is NVIDIA optimization for multi-GPU use.</p> <p>Ex: Use GPU in the computer in one external such as eGPU.</p> <p>https://developer.nvidia.com/nccl/nccl-download</p> <p></p> <p>Download and install NCCL using Gdeb package install</p> <p>O/S agnostic local installer</p> <p>cd to the download directory</p> <p>cd ~/Downloads/cuda/10.1</p> <p>tar -xf nccl_2.4.8-1+cuda10.1_x86_64.txz</p> <p>cd nccl_2.4.8-1+cuda10.1_x86_64</p> <p>sudo cp -R * /usr/local/cuda-10.1/targets/x86_64-linux/</p> <p>sudo ldconfig</p> <p>Based on the NVIDIA video driver and CUDA that I installed</p> <p>Pay attention to the version listed ex: CUDA 10.2</p> <p>Adapt 10.1 to 10.2 or whatever version you installed</p> <p>nvcc -V</p> <p>Command 'nvcc' not found, but can be installed with:</p> <p>sudo apt install nvidia-cuda-toolkit</p> <p>This line should make nvcc -V work</p> <p>echo 'export PATH=/usr/local/cuda/bin\\${PATH:+:\\${PATH}}' &gt;&gt; ~/.bashrc</p> <p>adminlnx@lnxsrv6:~\\$ nvcc -V</p> <p>nvcc: NVIDIA (R) Cuda compiler driver</p> <p>Copyright (c) 2005-2020 NVIDIA Corporation</p> <p>Built on Wed_Jul_22_19:09:09_PDT_2020</p> <p>Cuda compilation tools, release 11.0, V11.0.221</p> <p>Build cuda_11.0_bu.TC445_37.28845127_0</p> <p>adminlnx@lnxsrv6:~\\$</p> <p>Double checking</p> <p>adminlnx@lnxsrv6:~\\$ cd Downloads</p> <p>adminlnx@lnxsrv6:~/Downloads\\$ cd cuda</p> <p>adminlnx@lnxsrv6:~/Downloads/cuda\\$ cd 11</p> <p>adminlnx@lnxsrv6:~/Downloads/cuda/11\\$ sudo apt-get -y install cuda</p> <p>[sudo] password for adminlnx:</p> <p>Reading package lists... Done</p> <p>Building dependency tree</p> <p>Reading state information... Done</p> <p>cuda is already the newest version (11.0.3-1).</p> <p>0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.</p> <p>We have the latest version installed...</p> <p>We will install the nvidia-cuda-toolkit next</p> <p>Easier way? But older cuDNN</p> <p>https://linuxconfig.org/how-to-install-cuda-on-ubuntu-20-04-focal-fossa-linux</p> <p>Although you might not end up with the latest CUDA toolkit version, the easiest way to install CUDA on Ubuntu 20.04 is to perform the installation from Ubuntu's standard repositories.</p> <p>To install CUDA execute the following commands:</p> <p>\\$ sudo apt update</p> <p>\\$ sudo apt install nvidia-cuda-toolkit</p> <p></p> <p>All should be ready now. Check your CUDA version:</p> <p>If not, try this</p> <p>echo 'export PATH=/usr/local/cuda/bin\\${PATH:+:\\${PATH}}' &gt;&gt; ~/.bashrc</p> <p>\\$ nvcc --version</p> <p>nvcc: NVIDIA (R) Cuda compiler driver</p> <p>Copyright (c) 2005-2019 NVIDIA Corporation</p> <p>Built on Sun_Jul_28_19:07:16_PDT_2019</p> <p>Cuda compilation tools, release 10.1, V10.1.243</p> <p>As 09Ag20, 10.1 is not the latest version of CUDA toolkit.</p> <p>Let me install it manually</p> <p>https://developer.nvidia.com/rdp/cudnn-download</p> <p>As of 23Apr20</p> <p>cuDNN7.6.5</p> <p>Adjust instructions accordingly</p> <p></p> <p>Download cuDNN Runtime Library for Ubuntu18.04 (Deb)</p> <p>libcudnn7_7.6.2.24-1+cuda10.1_amd64.deb</p> <p>Download cuDNN v7.5.1 for CUDA 10.1</p> <p>cuDNN Runtime Library for Ubuntu18.04 (Deb)</p> <p>get Gdebi package and install libcudnn using Gdebi</p> <p>install libcudnn7_7.6.2.24-1+cuda10.1_amd64.deb</p> <p>Type this to add the CUDA 10.1 to the path</p> <p>export PATH=/usr/local/cuda-10.1/bin\\${PATH:+:\\${PATH}}</p> <p>export PATH=/usr/local/cuda-10.2/bin\\${PATH:+:\\${PATH}}</p> <p>Then nvcc -V worked</p> <p></p> <p>note release 10.1, adjust for 10.2 or other version you may be installing</p> <p>Now that it works, let's add the path to be persistent add at the end of the file ~./profile nano ~/.profile</p> <p>Add the PATH to include cuda-10.1 ...</p> <p>set PATH so it includes user's private bin if it exists</p> <p>if [ -d \"\\$HOME/.local/bin\" ] ; then</p> <p>PATH=\"\\$HOME/.local/bin:\\$PATH\"</p> <p>fi</p> <p>PATH=/usr/local/cuda-10.2/bin\\${PATH:+:\\${PATH}}</p> <p>reboot to test nvidia-smi and nvcc -V</p> <p>when installing CUDA it seems it reverted my NVIDIA driver to 4.10</p> <p>let me install version 430 (the latest as 11May19), I used the</p> <p>Ubuntu software update / additional drivers - then I tried again nvidia-smi and nvcc -V</p> <p></p> <p></p> <p>https://developer.nvidia.com/rdp/cudnn-download</p> <p></p> <p>cudnn-10.1-linux-x64-v7.6.2.24.tgz</p> <p>cudnn-10.2-linux-x64-v7.6.5.32.tgz</p> <p>Make sure you are in the directory where you downloaded the files</p> <p>tar -xf cudnn-10.1-linux-x64-v7.6.2.24.tgz</p> <p>for cudnn 10.2</p> <p>tar -xf cudnn-10.2-linux-x64-v7.6.5.32.tgz</p> <p>sudo cp -R cuda/include/* /usr/local/cuda-10.2/include sudo cp -R cuda/lib64/* /usr/local/cuda-10.2/lib64</p> <p>Optional</p> <p>Let\u2019s install NCCL. NCCL is NVIDIA optimization for multi-GPU use.</p> <p>Ex: Use GPU in the computer in one external such as eGPU.</p> <p>https://developer.nvidia.com/nccl/nccl-download</p> <p></p> <p>Download and install NCCL using Gdeb package install</p> <p>O/S agnostic local installer</p> <p>cd to the download directory</p> <p>cd ~/Downloads/cuda/10.1</p> <p>tar -xf nccl_2.4.8-1+cuda10.1_x86_64.txz</p> <p>cd nccl_2.4.8-1+cuda10.1_x86_64</p> <p>sudo cp -R * /usr/local/cuda-10.1/targets/x86_64-linux/</p> <p>sudo ldconfig</p> <p>29Aug20</p> <p>TensorFlow 2.3 on Ubuntu 20.04 LTS with CUDA 11.0 and CUDNN 8.0</p> <p>https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03</p> <p>https://medium.com/@cwbernards/tensorflow-2-3-on-ubuntu-20-04-lts-with-cuda-11-0-and-cudnn-8-0-fb136a829e7f</p> <p>After installing NVIDIA drivers and cuDNN or here in this same document I have examples</p> <p>Please refer to my instructions here.</p> <p>Now lets install Tensorflow</p> <p>If you don\u2019t have the python3 virtual environment, create one</p> <p>I am using ~/projects/envs/env1</p> <p>sudo apt-get update</p> <p>sudo apt-get install virtualenv</p> <p>virtualenv --system-site-packages -p python3 ~/projects/envs/env1</p> <p>Activate your virtual environment</p> <p>source ~/projects/envs/env1/bin/activate</p> <p>(env) jack@lnxmbp01:~/projects\\$</p> <p>easy_install -U pip</p> <p>To install the latest version of tensorflow with GPU support, you can use</p> <p>pip3 install --upgrade tensorflow-gpu</p> <p>The version on the Zipped file is already old. Left available for testing only</p> <p>(if you have new CPUs like Intel I5 and I7), you can use the tensorflow file included</p> <p>in the downloaded Zip file earlier in these instructions.</p> <p></p> <p>tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl</p> <p>Or alternatively if you believe the released tensorflow use the same CUDA install you have</p> <p>running in your computer, you can use</p> <p>pip3 install --upgrade tensorflow-gpu</p> <p>Note on using the latest CUDA version. The packages available at the standard repository may</p> <p>not work with the latest CUDA such as CUDA 10</p> <p>You need to download Tensorflow that was built for the version of CUDA and relate</p> <p>software you installed</p> <p>You can install a particular version of Tensorflow such as 1.4</p> <p>pip3 install --upgrade tensorflow-gpu==1.4</p> <p>or 1.3.1</p> <p>pip3 install --upgrade tensorflow-gpu==1.3.0</p> <p>To install a particular Tensorflow such as the *.whl file included at the Zipped</p> <p>file you have downloaded</p> <p>use the Tensorflow file name you downloaded</p> <p>pip3 install --upgrade tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl</p> <p>you need to be in the same directory where the file is or specify the complete path.</p> <p>I usually copy over the tensorflow .whl file to my projects directory and install it from there.</p> <p>After a successful install</p> <p>Run a short Python program to test the Tensorflow install</p> <p>(env) jack@lnxmbp01:~/projects\\$</p> <p>python</p> <p>Enter the following txt, you can cut and paste</p> <p>Python</p> <p>import tensorflow as tf</p> <p>hello = tf.constant('Hello, TensorFlow!')</p> <p>sess = tf.Session()</p> <p>print(sess.run(hello))</p> <p>It worked.</p> <p>2018-11-28 03:33:42.219251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 913 MB memory) -&gt; physical GPU (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0, compute capability: 3.0) &gt;&gt;&gt; print(sess.run(hello)) b'Hello, TensorFlow!' &gt;&gt;&gt; </p> <p>end of the Tesnforflow GPU install on Linux Ubuntu</p> <p>Some previous Instructions and for older GPUs</p> <p>How to install Tenforflow with GPU support</p> <p>Note the instructions below are for an older NVIDIA GPUs. It will work for modern NVIDIA GPUs</p> <p>but it wont take advantage of the new features and all its performance</p> <p>You can adapt the instructions to have the latest CUDA, cuDNN, and latest Tensorflow with GPU</p> <p>support.</p> <p>On some Macbook Pro (~2013) you may have NVIDIA dGPU (dedicated GPU).</p> <p>It may require CUDA 8</p> <p>Make sure you have the NVIDIA driver installed</p> <p></p> <p></p> <p>Download the CUDA files from here</p> <p>https://developer.nvidia.com/cuda-toolkit-archive</p> <p></p> <p>Download cuDNN v6.0 for CUDA 8 from here</p> <p>https://developer.nvidia.com/rdp/cudnn-archive</p> <p></p> <p></p> <p></p> <p>Open a terminal and navigate to where you saved the files</p> <p>Then issue the following commands</p> <p>sudo apt-get update</p> <p>sudo apt-get install gdebi</p> <p>sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb</p> <p>sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-cublas-performance-update_8.0.61-1_amd64.deb</p> <p>sudo dpkg -i libcudnn6_6.0.21-1+cuda8.0_amd64.deb</p> <p>sudo apt-get update</p> <p>sudo apt-get install cuda</p> <p>Let it install. It may take a while.</p> <p>\u2026</p> <p>Setting up cuda-toolkit-8-0 (8.0.61-1) ...</p> <p>Setting up cuda-drivers (375.26-1) ...</p> <p>Setting up cuda-runtime-8-0 (8.0.61-1) ...</p> <p>Setting up cuda-demo-suite-8-0 (8.0.61-1) ...</p> <p>Setting up cuda-8-0 (8.0.61-1) ...</p> <p>Setting up cuda (8.0.61-1) ...</p> <p>Processing triggers for initramfs-tools (0.130ubuntu3.5) ...</p> <p>update-initramfs: Generating /boot/initrd.img-4.15.0-36-generic</p> <p>Processing triggers for libc-bin (2.27-3ubuntu1) ...</p> <p>\u2026.</p> <p>Lets test the install</p> <p>nvcc --version</p> <p>Command 'nvcc' not found, but can be installed with:</p> <p>export PATH=/usr/local/cuda-8.0/bin\\${PATH:+:\\${PATH}}</p> <p>nvcc --version</p> <p>nvcc: NVIDIA (R) Cuda compiler driver</p> <p>Copyright (c) 2005-2016 NVIDIA Corporation</p> <p>Built on Tue_Jan_10_13:22:03_CST_2017</p> <p>Cuda compilation tools, release 8.0, V8.0.61</p> <p>nano ~/.profile</p> <p>Add this line to the end of the file</p> <p>export PATH=/usr/local/cuda-8.0/bin\\${PATH:+:\\${PATH}}</p> <p>sudo reboot now</p> <p>Open a terminal and run again</p> <p>nvcc --version</p> <p>nvidia-smi</p> <p></p> <p>Now lets install Tensorflow-GPU and test it</p> <p>sudo apt-get update</p> <p>sudo apt-get install virtualenv build-essential python3-dev gfortran libhdf5-dev libatlas-base-dev virtualenv env -p python3 source env/bin/activate pip install keras==2.2.2</p> <p>pip install tensorflow-gpu==1.3.0</p> <p>Lets Test Tensorflow python</p> <p>Enter the following lines, it is a short program, inside the python interactive shell:</p> <p>Python import tensorflow as tf hello = tf.constant('Hello, TensorFlow!') sess = tf.Session() print(sess.run(hello))</p> <p></p> <p>It worked. Note that Tensorflow can see the dGPU</p> <p>Also try this when inquiring about the Tensorflow version you have installed</p> <p>pip3 show tensorflow</p> <p>Install CUDA to enable GPU Support on Ubuntu 16.04 - Not supported in this course</p> <p>http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.htmlpost-installation-actions</p> <p>Installing CUDA on Ubuntu 16.04 to enable using GPU computation such as TensorFlow.</p> <p>First I installed the latest NVIDIA Driver from the Ubuntu setup 3rd party driver</p> <p>System Settings/Software &amp; Updates/Additional Drivers</p> <p>NVIDIA xxx Proprietary Tested</p> <p>CUDA Install on Ubuntu 16.04</p> <p>Note: Initially I installed the latest CUDA (9), I had to revert to CUDA 8 install because Tensorflow still looks for it.</p> <p>In the future it may work with CUDA 9</p> <p>Installing CUDA on Ubuntu 16.04 to enable using GPU computation such as TensorFlow.</p> <p>First I installed the latest NVIDIA Driver from the Ubuntu setup 3rd party driver</p> <p>System Settings/Software &amp; Updates/Additional Drivers</p> <p>NVIDIA xxx Proprietary Tested</p> <p>To remove other CUDA and NVIDIA installs</p> <p>https://devtalk.nvidia.com/default/topic/903867</p> <p>sudo apt-get remove --purge nvidia-*</p> <p>After this command, you need to enable the 3rd party video driver again</p> <p>If needed, remove the repository that may be installing CUDA 9.</p> <p>https://askubuntu.com/questions/43345/how-to-remove-a-repository</p> <p>Download Installers for Linux Ubuntu 16.04 x86_64</p> <p>https://developer.nvidia.com/cuda-80-ga2-download-archive</p> <p>Get the base installer and patch</p> <p>Get CUDA 8 (because Tensorflow was/is not compatible with CUDA 9 yet)</p> <p>jack@lnxmbp01:~/Downloads/NVIDIA\\$</p> <p>sudo apt-get update</p> <p>sudo apt-get install gdebi</p> <p>sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb</p> <p>sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-cublas-performance-update_8.0.61-1_amd64.deb</p> <p>jack@lnxmbp01:~/Downloads/NVIDIA\\$ sudo apt-get update</p> <p>jack@lnxmbp01:~/Downloads/NVIDIA\\$ sudo apt-get install cuda</p> <p>Let it install. It may take a while.</p> <p>jack@lnxmbp01:~\\$ nvcc --version</p> <p>The program 'nvcc' is currently not installed. You can install it by typing:</p> <p>sudo apt install nvidia-cuda-toolkit</p> <p>Cuda8</p> <p>jack@lnxmbp01:~\\$ export PATH=/usr/local/cuda-8.0/bin\\${PATH:+:\\${PATH}}</p> <p>jack@lnxmbp01:~\\$ nvcc -V</p> <p>nvcc: NVIDIA (R) Cuda compiler driver</p> <p>Copyright (c) 2005-2016 NVIDIA Corporation</p> <p>Built on Tue_Jan_10_13:22:03_CST_2017</p> <p>Cuda compilation tools, release 8.0, V8.0.61</p> <p>If you see an error with libcudnn, check versions. CUDA and Tensorflow need to play nice together with particular versions specially for older GPUs.</p> <p>ImportError: libcudnn.so.6: cannot open shared object file: No such file or directory Fixed it! When I installed CUDA 8, I originally I had cudnn7 I had to download cudnn6 from here https://developer.nvidia.com/rdp/cudnn-downloada-collapse6-8</p> <p>Download cuDNN v6.0 (April 27, 2017), for CUDA 8.0://developer.nvidia.com/rdp/cudnn-download Need to install cuDNN V6.</p> <p>Verify CUDA installation</p> <p>Reboot, if not the nvidia-smi command may not work</p> <p>jack@lnxmbp01:~\\$ nvcc --version</p> <p>The program 'nvcc' is currently not installed. You can install it by typing:</p> <p>sudo apt install nvidia-cuda-toolkit</p> <p>jack@lnxmbp01:~\\$ export PATH=/usr/local/cuda-9.0/bin\\${PATH:+:\\${PATH}}</p> <p>jack@lnxmbp01:~\\$ nvcc --version</p> <p>nvcc: NVIDIA (R) Cuda compiler driver</p> <p>Copyright (c) 2005-2017 NVIDIA Corporation</p> <p>Built on Fri_Sep__1_21:08:03_CDT_2017</p> <p>Cuda compilation tools, release 9.0, V9.0.176</p> <p>jack@lnxmbp01:~\\$ nvidia-smi</p> <p>Sat Sep 30 13:22:52 2017</p> <p>+-----------------------------------------------------------------------------+</p> <p>| NVIDIA-SMI 384.81 Driver Version: 384.81 |</p> <p>|-------------------------------+----------------------+----------------------+</p> <p>| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |</p> <p>| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |</p> <p>|===============================+======================+======================|</p> <p>| 0 GeForce GT 750M Off | 00000000:01:00.0 N/A | N/A |</p> <p>| N/A 67C P0 N/A / N/A | 403MiB / 1998MiB | N/A Default |</p> <p>+-------------------------------+----------------------+----------------------+</p> <p>+-----------------------------------------------------------------------------+</p> <p>| Processes: GPU Memory |</p> <p>| GPU PID Type Process name Usage |</p> <p>|=============================================================================|</p> <p>| 0 Not Supported |</p> <p>+-----------------------------------------------------------------------------+</p> <p>For CUDA 9.2</p> <p>jack@lnx01:~\\$ nvcc -V</p> <p>Command 'nvcc' not found, but can be installed with:</p> <p>sudo apt install nvidia-cuda-toolkit</p> <p>jack@lnx01:~\\$ export PATH=/usr/local/cuda-9.2/bin\\${PATH:+:\\${PATH}}</p> <p>jack@lnx01:~\\$ nvcc -V</p> <p>nvcc: NVIDIA (R) Cuda compiler driver</p> <p>Copyright (c) 2005-2018 NVIDIA Corporation</p> <p>Built on Tue_Jun_12_23:07:04_CDT_2018</p> <p>Cuda compilation tools, release 9.2, V9.2.148</p> <p>Now that ii works, lets add the path to be persistent</p> <p>jack@lnxmbp01:~\\$ nano ~/.profile</p> <p>..</p> <p>set PATH so it includes user's private bin directories</p> <p>PATH=\"\\$HOME/bin:\\$HOME/.local/bin:\\$PATH\"</p> <p>PATH=\"/usr/local/cuda-8.0/bin\\${PATH:+:\\${PATH}}\"</p> <p>PATH=/usr/local/cuda-9.2/bin\\${PATH:+:\\${PATH}}</p> <p>Installing TensorFlow with GPU Support on Ubuntu 16.04- not supported in this course</p> <p>CUDA needs to be installed and tested first - search for Install CUDA in this document.</p> <p>https://www.tensorflow.org/install/install_linuxdetermine_which_tensorflow_to_install</p> <p>https://www.tensorflow.org/install/install_linuxInstallingVirtualenv</p> <p>https://github.com/mind/wheels/releases/</p> <p>sudo apt-get update</p> <p>sudo apt-get upgrade</p> <p>sudo apt-get install virtualenv</p> <p>mkdir projects</p> <p>jack@lnxmbp01:~/projects\\$</p> <p>virtualenv --system-site-packages -p python3 ~/projects/env</p> <p>jack@lnxmbp01:~/projects\\$</p> <p>source ~/projects/env/bin/activate</p> <p>(env) jack@lnxmbp01:~/projects\\$</p> <p>(env) jack@lnxmbp01:~/projects\\$</p> <p>easy_install -U pip</p> <p>(env) jack@lnxmbp01:~/projects\\$</p> <p>pip3 install --upgrade tensorflow-gpu</p> <p>Successfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.6.9 numpy-1.13.3 protobuf-3.4.0 six-1.11.0 tensorflow-gpu-1.3.0 tensorflow-tensorboard-0.1.8 werkzeug-0.12.2</p> <p>To install a particular version of tensorflow, example</p> <p>pip3 install --upgrade tensorflow-gpu==1.4</p> <p>(env) jack@lnxmbp01:~/projects\\$</p> <p>pip3 install --upgrade tensorflow-gpu==1.3.0</p> <p>Run a short TensorFlow program to test it</p> <p>(env) jack@lnxmbp01:~/projects\\$</p> <p>python</p> <p>Python 3.5.2 (default, Sep 14 2017, 22:51:06)</p> <p>[GCC 5.4.0 20160609] on linux</p> <p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p> <p>&gt;&gt;&gt;</p> <p>Enter the following txt, you can cut and paste</p> <p>Python</p> <p>import tensorflow as tf</p> <p>hello = tf.constant('Hello, TensorFlow!')</p> <p>sess = tf.Session()</p> <p>print(sess.run(hello))</p> <p>The result should have this at the end.</p> <p>...</p> <p>&gt;&gt;&gt; print(sess.run(hello))</p> <p>b'Hello, TensorFlow!'</p> <p>&gt;&gt;&gt;</p> <p>Also try this when inquiring about the Tensorflow version you have installed</p> <p>pip3 show tensorflow</p> <p>Install TensorFlow on MacOS - not supported in this course</p> <p>From Tawn Kramer Instructions</p> <p>Install miniconda Python 3.6 64 bit</p> <p>https://conda.io/docs/user-guide/tasks/manage-python.html</p> <p>https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/20/conda/</p> <p>https://conda.io/docs/commands.htmlconda-general-commands</p> <p>Install git 64 bit </p> <p>Start Terminal  </p> <p>cd ~ mkdir projects cd projects</p> <p>Download the latest version of miniconda for macos 64 bits from https://conda.io/miniconda.html</p> <p>Save the file on your projects directory</p> <p>bash Miniconda3-latest-MacOSX-x86_64.sh -u</p> <p>As of 27Oct18, the default Python to be installed will be 3.7</p> <p>To revert conda to use python 3.6</p> <p>conda install python=3.6</p> <p>If you need to create a virtual environment with Python 3.6, that is required for many of the</p> <p>TensorFlow for MacOS at the moment. The command below creates an environment called py36</p> <p>conda create -n py36 python=3.6 anaconda</p> <p>Activate the environment</p> <p>source activate py36</p> <p>Deactivate the environment</p> <p>source deactivate</p> <p>Get the latest donkey from Github. git clone https://github.com/tawnkramer/donkey cd donkey</p> <p>Create the Python anaconda environment conda env create -f envs/mac.yml</p> <p>Activate the virtual environment</p> <p>source activate donkey</p> <p>To deactivate the virtual environment</p> <p>source deactivate</p> <p>To delete a virtual environment</p> <p>conda remove -n your_env_name --all --verbose</p> <p>Install Tensorflow pip install tensorflow</p> <p>pip3 show tensorflow</p> <p>python</p> <p>Enter the following txt, you can cut and paste</p> <p>Python import tensorflow as tf hello = tf.constant('Hello, TensorFlow!') sess = tf.Session() print(sess.run(hello))  </p> <p>The result should have this at the end. ... &gt;&gt;&gt; print(sess.run(hello))  </p> <p>b'Hello, TensorFlow!' &gt;&gt;&gt;</p> <p>Install donkey source and create your local working dir pip install -e .[pc] donkey createcar --path ~/projects/d2t</p> <p>Note: After closing the Terminal, when you open it again</p> <p>you will need to type source activate donkey to re-enable the mappings to</p> <p>donkey specific Python libraries</p> <p>2nd Method for installation on MacOS</p> <p>http://exponential.io/blog/2015/02/11/install-python-on-mac-os-x-for-development/</p> <p>cd curl -O https://raw.githubusercontent.com/Homebrew/install/master/install ruby install rm install</p> <p>\\$ brew install python</p> <p>\\$ brew install python3</p> <p>\\$ brew unlink python &amp;&amp; brew link python</p> <p>\\$ sudo pip install --upgrade pip</p> <p>\\$ sudo pip install virtualenv \\$ mkdir projects</p> <p>\\$ cd projects</p> <p>\\$ virtualenv --system-site-packages -p python3 ~/projects/env</p> <p>\\$ source ~/projects/env/bin/activate</p> <p>(env) Jacks-MBP:projects jack\\$ easy_install -U pip</p> <p>(env) Jacks-MBP:projects jack\\$ pip3 install --upgrade tensorflow</p> <p>\u2026</p> <p>Successfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.6.9 numpy-1.13.3 protobuf-3.4.0 six-1.11.0 tensorflow-1.3.0 tensorflow-tensorboard-0.1.8 werkzeug-0.12.2</p> <p>\u2026</p> <p>Run a short TensorFlow program to test it</p> <p>(env) Jacks-MBP:projects jack\\$ python</p> <p>Python 3.6.3 (default, Oct 4 2017, 06:09:38)</p> <p>[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.37)] on darwin</p> <p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p> <p>&gt;&gt;&gt;</p> <p>Enter the following txt, you can cut and paste</p> <p>Python</p> <p>import tensorflow as tf</p> <p>hello = tf.constant('Hello, TensorFlow!')</p> <p>sess = tf.Session()</p> <p>print(sess.run(hello))</p> <p>The output should be</p> <p>&gt;&gt;&gt; Python</p> <p>... import tensorflow as tf</p> <p>&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!')</p> <p>&gt;&gt;&gt; sess = tf.Session()</p> <p>2017-10-13 18:48:46.858423: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.</p> <p>2017-10-13 18:48:46.858440: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</p> <p>2017-10-13 18:48:46.858455: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.</p> <p>2017-10-13 18:48:46.858459: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</p> <p>&gt;&gt;&gt; print(sess.run(hello))</p> <p>b'Hello, TensorFlow!'</p> <p>&gt;&gt;&gt;</p> <p>Ctr-D gets out of the Python Tensorflow test.</p> <p>Install TensorFlow on MacOS with GPU (NVIDIA CUDA)</p> <p>not supported in this course</p> <p>After you have all CUDA configuration working, you may want to use the version of Tensorflow or compile it from source. Lots of work ...</p> <p>pip3 install https://storage.googleapis.com/74thopen/tensorflow_osx/tensorflow-1.8.0-cp36-cp36m-macosx_10_13_x86_64.whl</p> <p>To compile from source</p> <p>https://github.com/zylo117/tensorflow-gpu-macosx</p> <p>https://docs.bazel.build/versions/master/install-os-x.htmlinstall-with-installer-mac-os-x</p> <p>https://storage.googleapis.com/74thopen/tensorflow_osx/index.html</p> <p>1.INSTALL NVIDIA DRIVER  </p> <p>2.INSTALL NVIDIA CUDA TOOLKIT (9.1 OR LATER)  </p> <p>3.INSTALL NVIDIA CUDA CUDNN (7.0 OR LATER)  </p> <p>4.SET UP CUDA ENVIRONMENT (MAKE SURE</p> <p>nvcc -V</p> <p>WORKS AND PRINTS CUDA VERSION)</p> <p>5.INSTALL XCODE/COMMAND LINE TOOL 9.3+</p> <p>6.INSTALL HOMEBREW</p> <p>7.INSTALL COREUTILS USING</p> <p>brew install coreutils</p> <p>brew install llvm</p> <p>brew install cliutils/apple/libomp</p> <p>download bazel</p> <p>https://github.com/bazelbuild/bazel/releases</p> <p>./bazel-0.18.0-installer-darwin-x86_64.sh --user</p> <p>add /Users/user_name/bin</p> <p>sudo nano /etc/paths</p> <p>/usr/local/bin</p> <p>/usr/bin</p> <p>/bin</p> <p>/usr/sbin</p> <p>/sbin</p> <p>/Users/jack/bin</p> <p>Close the terminal then open a new terminal so the path is in effect. Basel will work.</p> <p>cd projects</p> <p>git clone https://github.com/zylo117/tensorflow-gpu-macosx</p> <p>Change directory to where you downloaded the tensorflow source</p> <p>cd tensorflow-gpu-macosx</p> <p>./configure</p> <p>Just an example for my macbook pro 2013</p> <p>Found possible Python library paths:</p> <p>/Users/jack/miniconda3/lib/python3.6/site-packages</p> <p>Please input the desired Python library path to use. Default is [/Users/jack/miniconda3/lib/python3.6/site-packages]</p> <p>Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n</p> <p>No Google Cloud Platform support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n</p> <p>No Hadoop File System support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: n</p> <p>No Amazon AWS Platform support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n</p> <p>No Apache Kafka Platform support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with XLA JIT support? [y/N]: n</p> <p>No XLA JIT support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with GDR support? [y/N]: n</p> <p>No GDR support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with VERBS support? [y/N]: n</p> <p>No VERBS support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n</p> <p>No OpenCL SYCL support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with CUDA support? [y/N]: y</p> <p>CUDA support will be enabled for TensorFlow.</p> <p>Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.1</p> <p>Please specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/loca/cuda/include</p> <p>Invalid path to CUDA 9.1 toolkit. /usr/loca/cuda/include/lib/libcudart.9.1.dylib cannot be found</p> <p>Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.1</p> <p>Please specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:</p> <p>Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.0</p> <p>Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:</p> <p>Please specify a list of comma-separated Cuda compute capabilities you want to build with.</p> <p>You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.</p> <p>Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]3.0,5.2,6.1</p> <p>Do you want to use clang as CUDA compiler? [y/N]: n</p> <p>nvcc will be used as CUDA compiler.</p> <p>Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:</p> <p>Do you wish to build TensorFlow with MPI support? [y/N]: n</p> <p>No MPI support will be enabled for TensorFlow.</p> <p>Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:</p> <p>Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n</p> <p>Not configuring the WORKSPACE for Android builds.</p> <p>Preconfigured Bazel build configs. You can use any of the below by adding \"--config=\\&lt;&gt;\" to your build command. See tools/bazel.rc for more details.</p> <p>--config=mkl Build with MKL support.</p> <p>--config=monolithic Config for mostly static monolithic build.</p> <p>Configuration finished</p> <p>MCPB01:tensorflow-gpu-macosx jack\\$</p> <p>bazel build --config=cuda --config=opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package</p> <p>Install Donkey on your MacOS - not supported in this course</p> <p>This is assuming you have the virtual env created already with Python3</p> <p>If needed, create the projects directory</p> <p>mkdir projects</p> <p>cd projects</p> <p>Activate the virtual environment</p> <p>source env/bin/activate</p> <p>Look for the (env) in front of your command line. It is indication that env is active</p> <p>To deactivate an environment type deactivate from inside the environment</p> <p>To activate, make sure you are at the projects directory then type source env/bin/activate</p> <p>This is like you did on RPI but I had my car under the ~/projects directory</p> <p>Let's get the latest Donkey Framework from Tawn Kramer</p> <p>git clone https://github.com/tawnkramer/donkey pip install -e donkey[pc]</p> <p>Create a car</p> <p>donkey createcar --path ~/projects/d2t</p> <p>from here you can transfer data from your RPI, train,</p> <p>create a model (autopilot), transfer the model to the RPI, test it.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docfinal/#donkeycar-ai-framework_1","title":"DonkeyCar AI Framework","text":"<p>Donkey AI Framework Explained</p> <p>https://ori.codes/artificial-intelligence/</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/","title":"10docoriginal","text":"<p>UCSD RoboCar ECE &amp; MAE 148</p> <p>Version 20.7 - 23Oct2022</p> <p>Prepared by</p> <p>Dr. Jack Silberman</p> <p>Department of Electrical and Computer Engineering</p> <p>and</p> <p>Dominic Nightingale</p> <p>Department of Mechanical and Aerospace Engineering</p> <p>University of California, San Diego</p> <p>9500 Gilman Dr, La Jolla, CA 92093</p> <p>{width=\"3.3984372265966756in\" height=\"0.8055555555555556in\"}</p> <p>{width=\"1.4427088801399826in\" height=\"1.4427088801399826in\"}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#table-of-contents","title":"Table of Contents","text":"<p>Table of Contents 2</p> <p>Introduction 4</p> <p>Single Board Computer (SBC) Basic Setup 5</p> <p>Jetson Nano (JTN) Configuration 5</p> <p>Jetson Xavier NX Configuration 5</p> <p>Hardware Setup 6</p> <p>Jetson Nano GPIO Header PINOUT 6</p> <p>VESC 7</p> <p>VESC Hardware V6.x 8</p> <p>Motor Detection 10</p> <p>Sensor Detection 14</p> <p>Enable Servo Control For Steering 16</p> <p>VESC Hardware V4.12 (if you have V6, skip this) 17</p> <p>PWM Controller 18</p> <p>Wiring adafruit board 19</p> <p>Detecting the PWM controller 20</p> <p>Emergency stop - Relay 21</p> <p>Logitech F710 controller 23</p> <p>Other JoyStick Controllers 23</p> <p>LD06 Lidar 25</p> <p>Laser Map 25</p> <p>Mechanical drawing 25</p> <p>Install OpenCV from Source 26</p> <p>DonkeyCar AI Framework 26</p> <p>Setting up the DonkeyCar AI Framework 26</p> <p>Create a virtual environment for the DonkeyCar AI Framework. 27</p> <p>Confirm openCV build from previous steps 28</p> <p>Tensorflow 29</p> <p>PyTorch 32</p> <p>Installing Donkeycar AI Framework 35</p> <p>Create a Car 36</p> <p>Donkeycar manage.py 37</p> <p>Modifying PWM board configuration 37</p> <p>Modifying Camera 37</p> <p>Quick Test 38</p> <p>Modifying Joystick 39</p> <p>Calibration of the Throttle and Steering 41</p> <p>Begin Calibration 42</p> <p>Saving car configuration file 44</p> <p>Driving the Robot to Collect data 46</p> <p>Backup of the uSD Card 50</p> <p>If needed, we have an uSD Card Image Ready for Plug and Play 52</p> <p>ROS with Docker 53</p> <p>Supporting material 54</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_1","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#introduction","title":"Introduction","text":"<p>This document was derived from the DIY RoboCar - Donkey Car Framework</p> <p>Reference information can be found at [http://docs.donkeycar.com]{.underline}</p> <p>At UC San Diego's Introduction to Autonomous Vehicles class (ECE MAE148), we use an AI Framework called Donkey Car which is based on Deep Learning / Human behavior cloning as well as we do traditional programming using Robot Operating System (ROS2).</p> <p>DonkeyCar can be seen as the \"hello world\" of affordable scaled autonomous cars</p> <p>We have added other features into our UCSD scale robot cars that are not found</p> <p>at the Default Donkey car build such as a wireless emergency off switch. Therefore, please follow</p> <p>the instructions found in this document vs. the default Donkey built.</p> <p>Another framework we use called UCSD Robocar is primarily maintained and developed by Dominic Nightingale right here at UC San Diego. UCSD Robocar uses ROS and ROS2 for controlling our scaled robot cars which can vary from traditional programming or machine learning to achieve an objective. The framework works with a vast selection of sensors and actuation methods in our inventory making it a robust framework to use across various platforms. Has been tested on 1/16, 1/10, 1/5 scaled robot cars and soon our go-karts.</p> <p>As August 2019 we transitioned from the single board computer (SBC) called Raspberry PI to the</p> <p>Nvidia Jetson Nano. If you are using a Raspberry PI, then search in this document for</p> <p>Raspberry PI (RPI or PI) Configuration.</p> <p>On 28Aug19, we updated the instructions to include Raspberry PI 4B</p> <p>Depending on your Single Board Computer, Jetson Xavier NX, Jetson Nano,</p> <p>then follow the related instructions.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_2","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#single-board-computer-sbc-basic-setup","title":"Single Board Computer (SBC) Basic Setup","text":"<p>We will be using the Ubuntu Linux distribution. In the class you have access to a virtual</p> <p>machine image file with Ubuntu.</p> <p>We won't install ROS2 directly into the SBC, we will be using Docker images and containers.</p> <p>You will install OpenCV from source as part of your learning on compiling and building</p> <p>software from source.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#jetson-nano-jtn-configuration","title":"Jetson Nano (JTN) Configuration","text":"<p>[Instructions to configure the Jetson Nano]{.underline}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#jetson-xavier-nx-jnx-configuration","title":"Jetson Xavier NX (JNX) Configuration","text":"<p>[Instructions to Configure the Jetson Xaviver NX]{.underline}</p> <p>[Archive location to previous JetPack versions]{.underline}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#editing-remotely-with-jupyter-notebooks","title":"Editing Remotely with Jupyter Notebooks","text":"<p>Install Jupyter notebook on your Jetson:</p> <p>sudo apt install jupyter-notebook</p> <p>[https://ljvmiranda921.github.io/notebook/2018/01/31/running-a-jupyter-notebook/]{.underline}</p> <p>Help document for editing using Jupyter notebook:</p> <p>[Conifguring Jupyter Notebook on SSH]{.underline}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#hardware-setup","title":"Hardware Setup","text":"<p>You should consider breaking the work on building the robots per team member:</p> <p>a)  Someone could start to build OpenCV GPU accelerated in parallel     while you build the robot. [It will take several hours building     OpenCV from     source]{.underline}.     Try to divide the work by team members ...</p> <p>b)  Start designing, 3D Printing, Laser Cutting the parts</p> <p>As of Spring 2022, we upgraded all the ECE MAE 148 robots to use VESCs.</p> <p>If you are using a regular Electronic Speed Controller (ESC) vs. a VESC you may need to use a I2C PWM board to generate reliable PWM to the ESC and Steering Servo, look for the PWM Controller text below.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#jetson-nano-gpio-header-pinout","title":"Jetson Nano GPIO Header PINOUT","text":"<p>I2C and UART pins are connected to hardware and should not be reassigned. By default, all other pins</p> <p>(except power) are assigned as GPIO. Pins labeled with other functions are recommended functions if</p> <p>using a different device tree. Here's [a spreadsheet map to RPi]{.underline} to help.</p> <p>{width=\"2.8347725284339456in\" height=\"5.244792213473316in\"}</p> <p>sudo usermod -aG i2c jetson</p> <p>sudo reboot now</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#vesc","title":"VESC","text":"<p>VESC is a super cool Electronic Speed Controller (ESC) that runs open source code with</p> <p>significantly more capabilities than a regular RC Car ESC.</p> <p>VESCs are very popular for electrical skateboards, DIY electrical scooters, and robotics.</p> <p>For robotics, one of the capabilities we will use the most is Odometry (speed and position)</p> <p>based on the sensors on brushless motors (sensored) or to some extent, specially using the latest VESCs and firmware, it is also available with brushless motors without sensors (sensorless).</p> <p>[https://vesc-project.com/]{.underline}</p> <p>[VESC Setup Instructions]{.underline}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#logitech-f710-controller","title":"Logitech F710 controller","text":"<p>Place your Logitech F710 controller on the [x mode]{.mark}</p> <p>(look for small switch in one of the controller face)</p> <p>Connect the USB JoyStick Dongle into the JTN and then list the input devices again</p> <p>ls /dev/input</p> <p>(env) jetson@ucsdrobocar00:\\~/projects/d3\\$ ls /dev/input</p> <p>by-id event0 event2 event4 mice mouse1</p> <p>by-path event1 event3 [js0]{.mark} mouse0</p> <p>We are looking for a js0</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#other-joystick-controllers","title":"Other JoyStick Controllers","text":"<p>JoyStick Controllers - either the Logitech F-10 or PS4</p> <p>Make sure your myconfig.py on your car directory reflects your controller</p> <p>At the SBC</p> <p>Connecting the LogiTech Controller is as easy as plugging in the USB dongle at the SBC.</p> <p>Or if you have a PS4 controller</p> <p>Connecting a PS4 Controller - Bluetooth</p> <p>Deactivate the virtual environment if you are using one</p> <p>deactivate</p> <p>sudo apt-get install bluetooth libbluetooth3 libusb-dev</p> <p>sudo systemctl enable bluetooth.service</p> <p>[Need these for XBox Controller, skip for PS4]{.mark}</p> <p>[sudo apt install sysfsutils]{.mark}</p> <p>[sudo nano /etc/sysfs.conf]{.mark}</p> <p>[add this line at the end of the file:]{.mark}</p> <p>[/module/bluetooth/parameters/disable_ertm=1]{.mark}</p> <p>[Reboot your SBC and see if ertm was disabled.]{.mark}</p> <p>[cat /sys/module/bluetooth/parameters/disable_ertm]{.mark}</p> <p>[The result should print Y]{.mark}</p> <p>[At the PS4 controller]{.mark}</p> <p>[Press the Share and PS buttons at the same time.]{.mark}</p> <p>[The controller light will flash like a strobe light. That means it is in the pairing mode]{.mark}</p> <p>[If the SBC is not seeing the PS4, you should try a mUSB cable between the SBC and the]{.mark}</p> <p>[PS4 controller.]{.mark}</p> <p>[At the SBC]{.mark}</p> <p>sudo bluetoothctl</p> <p>agent on</p> <p>default-agent</p> <p>scan on</p> <p>[If your controller is off, Press Share/PlayStation]{.mark}</p> <p>[example of a PS4 controller mac address]{.mark}</p> <p>[Device A6:15:66:D1:46:1B Alias: Wireless Controller]{.mark}</p> <p>connect YOUR_MAC_ADDRESS</p> <p>trust YOUR_MAC_ADDRESS</p> <p>quit</p> <p>[If your ps4 turns off, turn it on again by pressing PS]{.mark}</p> <p>[If you want to check controller was connected]{.mark}</p> <p>ls /dev/input</p> <p>[and see if js0 is listed.]{.mark}</p> <p>[Lets test the joystick in a linux machine]{.mark}</p> <p>sudo apt-get update</p> <p>sudo apt-get install -y jstest-gtk</p> <p>jstest /dev/input/js0</p> <p>[Turn off your PS4 controller by pressing and holding PS]{.mark}</p> <p>To remove a device, let\\'s say another JoyStick that you don't use anymore</p> <p>bluetoothctl</p> <p>paired-devices</p> <p>remove THE_CONTROLLER_MAC_ADDRESS</p> <p>I had to try the method above twice, I rebooted the SBC in between</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_3","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#ld06-lidar","title":"LD06 Lidar","text":"<p>Datasheet for LD06 lidar: [datasheet]{.underline}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#laser-map","title":"Laser Map","text":"<p>{width=\"3.4479166666666665in\" height=\"2.9040201224846895in\"}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#mechanical-drawing","title":"Mechanical drawing","text":"<p>{width=\"4.705857392825897in\" height=\"3.932292213473316in\"}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#previous-versions-of-hardware","title":"Previous Versions of Hardware","text":"<p>Skip the PWM Controller and EMO starting in 2022 Summer II. Left here for people using these as low cost alternative robot.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#pwm-controller","title":"PWM Controller","text":"<p>If you are using a VESC, we don't use the PWM board, you can skip this part.</p> <p>These PWM controllers are used only if one has a regular ESC (not the cool VESC).</p> <p>[We are following the standard from RC CAR world, Channel 1 for Steering, Channel 2 for Throttle]{.mark}</p> <p>Note:The default DonkeyCar build uses Channels 0 and 1</p> <p>The UCSDRoboCar has at least two actuators. A steering servo and the DC motor connected to an Electronics Speed Controller (ESC). These are controlled by PWM (Pulse Width Modulation).</p> <p>We use PWM Controller to generate the PWM signals, a device similar to the one in the picture below</p> <p>{width=\"3.0268350831146105in\" height=\"2.1093755468066493in\"}</p> <p>Shutdown the JTN if it is on by typing this command</p> <p>sudo shutdown -h now</p> <p>Connect the Steering Servo to the Channel 1</p> <p>Connect the Throttle (Electronic Speed Controller ESC) to Channel 2</p> <p>Observe the orientation of the 3 wires connector coming from the Steering Servo and ESC. Look for a the black or brown wire, that is the GND (-).</p> <p>Lets install the PWM Controller</p> <p>[https://www.jetsonhacks.com/nvidia-jetson-nano-j41-header-pinout/]{.underline}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#wiring-adafruit-board","title":"Wiring adafruit board","text":"<p>You need to connect the following pins between the JTN and the PWM board:</p> <p>Disconnect the power to the JTN</p> <p>+3.3v, the two I2C pins (SDA and SCL) and ground</p> <p>The 3.3V from the JTN goes to the VCC at the PWM board</p> <p>Note: for the ground connection you need to skip one pin (skip pin 7)</p> <ul> <li> <p>3.3V - pin 1</p> </li> <li> <p>SDA - pin 3</p> </li> <li> <p>SCL- pin 5</p> </li> <li> <p>No_Connect (Skip) - pin 7</p> </li> <li> <p>Ground - pin 9</p> </li> </ul> <p>{width=\"2.115685695538058in\" height=\"2.8177088801399823in\"}{width=\"2.021828521434821in\" height=\"2.6927088801399823in\"}{width=\"2.2616382327209097in\" height=\"3.0364588801399823in\"}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_4","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#detecting-the-pwm-controller","title":"Detecting the PWM controller","text":"<p>Lets detect the PWM controller</p> <p>sudo i2cdetect -r -y 1</p> <p>0 1 2 3 4 5 6 7 8 9 a b c d e f</p> <p>00: -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>40: [40]{.mark} -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>70: [70]{.mark} -- -- -- -- -- -- --</p> <p>If you see the 40 and 70 above, the JTN is communicating with the PWM board</p> <p>If you want to save some typing everytime you log into the JTN, add it to the end of .bashrc</p> <p>nano \\~/.bashrc</p> <p>source /projects/envs/donkey/bin/activate</p> <p>cd \\~/projects/d3</p> <p>Test is with a reboot</p> <p>sudo reboot now</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_5","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#emergency-stop-relay","title":"Emergency stop - Relay","text":"<p>If using an ESC vs. VESC follow this, with VESC skip</p> <p>Connecting the Emergency Stop Circuit and Batteries into the Robot</p> <p>For this part of your robot, you will have to do some hacking. That is part of the class.</p> <p>The instructor will discuss the principle of the circuit and how to implement it with the component in your robot kit.</p> <p>Long story short, the PWM Controller we use has a disable pin. If the correct logic is applied to it for example Logic 1 or 0 (3.3V or 0V) it will disable the PWM signals and the UCSDRoboCar will stop.</p> <p>Think why one needs a separate EMO circuit vs. relying on the software, operating system and computer communicating with the PWM controller that then send PWM pulses to the actuators (steering servo and driving DC motor by the electronics speed controller)</p> <p>First search on the web and read the datasheet of the emergency stop switch (EMO) components provided with the robot kit and discuss with your teammates how the EMO will work. You got two main components to build the WireLess EMO switch:</p> <p>a)  A Wireless Relay with wireless remote controls</p> <p>b)  A Red/Blue high power LED. This is to help the user know if the car     is disable (Red) or Enabled (Blue).</p> <p>Do some discussion with your Team and pay attention to the Lecture explanations:</p> <ul> <li> <p>What is the disable pin of the PWM controller?</p> </li> <li> <p>Does it disable logic 1 or 0?</p> </li> <li> <p>How to wire the wireless relay to provide the logic you need to     disable the PWM controller? (1 or 0)</p> </li> <li> <p>What pin at the Single Board Computer (SBC) can provide the logic     you need to disable the PWM controller</p> </li> <li> <p>How to connect the LEDs (Blue and Red) to indicate (BLUE - enabled),     (RED - power is on / robot is disabled).</p> </li> <li> <p>What is the fail safe situation, using the normally closed or     normally open pins of the wireless relay to disable the PWM     controller?</p> </li> <li> <p>Note: The power to the PWM controller, that powers the LEDs, comes     from ESC (Electronics Speed Controller). Therefore, you have to     connect the robot batteries to the ESC and the ESC to the PWM     controller. We are using channel 2 for the ESC. Channel 1 for     the Steering. Then you need to power the ESC for the circuit to work     ...</p> </li> </ul> <p>After you see that the EMO is working, i.e. wireless remote control disables the PWM, and the LEDs light up as planned, then you need to document your work. Please use a schematic software such as Fritzing ([http://fritzing.org/home/]{.underline}) to document your electrical schematic.</p> <p>It may seem we did the opposite way; document after your build. In our case, you learned by looking at components information, thinking about the logic, and experimenting. Since you are an engineer you need to document even if it was a hack / test try first...</p> <p>Working in a company you may do fast prototyping first then document your work when the risk is low. On larger projects you design, make schematics, diagrams, drawings, work instructions, then build it. Keep that in mind!</p> <p>Now you can go drive your robot to collect data. Make sure to keep the</p> <p>EMO handy and used when needed!</p> <p>Also keep in mind that \\&lt;X&gt; on your controller is like an emergency break.</p> <p>When you run Donkey it will display the functions associated with the joystick buttons.</p> <p>Read and remember them</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_6","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_7","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#install-opencv-from-source","title":"Install OpenCV from Source","text":"<p># Installing an Open Source Computer Vision (OpenCV) package with CUDA Support</p> <p># As of Jan 2020, NVIDIA is not providing OpenCV optimized to use CUDA (GPU acceleration).</p> <p># Search the web if you are curious why.</p> <p># [https://forums.developer.nvidia.com/t/opencv-cuda-python-with-jetson-nano/72902]{.underline}</p> <p>#[Here are the instructions to build OpenCV from source]{.underline}</p> <p>It will take an approximate 4 hrs to install opencv</p> <p>1. jtop</p> <p>2. 4</p> <p>3. Add 6 GB of swap space and enable</p> <p>4. cd \\~</p> <p>5. nano install_opencv.sh</p> <p>6. Copy the entire contents of the attached [file]{.underline}</p> <p>7. bash install_opencv.sh</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#donkeycar-ai-framework","title":"DonkeyCar AI Framework","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#setting-up-the-donkeycar-ai-framework","title":"Setting up the DonkeyCar AI Framework","text":"<p>Reference [http://docs.donkeycar.com]{.underline}</p> <p>Make sure that the OpenCV you want to use supporting CUDA is already available as a system</p> <p>wide package.</p> <p>Remember that when you are compiling and building software from source, it may take a few</p> <p>hours ...</p> <p>SSH into the Single Board Computer (SBC) e.g., RPI, JTN, JNX, etc.</p> <p># Install some packaged, some may be already installed</p> <p>sudo apt update -y</p> <p>sudo apt upgrade -y</p> <p>sudo usermod -aG dialout jetson</p> <p>sudo apt-get install -y build-essential python3 python3-dev python3-pip libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran libxslt1-dev libxml2-dev libffi-dev libcurl4-openssl-dev libssl-dev libpng-dev libopenblas-dev openmpi-doc openmpi-bin libopenmpi-dev libopenblas-dev git nano</p> <p>Install RPi.GPIO clone for Jetson Nano</p> <p>[https://github.com/NVIDIA/jetson-gpio]{.underline}</p> <p>pip3 install Jetson.GPIO</p> <p>If the pip install complains about ownership of the directory*</p> <p>then execute the following command</p> <p>sudo chown -R jetson:jetson /home/jetson/.cache/pip</p> <p>ex:</p> <p>WARNING: The directory \\'/home/jetson/.cache/pip/http\\' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo\\'s -H flag.</p> <p>WARNING: The directory \\'/home/jetson/.cache/pip\\' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of</p> <p>that directory. If executing pip with sudo, you may want sudo\\'s -H flag.</p> <p>If pip breaks for some reason, you can reinstall it with the following lines</p> <p>python3 -m pip uninstall pip</p> <p>sudo apt install python3-pip --reinstall</p> <p>If the install request elevated privileges, execute the following command</p> <p>sudo pip3 install Jetson.GPIO</p> <p>if pip has a new version</p> <p>pip3 install --upgrade pip</p> <p>Let's make sure the user jetson can use gpio</p> <p>sudo groupadd -f -r gpio</p> <p>sudo usermod -a -G gpio jetson</p> <p>sudo cp /opt/nvidia/jetson-gpio/etc/99-gpio.rules /etc/udev/rules.d/</p> <p>28Jan20 - did not work with JetPack3.4</p> <p>15May21- did not work with JetPack4.5</p> <p>19Oct21 - did not work with JetPack4.6</p> <p>18Sep22 - did not work with JetPack4.6.2</p> <p>Will get back to it later if the jetson user can not access GPIO</p> <p>sudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger</p> <p>We want to have control over the versions of each software library to minimize the framework from</p> <p>breaking after system-wide upgrades. Therefore, lets create a virtual environment for the</p> <p>DonkeyCar.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#if-packages-are-being-held-back","title":"If packages are being held back","text":"<p>sudo apt-get --with-new-pkgs upgrade</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#create-a-virtual-environment-for-the-donkeycar-ai-framework","title":"Create a virtual environment for the DonkeyCar AI Framework.","text":"<p>If you want the virtual environment to be under the user's home directory, make sure to be on the</p> <p>home directory for user jetson</p> <p>If you have not done so, lets create a directory to store our projects and one subdirectory</p> <p>to store virtual environments</p> <p>cd \\~</p> <p>mkdir projects</p> <p>cd projects</p> <p>mkdir envs</p> <p>cd envs</p> <p>pip3 install virtualenv</p> <p>if complains about user permission</p> <p>pip3 install virtualenv --user</p> <p>We will create a virtual environment called donkey since our AI framework is</p> <p>based on the Donkey Car project</p> <p>Since your SBC will be initially dedicated to the class AI framework (Donkey), at least until</p> <p>your custom project starts, let's activate the donkey virtual env automatically every time the user</p> <p>Jetson logs into the SBC. We can remove this settings later if needed when using ROS2</p> <p>echo \\\"source \\~/projects/envs/donkey/bin/activate\\\" &gt;&gt; \\~/.bashrc</p> <p>source \\~/.bashrc</p> <p>When a virtual environment is active, you should see (name_of_virtual_enviroment) in front of the terminal prompt.</p> <p>ex:</p> <p>[(donkey)]{.mark} jetson@ucsdrobocar-xxx-yy:\\~\\$</p> <p>At this point, using pip and pip3 should be the same as using pip3 by default in this virtual environment.</p> <p>https://docs.donkeycar.com/guide/robot_sbc/setup_jetson_nano/ 46</p> <p>cd \\~/projects/envs/donkey/lib/python3.6/site-packages/</p> <p>ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so cv2.so</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#it-is-necessary-to-create-a-link-to-it","title":"it is necessary to create a link to it","text":"<p># Go to the folder where OpenCV\\'s native library is built</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#cd-usrlocallibpython36site-packagescv2python-36","title":"cd /usr/local/lib/python3.6/site-packages/cv2/python-3.6","text":"<p># Rename</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#mv-cv2cpython-36m-xxx-linux-gnuso-cv2so","title":"mv cv2.cpython-36m-xxx-linux-gnu.so cv2.so","text":"<p># Go to your virtual environments site-packages folder if previously set</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#cd-envlibpython36site-packages","title":"cd \\~/env/lib/python3.6/site-packages/","text":"<p># Or just go to your home folder if not set a venv site-packages folder</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#cd","title":"cd \\~","text":"<p># Symlink the native library</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#ln-s-usrlocallibpython36site-packagescv2python-36cv2so","title":"ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so","text":"<p>cv2.so</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#note-that-it-is-almost-mandatory-to-create-a-virtual-environment-in","title":"NOTE that it is almost mandatory to create a virtual environment in","text":"<p>order to properly install</p> <p># tensorflow, scipy and keras, and always a best practice.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_8","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#confirm-that-opencv-built-from-previous-steps-is-working-on-the-virtual-environment-donkey","title":"Confirm that OpenCV built from previous steps is working on the virtual environment Donkey","text":"<p># Testing to see if OpenCV is installed in the virtual env.</p> <p>python3 -c \\'import cv2 as cv; print(cv.__version__)\\'</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:\\~/projects/envs/donkey\\$ python3 -c \\'import cv2 as cv; print(cv.__version__)\\'</p> <p>4.6.0</p> <p># We won\\'t use Python2, but just in case one will need it for some reason</p> <p>python2 -c \\'import cv2 as cv; print(cv.__version__)\\'</p> <p>We are not done installing software yet. We need to install more dependencies..</p> <p>Make sure you have the donkey virtual environment activated</p> <p>Remember some of these installs may take a while. It does not mean that the SBC is frozen, you</p> <p>can see that the CPU is busy with top, htop, or jtop</p> <p>source \\~/projects/envs/donkey/bin/activate</p> <p>pip3 install -U pip testresources setuptools</p> <p>pip3 install -U futures==3.1.1 protobuf==3.12.2 pybind11==2.5.0</p> <p>pip3 install -U cython==0.29.21 pyserial</p> <p>pip3 install -U future==0.18.2 mock==4.0.2 h5py==2.10.0 keras_preprocessing==1.1.2 keras_applications==1.0.8 gast==0.3.3</p> <p>pip3 install -U absl-py==0.9.0 py-cpuinfo==7.0.0 psutil==5.7.2 portpicker==1.3.1 six requests==2.24.0 astor==0.8.1 termcolor==1.1.0 wrapt==1.12.1 google-pasta==0.2.0</p> <p>pip3 install -U gdown</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#tensorflow","title":"Tensorflow","text":"<p>Now let\\'s install [Tensorflow]{.underline} (Artificial Neural Network software).</p> <p>\"TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.\"</p> <p>Lets install Tensorflow enabled for GPU acceleration</p> <p>Another chance for you to study while software is being installed...</p> <p>Background information here</p> <p>[https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html]{.underline}</p> <p>Remember you are using a low power SBC, depending on the size of the software it takes a while</p> <p>We are installing Tensorflow outside the virtual environment so it is available for other uses</p> <p>Here is another chance for you to study while software is being installed...</p> <p>[Background information here]{.underline}</p> <p>We are using JetPack 4.5 because the new DonkeyCar release was breaking the install with JetPack4.6.2. It requires Python 7 and newer.</p> <p>Let\\'s stick with JetPack4.5 for now</p> <p>[https://developer.download.nvidia.com/compute/redist/jp/v45/tensorflow/]{.underline}</p> <p>As of 18Sep22</p> <p>tensorflow-1.15.4+nv20.12-cp36-cp36m-linux_aarch64.whl 218MB 2020-12-18 14:54:04</p> <p>tensorflow-2.3.1+nv20.12-cp36-cp36m-linux_aarch64.whl 264MB 2020-12-18 14:54:06</p> <p>tensorflow-1.15.5+nv21.2-cp36-cp36m-linux_aarch64.whl 218MB 2021-02-26 16:10:00</p> <p>tensorflow-2.4.0+nv21.2-cp36-cp36m-linux_aarch64.whl 273MB 2021-02-26 16:10:14</p> <p>tensorflow-1.15.5+nv21.3-cp36-cp36m-linux_aarch64.whl 218MB 2021-03-25 18:14:17</p> <p>tensorflow-2.4.0+nv21.3-cp36-cp36m-linux_aarch64.whl 273MB 2021-03-25 18:14:48</p> <p>tensorflow-1.15.5+nv21.4-cp36-cp36m-linux_aarch64.whl 218MB 2021-04-26 20:36:59</p> <p>tensorflow-2.4.0+nv21.4-cp36-cp36m-linux_aarch64.whl 273MB 2021-04-26 20:38:13</p> <p>tensorflow-1.15.5+nv21.5-cp36-cp36m-linux_aarch64.whl 218MB 2021-05-20 20:19:08</p> <p>tensorflow-2.4.0+nv21.5-cp36-cp36m-linux_aarch64.whl 274MB 2021-05-20 20:19:20</p> <p>tensorflow-1.15.5+nv21.6-cp36-cp36m-linux_aarch64.whl 220MB 2021-06-29 18:18:14</p> <p>tensorflow-2.5.0+nv21.6-cp36-cp36m-linux_aarch64.whl 293MB 2021-06-29 18:18:43</p> <p># This will install the latest tensorflow compatible with the Jet Pack as a system package</p> <p>Alternatively if you want to chose a particular version:</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#pip3-install-pre-extra-index-url","title":"pip3 install --pre --extra-index-url","text":"<p>https://developer.download.nvidia.com/compute/redist/jp/v45 tensorflow==2.3.1</p> <p>Previous versions install</p> <p>pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v45 tensorflow==2.3.1</p> <p>pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v45 tensorflow==2.4.0</p> <p>Remember you are using a low power SBC, depending on the size of the software it takes a while</p> <p>Lets verify that Tensorflow installed correctly</p> <p>python3</p> <p>import tensorflow</p> <p>exit()</p> <p>No errors should be reported</p> <p>If you get errors importing Tensorflow 2.5.0, try these</p> <p>pip install numpy==1.19.2</p> <p>ex:</p> <p>If you see info with libcuda it means, Tensorflow will be accelerated using the CUDA</p> <p>cores of the SBC's GPU</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:\\~/projects\\$ python3</p> <p>Python 3.6.9 (default, Jun 29 2022, 11:45:57)</p> <p>[GCC 8.4.0] on linux</p> <p>Type \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more information.</p> <p>&gt;&gt;&gt; import tensorflow</p> <p>2022-08-09 12:19:25.285765: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library [libcudart.]{.mark}so.10.2</p> <p>&gt;&gt;&gt; exit()</p> <p>Verifying that TensorRT was installed</p> <p>sudo apt-get update</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:\\~\\$ sudo apt-get install tensorrt</p> <p>Reading package lists... Done</p> <p>Building dependency tree</p> <p>Reading state information... Done</p> <p>tensorrt is already the newest version (7.1.3.0-1+cuda10.2).</p> <p>0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.</p> <p>dpkg -l | grep TensorRT</p> <p>arm64 Meta package of TensorRT</p> <p>ii uff-converter-tf 7.1.3-1+cuda10.2 arm64 UFF converter for TensorRT package</p> <p># Periodically the versions of tensorflow, cuDNN / CUDA give us conflict. Here is a list of compatibility</p> <p>[https://www.tensorflow.org/install/sourcegpu]{.underline}</p> <p>Installing pycuda - it will take a while again...</p> <p>pip3 install pycuda</p> <p>If you are having errors installing pycuda use the following command:</p> <p>pip3 install pycuda==2020.1</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#pytorch","title":"PyTorch","text":"<p>Lets install [PyTorch]{.underline} too</p> <p>\"An open source machine learning framework that accelerates the path from research prototyping to production deployment\"</p> <p>Again, these steps will take some time. Use your time wisely</p> <p>cd \\~/projects</p> <p>wget https://nvidia.box.com/shared/static/p57jwntv436lfrd78inwl7iml6p13fzh.whl</p> <p>cp p57jwntv436lfrd78inwl7iml6p13fzh.whl torch-1.8.0-cp36-cp36m-linux_aarch64.whl</p> <p>pip3 install torch-1.8.0-cp36-cp36m-linux_aarch64.whl</p> <p>sudo apt-get install libjpeg-dev zlib1g-dev libpython3-dev libavcodec-dev libavformat-dev libswscale-dev</p> <p>git clone -b v0.9.0 https://github.com/pytorch/vision torchvision</p> <p>cd torchvision</p> <p>python setup.py install</p> <p>cd ../</p> <p># it will take a good while again. Keep studying other things...</p> <p>Testing Pythorch install</p> <p>python</p> <p>import torch</p> <p>print(torch.__version__)</p> <p>exit()</p> <p>ex:</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:\\~/projects\\$ python</p> <p>Python 3.6.9 (default, Jan 26 2021, 15:33:00)</p> <p>[GCC 8.4.0] on linux</p> <p>Type \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more information.</p> <p>&gt;&gt;&gt; import torch</p> <p>&gt;&gt;&gt; print(torch.__version__)</p> <p>1.8.0</p> <p>&gt;&gt;&gt; exit()</p> <p>(donkey) jetson@ucsdrobocar-xxx-yy:\\~/projects\\$</p> <p>One more test</p> <p>pip3 show torch</p> <p>ex:</p> <p>Name: torch</p> <p>Version: 1.8.0</p> <p>Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration</p> <p>Home-page: https://pytorch.org/</p> <p>Author: PyTorch Team</p> <p>Author-email: packages@pytorch.org</p> <p>License: BSD-3</p> <p>Location: /home/jetson/projects/envs/donkey/lib/python3.6/site-packages</p> <p>Requires: dataclasses, typing-extensions, numpy</p> <p>Required-by: torchvision</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_9","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#as-of-summer-ii-2022-we-are-using-a-new-stereo-camera-from-luxonis","title":"As of Summer II 2022, we are using a new Stereo Camera from Luxonis","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#configuring-oakd-lite","title":"Configuring OAKD Lite","text":"<p>Open a terminal window and run the following commands:</p> <p>sudo apt update &amp;&amp; sudo apt upgrade</p> <p># after upgrades</p> <p>sudo reboot now</p> <p>If you have not added the extra swap space while building OpenCV, please add it</p> <p>You can use jtop to add more swap space using the left and right keys and clicking the plus button</p> <p>jtop</p> <p>4</p> <p>{width=\"4.005208880139983in\" height=\"1.776890857392826in\"}</p> <p>Add 4G of swap and press \\&lt;S&gt; to enable it.</p> <p>{width=\"3.0156255468066493in\" height=\"2.161738845144357in\"}</p> <p>Alternatively you can use the command line</p> <p># Disable ZRAM:</p> <p>sudo systemctl disable nvzramconfig</p> <p># Create 4GB swap file</p> <p>sudo fallocate -l 4G /mnt/4GB.swap</p> <p>sudo chmod 600 /mnt/4GB.swap</p> <p>sudo mkswap /mnt/4GB.swap</p> <p>If you have an issue with the final command, you can try the following:</p> <p>sudo nano /etc/fstab</p> <p># Add this line at the bottom of the file</p> <p>/mnt/4GB.swap swap swap defaults 0 0</p> <p># Reboot</p> <p>sudo reboot now</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#installing-dependencies","title":"Installing dependencies","text":"<p>Navigate to the directory where you will be installing the luxonis libraries using cd \\~/projects</p> <p>sudo nano install_dependencies.sh</p> <p>Copy the entire contents of the file: [install_dependencies.sh]{.underline}</p> <p>bash install_dependencies.sh</p> <p>echo \\\"export OPENBLAS_CORETYPE=ARMV8\\\" &gt;&gt; \\~/.bashrc</p> <p>echo \\'SUBSYSTEM==\\\"usb\\\", ATTRS{idVendor}==\\\"03e7\\\", MODE=\\\"0666\\\"\\' | sudo tee /etc/udev/rules.d/80-movidius.rules</p> <p>sudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#navigate-using-cd-to-the-folder-where-you-would-like-to-install-the","title":"Navigate using cd to the folder where you would like to install the","text":"<p>camera example files and requirements</p> <p>cd \\~/projects</p> <p>git clone [https://github.com/luxonis/depthai-python.git]{.underline}</p> <p>cd depthai-python/examples</p> <p>python3 install_requirements.py</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#if-you-want-to-test-the-camera-and-you-have-remote-desktop","title":"If you want to test the camera and you have remote desktop","text":"<p>[NoMachine]{.underline} already installed and the OAKD Lite is connected to JTN , run the following in the terminal on a NoMachine session</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#navigate-to-the-examples-folder-in-depthai-python-first-and-then","title":"Navigate to the examples folder in depthai-python first and then","text":"<p>cd ColorCamera</p> <p>python3 rgb_preview.py</p> <p>You should be able to see preview video on the No machine desktop</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_10","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#installing-donkeycar-ai-framework","title":"Installing Donkeycar AI Framework","text":"<p>Lets Install the Donkeycar AI Framework</p> <p>If you are upgrading from Donkey3 then save the values from your calibration that</p> <p>you had on</p> <p>myconfig.py</p> <p>Then let\\'s remove the old donkeycar and d3 directories</p> <p>cd \\~/projects</p> <p>rm -rf donkeycar</p> <p>rm -rf d3</p> <p>If the projects directory was not created yet, mkdir projects</p> <p>Install more dependencies</p> <p>sudo apt-get install python3-dev python3-numpy python-dev libsdl-dev libsdl1.2-dev libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev libsdl1.2-dev libsmpeg-dev python-numpy subversion libportmidi-dev ffmpeg libswscale-dev libavformat-dev libavcodec-dev libfreetype6-dev libswscale-dev libjpeg-dev libfreetype6-dev</p> <p>pip install pygame</p> <p>Lets enable the use of synchronization of files with remote computers using rsync</p> <p>sudo apt-get install rsync</p> <p>This part will take a bit of time. Be patient, please keep in mind that you are using a low power</p> <p>single board computer (SBC).</p> <p>If you are curious if your SBC is really working, you can open another tab in the terminal window</p> <p>or a complete new terminal window, ssh to the JTN then execute the command top or htop</p> <p>look at the CPU utilization...</p> <p>Note I had problems installing Donkey with the latest version of pip (20.0.2). I had to revert</p> <p>to an earlier version of pip. See versions of pip here [https://pip.pypa.io/en/stable/news/]{.underline}</p> <p>On 28 May20, it worked. Keeping the line below for reference in case the problem happens again</p> <p># pip install --upgrade pip==18.1</p> <p>Install Donkey with</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#cd-projects","title":"cd \\~/projects","text":"<p>Get donkeycar from Github</p> <p>git clone https://github.com/autorope/donkeycar</p> <p>cd donkeycar</p> <p>cd \\~/projects</p> <p>git clone https://github.com/autorope/donkeycar</p> <p>cd donkeycar</p> <p>git fetch --all --tags -f</p> <p>git checkout 4.5.1</p> <p>pip install -e .[[nano]{.mark}]</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#pip3-install-e-nano","title":"pip3 install -e .[nano]","text":"<p>Proceed to [Create a Car]{.underline}</p> <p># 1/30/24</p> <p># [mlopezme@ucsd.edu]{.underline} []{.mark}</p> <p># Moises, did you change the instructions to have nano45? It was giving problems in WI24. Does it install Donkey 4.5.1? That is what we need</p> <p># For ECE MAE 148 when we are ready for the latest version of Donkey, let\\'s say using a docker container</p> <p>cd \\~/projects</p> <p>git clone https://github.com/autorope/donkeycar</p> <p>cd donkeycar</p> <p>git fetch --all --tags -f</p> <p>latestTag=\\$(git describe --tags `git rev-list --tags --max-count=1`)</p> <p>git checkout \\$latestTag</p> <p>pip install -e .[[nano45]{.mark}]</p> <p>It may take a while. You may not see progress on the terminal. You can ssh to the SBC</p> <p>and run the command top or htop or jtop from another terminal/tab</p> <p>Grab a coffee, go study something ...</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_11","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#create-a-car","title":"Create a Car","text":"<p>Let's create a car on the path \\~/project/d4</p> <p>cd \\~/projects/donkeycar</p> <p>donkey createcar --path \\~/projects/d4</p> <p>If complains about old version of numpy and the install fails</p> <p>pip install numpy --upgrade</p> <p>________ ______ _________</p> <p>___ __ \\_______________ /___________ __ __ ____/_____ ________</p> <p>__ / / / __ \\_ __ \\_ //_/ _ \\_ / / / _ / _ __ `/_ ___/</p> <p>_ /_/ // /_/ / / / / ,\\&lt; / __/ /_/ / / /___ / /_/ /_ /</p> <p>/_____/ \\____//_/ /_//_/|_| \\___/_\\__, / \\____/ \\__,_/ /_/</p> <p>/____/</p> <p>using donkey v4.3.22 ...</p> <p>Creating car folder: /home/jetson/projects/d4</p> <p>making dir /home/jetson/projects/d4</p> <p>The version of the Donkey car may be newer than the one above...</p> <p># For Winter 2024</p> <p>Make sure the DonkeyCar is version 4.5.1. The latest version of the DonkeyCar (5.x) does not work at the Jetson Nano yet.</p> <p>You spent several hours on this configuration right?! Please make a backup of your uSD card - \"[Backup of the uSD Card]{.underline}\"</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_12","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#if-you-are-using-a-pwm-board-with-a-esc-vs-a-vesc","title":"If you are using a PWM board with a ESC vs. a VESC","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#starting-on-fall22-we-use-a-vesc-for-controlling-the-bldc-motor-skip-setting-the-pwm-board-we-left-here-for-people-that-may-want-to-use-it-on-their-own-robot","title":"Starting on FALL'22, we use a VESC for controlling the BLDC motor. Skip setting the PWM board. We left here for people that may want to use it on their own robot","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#again-if-you-using-an-esc-skip-the-pwm-board-setup","title":"Again, if you using an ESC skip the PWM board setup","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#modifying-pwm-board-configuration","title":"#Modifying PWM board configuration","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#now-we-need-to-edit-the-myconfigpy-to-change-the-default-bus-number","title":"Now we need to edit the myconfig.py to change the default bus number","text":"<p>for the PWM board</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#pca9685","title":"(PCA9685)","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#nano-myconfigpy","title":"nano myconfig.py","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#jetson-nano-set-pca9685_i2c_busnum-1","title":"Jetson Nano: set PCA9685_I2C_BUSNUM = 1","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#remove-the-comment-from-the-line-and-add-1-to-the-busnum","title":"Remove the comment from the line; \"\" and add \"1\" to the BUSNUM","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#pca9685_i2c_busnum-1-none","title":"PCA9685_I2C_BUSNUM = 1 None ...","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#modifying-camera","title":"Modifying Camera","text":"<p>Change the camera type to MOCK to enable us to test the system without a camera</p> <p>nano myconfig.py</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#camera","title":"CAMERA","text":"<p>CAMERA_TYPE = \\\"MOCK\\\" (PICAM|WEBCAM|CVCAM|CSIC|V4L|MOCK)</p> <p># if you have USB camera connected to the JTN , use WEBCAM</p> <p># And change this line so the Donkey can run using the web interface</p> <p>USE_JOYSTICK_AS_DEFAULT = False</p> <p>In summary you change these 3 lines in the myconfig.py to be able to test your Donkey installation</p> <p># if using the PWM board the PWM board and ESC vs. VESC</p> <p>PCA9685_I2C_BUSNUM = 1</p> <p>CAMERA_TYPE = \\\"MOCK\\\"</p> <p>USE_JOYSTICK_AS_DEFAULT = False</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_13","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#quick-test","title":"Quick Test","text":"<p>Lets test the Donkey AI framework install</p> <p>python manage.py drive</p> <p>Adding part PWMSteering.</p> <p>Adding part PWMThrottle.</p> <p>Tub does NOT exist. Creating a new tub...</p> <p>New tub created at: /home/jetson/projects/d3/data/tub_1_19-08-05</p> <p>Adding part TubWriter.</p> <p>You can now go to \\&lt;your pi ip address&gt;:8887 to drive your car.</p> <p>Starting vehicle...</p> <p>8887</p> <p>Lets connect to the JTN by using a web browser from your PC</p> <p>[http://ucsdrobocar-xxx-yy:8887]{.underline}</p> <p>You should see a screen like this</p> <p>{width=\"4.6631517935258096in\" height=\"3.119792213473316in\"}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#we-stop-the-donkey-with-ctrl-c","title":"We stop the Donkey with Ctrl-C","text":"<p>Ctrl-C</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#modifying-joystick","title":"Modifying Joystick","text":"<p>Now let\\'s change the type of joystick we use with Donkey</p> <p>nano myconfig.py</p> <p>JOYSTICK</p> <p>[USE_JOYSTICK_AS_DEFAULT = True]{.mark} when starting the manage.py, when True, wil\\$</p> <p>JOYSTICK_MAX_THROTTLE = 0.5 this scalar is multiplied with the -1 to\\$</p> <p>JOYSTICK_STEERING_SCALE = 1.0 some people want a steering that is less\\$</p> <p>AUTO_RECORD_ON_THROTTLE = True if true, we will record whenever throttle\\$</p> <p>[CONTROLLER_TYPE=\\'F710\\']{.mark} (ps3|ps4|xbox|nimbus|wiiu|F710)</p> <p>python manage.py drive</p> <p>ex</p> <p>Starting vehicle...</p> <p>Opening /dev/input/js0...</p> <p>Device name: Logitech Gamepad F710</p> <p>recorded 10 records</p> <p>recorded 20 records</p> <p>recorded 30 records</p> <p>recorded 40 records</p> <p>erased last 100 records.</p> <p>E-Stop!!!</p> <p>recorded 10 records</p> <p>recorded 20 records</p> <p>recorded 30 records</p> <p>recorded 40 records</p> <p>recorded 50 records</p> <p>The Right Joystick is the Throttle, the Left Joystick is the Steering</p> <p>The Y Button deletes 5s of driving at the default configuration =100 records at 20 Hz</p> <p>The A Button is the emergency break</p> <p>Joystick Controls:</p> <p>+------------------+---------------------------+</p> <p>| control | action |</p> <p>+------------------+---------------------------+</p> <p>| start | toggle_mode |</p> <p>| B | toggle_manual_recording |</p> <p>| Y | erase_last_N_records |</p> <p>| A | emergency_stop |</p> <p>| back | toggle_constant_throttle |</p> <p>| R1 | chaos_monkey_on_right |</p> <p>| L1 | chaos_monkey_on_left |</p> <p>| circle | show_record_acount_status |</p> <p>| R2 | enable_ai_launch |</p> <p>| left_stick_horz | set_steering |</p> <p>| right_stick_vert | set_throttle |</p> <p>+------------------+---------------------------+</p> <p>If your joystick is not returning to neutral</p> <p>You can add a deadzone value</p> <p>on myconfig.py</p> <p>ex:</p> <p>NETWORK_JS_SERVER_IP = \\\"192.168.0.1\\\"when listening for network joystick cont\\$</p> <p>JOYSTICK_DEADZONE = 0.01 when non zero, this is the smallest throt\\$</p> <p>JOYSTICK_THROTTLE_DIR = -1.0 use -1.0 to flip forward/backward, use \\$</p> <p>Lets Integrate the JTN and PWM Controller into the RC Chassis</p> <p>Charge your LiPo Battery</p> <p>After you charge your Lithium Polymer (LiPo) battery(ries) - [some info here]{.underline}</p> <p>Connect the battery(ries) and batteries monitor/alarm</p> <p>Please do not use the batteries without the batteries monitor/alarms</p> <p>If you discharge a LiPO batteries below a threshold lets say 3.0</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_14","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#calibration-of-the-throttle-and-steering","title":"Calibration of the Throttle and Steering","text":"<p>Before you develop code or you can use someone\\'s code to control a robot, we need to calibrate the actuator/mechanism to find out its range of motion compared to the control / command a computer will send to the controller of the actuator/mechanism.</p> <p>The calibration is car specific. If you use the same platform, the numbers should be really close. Otherwise, you will need to calibrate your car.</p> <p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p> <p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p> <p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p> <p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p> <p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p> <p>Follow the safety guidelines provided in person in the class.</p> <p>If something does not sound right, don't do it. Safety first.</p> <p>Power the JTN</p> <p>Power the Electronic Speed Controller (ESC)</p> <p>Lets run a Python command to calibrate Steering and Throttle</p> <p>The donkey commands need to be run from the directory you created for your car, i.e., \\~/projects/[d4]{.mark}</p> <p>If you have not done so, SSH into the JTN</p> <p>If needed, change directory to d4</p> <p>cd \\~/projects/d4</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_15","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#begin-calibration","title":"Begin Calibration","text":"<p>donkey calibrate [--channel 1]{.mark} --bus 1</p> <p>Please make sure that when you are trying the steering values you do not keep the steering</p> <p>servo max out.</p> <p>Please reduce go back 10 or 15 values when you notice the servo motor is not moving or making a</p> <p>constant noise. If you leave the servo motor max-out you will most likely burn it.</p> <p>If by any chance your software locks-up, please turn off the ESC immediately.</p> <p>Then once you run the calibrate again, issue the center value before turning in the ESC again</p> <p>Note that after 10 values or so the calibration may time out. Just run it again.</p> <p>Try some values around 390 to center the steering</p> <p>Enter a PWM setting to test(100-600)370</p> <p>Enter a PWM setting to test(100-600)380</p> <p>Enter a PWM setting to test(100-600)390</p> <p>Enter a PWM setting to test(100-600)400</p> <p>[In my case]{.mark}, 390 seems to center the steering</p> <p>Take note of the left max and right max value. We will be able to adjust these after we try driving the</p> <p>car so it goes straight.</p> <p>ex:</p> <p>390 - Center</p> <p>[290]{.mark} - Steering left max</p> <p>[490]{.mark} - Steering right max</p> <p>Note: If the donkey calibration times-out, just run it again.</p> <p>You can end the calibration by typing CTRL-C</p> <p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p> <p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p> <p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p> <p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p> <p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p> <p>Now to calibrate the Throttle Electronic Speed Controller ESC (THROTTLE)</p> <p>Following the standard for R/C cars. Throttle goes on channel [2]{.mark}</p> <p>donkey calibrate [--channel 2 --bus 1]{.mark}</p> <p>Enter a PWM setting to test(100-600)370</p> <p>Enter a PWM setting to test(100-600)[380]{.mark} (neutral)</p> <p>Enter a PWM setting to test(100-600)390</p> <p>On my case, 380 when I power up the ESC seems to be the middle point (neutral),</p> <p>I will use 380. Your case may vaires. 370 seems a common value</p> <p>Neutral when power the ESC - [380]{.mark}</p> <p>[Make sure the car is balanced over the car stand]{.mark}</p> <p>Then go in increments of 10 until you can no longer hear an increase on the speed</p> <p>of the car. Don't worry much about the max speed since we won't drive that fast autonomously</p> <p>and during training the speed will be limited.</p> <p>Max speed forward - [490]{.mark}</p> <p>Reverse on RC cars is a little tricky because the ESC needs to receive</p> <p>a reverse pulse, zero pulse, and again reverse pulse to start to go backwards.</p> <p>Use the same technique as above set the PWM setting to your zero throttle</p> <p>(lets say 380 or 370).</p> <p>[Enter the reverse value]{.mark}, then the [zero throttle (e.g., 370)]{.mark} value, then a [reverse value again]{.mark}.</p> <p>Enter values +/- 10 of the reverse value to find a reasonable reverse speed. Remember this reverse PWM value.</p> <p>(dk)pi@jackrpi02:\\~/projects/d3 \\$</p> <p>donkey calibrate [--channel 2 --bus 1]{.mark}</p> <p>Enter a PWM setting to test(100-600)360</p> <p>Enter a PWM setting to test(100-600)[370]{.mark}</p> <p>Enter a PWM setting to test(100-600)360</p> <p>Enter a PWM setting to test(100-600)350</p> <p>Enter a PWM setting to test(100-600)340</p> <p>Enter a PWM setting to test(100-600)330</p> <p>Enter a PWM setting to test(100-600)320</p> <p>I got for Throttle</p> <p>[490]{.mark} - Max speed forward</p> <p>[380]{.mark} - Neutral</p> <p>[300]{.mark} - Max speed backwards</p> <p>---</p> <p>For my robocar I have:</p> <p>Steering</p> <p>[290]{.mark} - Steering left max</p> <p>[490]{.mark} - Steering right max</p> <p>Throttle</p> <p>[490]{.mark} - Max speed forward</p> <p>[380]{.mark} - Neutral</p> <p>[300]{.mark} - Max speed backwards</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#saving-car-configuration-file","title":"Saving car configuration file","text":"<p>Now let\\'s write these values into the car configuration file</p> <p>Edit the file my config.py</p> <p>nano myconfig.py</p> <p>Change these values according to YOUR [calibration values]{.mark} and [where you have the Steering Servo and ESC]{.mark} connected</p> <p>...</p> <p>STEERING</p> <p>STEERING_CHANNEL = 1 channel on the 9685 pwm board 0-15</p> <p>STEERING_LEFT_PWM = 290 pwm value for full left steering</p> <p>STEERING_RIGHT_PWM = 490 pwm value for full right steering</p> <p>THROTTLE</p> <p>THROTTLE_CHANNEL = 2 channel on the 9685 pwm board 0-15</p> <p>THROTTLE_FORWARD_PWM = 490 pwm value for max forward throttle</p> <p>THROTTLE_STOPPED_PWM = 380 pwm value for no movement</p> <p>THROTTLE_REVERSE_PWM = 300 pwm value for max reverse throttle</p> <p>Also, change these</p> <p>CAMERA</p> <p>CAMERA_TYPE = \\\"WEBCAM\\\" (PICAM|WEBCAM|CVCAM|CSIC|V4L|MOCK)</p> <p>9865, overrides only if needed, ie. TX2..</p> <p>PCA9685_I2C_ADDR = 0x40 I2C address, use i2cdetect to validate this numb\\$</p> <p>PCA9685_I2C_BUSNUM = 1 None will auto detect, which is fine on the pi. \\$</p> <p>JOYSTICK</p> <p>USE_JOYSTICK_AS_DEFAULT = True when starting the manage.py, when True, wil\\$</p> <p>and if needed to zero the joystick</p> <p>JOYSTICK_DEADZONE = 0.01 when non zero, this is the smallest thro\\$</p> <p>Note: When driving the robot, if you robot has too much power or not enough power</p> <p>you can adjust the max_throttle</p> <p>JOYSTICK_MAX_THROTTLE = 0.5</p> <p>This also will be your starting power setting when using the constant throttle autopilot.</p> <p>You can test driving your robot by issuing</p> <p>python manage.py drive</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#if-you-are-using-the-vesc-and-oakd-camera-on-the-physical-car","title":"If you are using the VESC and OAKD camera on the physical car","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#vesc_1","title":"VESC","text":"<p>Ensure that you have already configured the VESC device using the VESC Tool Software</p> <p>Edit the myconfig.py to have these values</p> <p>DRIVE_TRAIN_TYPE = \\\"VESC\\\"</p> <p>VESC_MAX_SPEED_PERCENT =.2 ## Max speed as a percent of actual max speed</p> <p>VESC_SERIAL_PORT= \\\"/dev/ttyACM0\\\" ## check this val with ls /dev/tty*</p> <p>VESC_HAS_SENSOR= True</p> <p>VESC_START_HEARTBEAT= True</p> <p>VESC_BAUDRATE= 115200</p> <p>VESC_TIMEOUT= 0.05</p> <p>VESC_STEERING_SCALE = .5</p> <p>VESC_STEERING_OFFSET = .5</p> <p>DONKEY_GYM = False</p> <p>(we will leave the CAMERA_TYPE = \"MOCK\" for now to make sure we can drive the car with the VESC)</p> <p>Download the following files</p> <p>~~[https://drive.google.com/drive/folders/1SBzChXK2ebzPHgZBP_AIhVXJOekVc0r3?usp=sharing]{.underline}~~</p> <p>[https://drive.google.com/drive/folders/19TS3VyNXQPBSr41yiPaxpI1lnxClI2c8?usp=sharing]{.underline}</p> <p>And replace them on the Jetson in the locations shown in the images below. Note - to get the files on the jetson you can use SFTP (secure file transfer protocol):</p> <p>Examples on how to use SFTP:\\ sftp jetson@ucsdrobocar-148-xx.local</p> <p>cd To the directory you want to go to</p> <p>put /File/Path/On/Your/Computer</p> <p>alternatively</p> <p>get filename /File/Location/You/Want/Them/Go/On/Your/Computer</p> <p>To get a directory</p> <p>get -rf filename /File/Location/You/Want/Them/Go/On/Your/Computer</p> <p>type \\\"exit\\\" to disconnect</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#width4351609798775153in-height3276042213473316inwidth4307292213473316in-height3131517935258093in","title":"{width=\"4.351609798775153in\" height=\"3.276042213473316in\"}{width=\"4.307292213473316in\" height=\"3.131517935258093in\"}","text":"<p>Once these have been replaced, you should run</p> <p>python manage.py drive</p> <p>It should first throw a pyvesc import error. Follow the description in the terminal to install the needed libraries</p> <p>Then run it again. It should throw a permissions error. Follow the advice on how to fix the error with chmod</p> <p>Int(10) error bug fix (Credit Saimai Lau and Albert Chang):\\ When running python manage.py drive, the intermittent \\\"invalid literal for int() with base 10: \\'None\\' error is from the VESC package checking whether the version of the VESC is below 3.0, so we can comment out that part since we\\'re using 6.0 just do</p> <p>nano /home/jetson/projects/envs/donkey/lib/python3.6/site-packages/pyvesc/VESC/VESC.py</p> <p>and put # at the beginning of lines 38-40 Then \\^S to save and \\^X to exit</p> <p>{width=\"7.5in\" height=\"2.0833333333333335in\"}</p> <p>Error explanation: The self.get_firmware_version() get thes version by requesting it from the VESC and reading the replied bytes, but sometimes the data is lost or incomplete so the function returns \\'None\\' as the version. We already know the version is 6.0 so we don't need this function.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#oakd","title":"OAKD","text":"<p>Once you have the VESC driving the car, you will need to make sure you have set up the document by following the instructions labeled Configuring OAKD Lite found [earlier in this document]{.underline}</p> <p>Edit the myconfig.py camera type to OAKD</p> <p>CAMERA_TYPE = \\\"OAKD\\\"</p> <p>Make sure the camera is working by checking the images that are being saved to the /data/images directory.</p> <p>The easiest way to do this is to go to</p> <p>[http://localhost:8887]{.underline} while running donkeysim, and you should be able to see a livestream from the camera. Note - if several people are running donkeysim at the same time on the same wifi this interface may get buggy</p> <p>You can also do this by either:</p> <p>transferring the files to you laptop or virtual machine</p> <p>with scp, rsync, winscp (windows) or filezilla (mac)</p> <p>Or</p> <p>Using NoMachine by following the instructions found [here]{.underline} in this document</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#gnss-configuration","title":"GNSS Configuration:","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#how-to-plug-pointonenav-to-donkeycar","title":"How to Plug PointOneNav to Donkeycar","text":"<ol> <li> <p>Make sure your user is added to the dialout group. If not</p> <p>a.  sudo adduser jetson dialout</p> <p>b.  sudo reboot now</p> </li> <li> <p>Download     [https://drive.google.com/file/d/1BK_UjH-He9d_D4eObWMHzpHHCqmtq75h/view?usp=share_link]{.underline}     (Note that this zip file cannot be shared outside of the class. It     is still proprietary as of now)</p> </li> <li> <p>Unzip.</p> </li> <li> <p>Run</p> <p>a.  cd quectel-lg69t-am.0.15.0/p1_runner</p> <p>b.  deactivate (This should get you out of the current environment)</p> <p>c.  wget     https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-pypy3-Linux-aarch64.sh     .</p> <p>d.  bash Mambaforge-pypy3-Linux-aarch64.sh</p> <p>e.  Reboot the jetson</p> <p>f.  mamba create --name py37 -c conda-forge python=3.7 pip</p> <p>g.  mamba activate py37</p> <p>h.  [pip3 install -e .]{.mark}</p> <pre><code>i.  %If this fails, you can try just going to the p1_runner\n    directory and running the python3 bin/config_tool.py\n    command, and then doing \"pip install \\_\\_\\_\\_\" for all the\n    missing things, you may just need\n\n    1.  pip install pyserial\n\n    2.  pip install fusion_engine_client\n\n    3.  pip install pynmea\n\n    4.  pip install ntripstreams\n\n    5.  pip install websockets\n</code></pre> <p>i.  python3 bin/config_tool.py reset factory</p> <p>j.  python3 bin/config_tool.py apply uart2_message_rate nmea gga on</p> <p>k.  python3 bin/config_tool.py save</p> <p>l.  python3 bin/runner.py --device-id \\&lt;polaris_username&gt;     --polaris \\&lt;polaris_password&gt; --device-port /dev/ttyUSB1</p> </li> </ol> <p>(if not getting any data including Nans try USB0)</p> <p>Note: The GPS corrections will only happen when you are actively running runner.py. I recommend making a bashrc command that you can run to start up the runner.py program easily in a 2nd terminal while using the GPS for anything.</p> <ol> <li> <p>Create a project with the DonkeyCar path follow template</p> <p>a.  Open a new terminal window</p> <p>b.  Make sure that the donkey car environment is running</p> <pre><code>i.  source \\~/projects/envs/donkey/bin/activate\n</code></pre> <p>c.  cd \\~/projects</p> <p>d.  donkey createcar --path ./mycar --template path_follow</p> </li> <li> <p>Set the following in the myconfig.py</p> <p>a.  GPS_SERIAL = \"/dev/ttyUSB2\" (USB1 if USB0 used above)</p> <p>b.  GPS_SERIAL_BAUDRATE = 460800</p> <p>c.  GPS_DEBUG = True</p> <p>d.  HAVE_GPS = True</p> <p>e.  GPS_NMEA_PATH = None</p> </li> <li> <p>Also set things like the VESC parameters in myconfig.py. You can     copy these over from the donkeycar you created earlier.</p> </li> <li> <p>Run</p> <p>a.  python3 manage.py drive</p> </li> <li> <p>You should see GPS positions being outputted after you run     Donkeycar. If you don't want to output set GPS_DEBUG to False</p> </li> <li> <p>Configure button actions</p> <p>a.  SAVE_PATH_BTN is the button to save the in-memory path to a     file.</p> <p>b.  LOAD_PATH_BTN is the button to (re)load path from the csv file     into memory.</p> <p>c.  RESET_ORIGIN_BTN is the button to set the current position as     the origin.</p> <p>d.  ERASE_PATH_BTN is the button to erase path from memory and reset     the origin.</p> <p>e.  TOGGLE_RECORDING_BTN is the button to toggle recording mode on     or off. Note that there is a pre-assigned button in the web ui,     so there is not need to assign this button to one of the web/w*     buttons if you are using the web ui.</p> <p>f.  INC_PID_D_BTN is the button to change PID \\'D\\' constant by     PID_D_DELTA.</p> <p>g.  DEC_PID_D_BTN is the button to change PID \\'D\\' constant by     -PID_D_DELTA</p> <p>h.  INC_PID_P_BTN is the button to change PID \\'P\\' constant by     PID_P_DELTA</p> <p>i.  DEC_PID_P_BTN is the button to change PID \\'P\\' constant by     -PID_P_DELTA</p> </li> </ol> <p>The logitech buttons are named stuff like \"X\" or \"R1\" See the example config below.\\ SAVE_PATH_BTN = \\\"R1\\\" # button to save path</p> <p>LOAD_PATH_BTN = \\\"X\\\" # button (re)load path</p> <p>RESET_ORIGIN_BTN = \\\"B\\\" # button to press to move car back to origin</p> <p>ERASE_PATH_BTN = \\\"Y\\\" # button to erase path</p> <p>TOGGLE_RECORDING_BTN = \\\"L1\\\" # button to toggle recording mode</p> <p>INC_PID_D_BTN = None # button to change PID \\'D\\' constant by PID_D_DELTA</p> <p>DEC_PID_D_BTN = None # button to change PID \\'D\\' constant by -PID_D_DELTA</p> <p>INC_PID_P_BTN = \\\"None\\\" # button to change PID \\'P\\' constant by PID_P_DELTA</p> <p>DEC_PID_P_BTN = \\\"None\\\" # button to change PID \\'P\\' constant by -PID_P_DELTA</p> <p>#</p> <ol> <li> <p>Recording a path</p> <p>a.  The algorithm assumes we will be driving in a continuous     connected path such that the start and end are the same. You can     adjust the space between recorded waypoints by editing the     PATH_MIN_DIST value in myconfig.py You can change the name and     location of the saved file by editing the PATH_FILENAME value.</p> <p>b.  Enter User driving mode using either the web controller or a     game controller.</p> <p>c.  Move the car to the desired starting point</p> <p>d.  Erase the path in memory (which will also reset the origin).</p> <pre><code>i.  Make sure to reset the origin!!! If you didn't need to erase\n    the path in memory you can just\n</code></pre> <p>e.  Toggle recording on.</p> <p>f.  Drive the car manually around the track until you reach the     desired starting point again.</p> <p>g.  Toggle recording off.</p> <p>h.  If desired, save the path.</p> </li> <li> <p>Following a path</p> <p>a.  Enter User driving mode using either the web controller or a     game controller.</p> <p>b.  Move the car to the desired starting point - make sure it's the     same one from when you recorded the path</p> <p>c.  Reset the origin (be careful; don\\'t erase the path, just reset     the origin).</p> <p>d.  Load the path</p> <p>e.  Enter Autosteering or Autopilot driving mode. This is normally     done by pressing the start button either once or twice If you     are in Autosteering mode you will need to manually provide     throttle for the car to move. If you are in Autopilot mode the     car should drive itself completely.</p> </li> <li> <p>Configuring Path Follow Parameters</p> <p>a.  So the algorithm uses the cross-track error between a desired     line and the vehicle\\'s measured position to decide how much and     which way to steer. But the path we recorded is not a simple     line; it is a lot of points that is typically some kind of     circuit. As described above, we use the vehicle\\'s current     position to choose a short segment of the path that we use as     our desired track. That short segment is recalculated every time     we get a new measured car position. There are a few     configuration parameters that determine exactly which two points     on the path that we use to calculate the desired track line.</p> <pre><code>i.  PATH_SEARCH_LENGTH = None \\# number of points to search for\n    closest point, None to search entire path\n\nii. PATH_LOOK_AHEAD = 1 \\# number of points ahead of the closest\n    point to include in cte track\n\niii. PATH_LOOK_BEHIND = 1 \\# number of points behind the closest\n     point to include in cte track\n</code></pre> <p>b.  Generally, if you are driving very fast you might want the look     ahead to be larger than if driving slowly so that your steering     can anticipate upcoming curves. Increasing the length of the     resulting track line, by increasing the look behind and/or look     ahead, also acts as a noise filter; it smooths out the track.     This reduces the amount of jitter in the controller. However,     this must be balanced with the true curves in the path; longer     track segments effectively \\'flatten\\' curves and so can result     in understeer; not steering enough when on a curve.</p> </li> <li> <p>Determining PID Coefficients</p> <p>a.  The PID coefficients are the most important (and time consuming)     parameters to configure. If they are not correct for your car     then it will not follow the path. The coefficients can be     changed by editing their values in the myconfig.py file.</p> <p>b.  PID_P is the proportional coefficient; it is multiplied with the     cross-track error. This is the most important parameter; it     contributes the most to the output steering value and in some     cases may be all that is needed to follow the line. If this is     too small then car will not turn enough when it reaches a curve.     If this to too large then it will over-react to small changes in     the path and may start turning in circles; especially when it     gets to a curve.</p> <p>c.  PID_D is the differential coefficient; it is multiplied with the     change in the cross-track error. This parameter can be useful in     reducing oscillations and overshoot.</p> <p>d.  PID_I is the integral coefficient; it is multiplied with the     total accumulated cross-track error. This may be useful in     reducing offsets caused by accumulated error; such as if one     wheel is slightly smaller in diameter than another.</p> <p>e.  Determining PID Coefficients can be difficult. One approach is:</p> <pre><code>i.  First determine the P coefficient.\n\nii. zero out the D and the I coefficients.\n\niii. Use a kind of \\'binary\\' search to find a value where the\n     vehicle will roughly follow a recorded straight line;\n     probably oscillating around it. It will be weaving\n\niv. Next find a D coefficient that reduces the weaving\n    (oscillations) on a straight line. Then record a path with a\n    tight turn. Find a D coefficient that reduces the overshoot\n    when turning.\n\nv.  You may not even need the I value. If the car becomes\n    unstable after driving for a while then you may want to\n    start to set this value. It will likely be much smaller than\n    the other values.\n\nvi. Be patient. Start with a reasonably slow speed. Change one\n    thing at a time and test the change; don\\'t make many\n    changes at once. Write down what is working.\n\nvii. Once you have a stable PID controller, then you can figure\n     out just how fast you can go with it before autopilot\n     becomes unstable. If you want to go faster then set the\n     desired speed and start tweaking the values again using the\n     method suggested above.\n</code></pre> </li> </ol>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_16","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_17","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_18","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_19","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_20","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_21","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_22","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_23","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#driving-the-robot-to-collect-data","title":"Driving the Robot to Collect data","text":"<p>Remember you are driving at max of x (0.x) Throttle power based on the</p> <p>myconfig.py that you edited.</p> <p>The robot is not controlling speed but power given the motor using PWM values</p> <p>Transmitted from the Single Board Computer (SBC) like a Jetson Nano (JTN) to the</p> <p>Electronic Speed Controller (ESC).</p> <p>To reverse you may have to reverse, stop, reverse. This is a feature of some</p> <p>ESCs used in RC cars to prevent damaging gears when changing from forward to reverse.</p> <p>On the JTN</p> <p>If you are not changing directory automatically when the user logs in</p> <p>cd \\~/projects/d3</p> <p>(env) jetson@ucsdrobocar00:\\~/projects/d3 \\$</p> <p>python manage.py drive</p> <p>Note: CTRL-C stop the manage.py drive</p> <p>---</p> <p>If you get an error on the joystick or Donkey stops loading the JoyStick</p> <p>it is because game controller is off</p> <p>or not connected/paired with the JTN</p> <p>ls</p> <p>config.py data logs manage.py models</p> <p>ls data</p> <p>tub_1_17-10-13</p> <p>If you want to wipe clean the data collected</p> <p>Remove the content of the \\~/projects/d3/data directory. It should be tub_......</p> <p>You can delete the entire directory then create it again.</p> <p>\\~/projects/d3 \\$</p> <p>rm -rf data</p> <p>mkdir data</p> <p>Follow the Donkey Docs to install the Donkey AI framework into your PC [http://docs.donkeycar.com]{.underline}</p> <p>On the PC</p> <p>Activate the virtual environment. I am assuming you have your virtual environment under</p> <p>\\~/projects/envs/donkey</p> <p>source \\~/projects/envs/donkey/bin/activate</p> <p>cd projects</p> <p>cd d3</p> <p>SSH into the JTN</p> <p>ssh jetson@ucsdrobocar00.local</p> <p>On the JTN</p> <p>If you are not changing directory automatically when the user logs in</p> <p>cd \\~/projects/d3</p> <p>(env) jetson@ucsdrobocar00:\\~/projects/d3 \\$</p> <p>python manage.py drive</p> <p>Note: CTRL-C stop the manage.py drive</p> <p>Drive the robot to collect data</p> <p>On the PC</p> <p>Get data from JTN</p> <p>Transfer all data from the JTN to the PC and delete data that was deleted from the JTN</p> <p>rsync -a --progress --delete jetson@ucsdrobocar00.local:\\~/d3/data \\~/projects/d3</p> <p>ls data</p> <p>tub_1_17-10-12</p> <p>Train model on all data (Tubes)</p> <p>python train.py --model=models/date_name.h5</p> <p>To train using a particular tube</p> <p>python train.py --tub \\~/projects/d3/data/tub_1_18-01-07 --model=models/model_name.h5</p> <p>To make an incremental training using a previous model</p> <p>python train.py --tub \\~/projects/d3/data/NAME_OF_NEW_TUBE --transfer=models/NAME_OF_PREVIOUS_MODEL.h5 --model=models/NAME_OF_NEW_MODEL.h5</p> <p>On your personal PC (Not required, only if you installed on your personal computer)</p> <p>clean-up tubs removing possible bad data</p> <p>\\~/projects/d3</p> <p>donkey tubclean data</p> <p>using donkey ...</p> <p>Listening on 8886...</p> <p>Open a browser and type</p> <p>[http://localhost:8886]{.underline}</p> <p>{width=\"4.807292213473316in\" height=\"2.717165354330709in\"}</p> <p>You can clean-up your tub directories. Please make a backup of your data before you start</p> <p>to clean it up.</p> <p>On the mac if the training complains</p> <p>rm \\~/projects/d2t/data/.DS_Store</p> <p>If it complains about docopt, install it again. And I did not change anything</p> <p>from the previous day. Go figure...</p> <p>(env) jack@lnxmbp01:\\~/projects/d2\\$ pip list</p> <p>(env) jack@lnxmbp01:\\~/projects/d2\\$ pip install docopt</p> <p>See the models here</p> <p>\\~/projects/d3</p> <p>ls models</p> <p>[ucsd_12oct17.h5]{.mark}</p> <p>Place Autopilot into RPI</p> <p>rsync -a --progress \\~/projects/d3/models/ jetson@ucsdrobocar00:\\~/projects/d3/models/</p> <p>At JTN</p> <p>ls models</p> <p>Ucsd_12oct17.h5</p> <p>On the JTN</p> <p>Run AutoPilot at the RPI</p> <p>python manage.py drive --model=./models/ucsd_12oct17.h5</p> <p>...</p> <p>Using TensorFlow backend.</p> <p>loading config file: /home/pi/d2/config.py</p> <p>config loaded</p> <p>PiCamera loaded.. .warming camera</p> <p>Starting vehicle...</p> <p>/home/pi/env/lib/python3.4/site-packages/picamera/encoders.py:544: PiCameraResolutionRounded: frame size rounded up from 160x120 to 160x128</p> <p>width, height, fwidth, fheight)))</p> <p>END of DonkeyCar AI Framework Instructions</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#remote-desktop-installation","title":"Remote Desktop Installation","text":"<p>Remote Access to the SBC Graphical User Interface (GUI)</p> <p>Lets install a remote desktop server on the SBC and a client on your computer</p> <p>We will be using [NoMachine]{.underline}</p> <p>[Here is a link to the instructions]{.underline} to install NOMACHINE on a Single Board Computers base</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#backup-of-the-usd-card","title":"Backup of the uSD Card","text":"<p>Once you finished the configuration of your SBC, why not making another backup of the uSD card</p> <p>(or first one if you have not done it yet)</p> <p>It can save you lots of time during a recovery process. In case of a crash, you will only need to</p> <p>restore the image vs. install all over again.</p> <p>Backup the uSD card following these steps in a Linux machine</p> <p>Eject the uSD card from your SBC, plug it into a linux PC using a uSD adapter</p> <p>sudo fdisk -l</p> <p>Disk /dev/sda: 59.6 GiB, 64021856256 bytes, 125042688 sectors</p> <p>Units: sectors of 1 * 512 = 512 bytes</p> <p>Sector size (logical/physical): 512 bytes / 512 bytes</p> <p>I/O size (minimum/optimal): 512 bytes / 512 bytes</p> <p>Disklabel type: gpt</p> <p>Disk identifier: D048AD43-24FD-4DED-B06E-7BB8ED98158C</p> <p>Device Start End Sectors Size Type</p> <p>/dev/sda1 24576 125042654 125018079 59.6G Linux filesystem</p> <p>/dev/sda2 2048 2303 256 128K Linux filesystem</p> <p>/dev/sda3 4096 4991 896 448K Linux filesystem</p> <p>/dev/sda4 6144 7295 1152 576K Linux filesystem</p> <p>/dev/sda5 8192 8319 128 64K Linux filesystem</p> <p>/dev/sda6 10240 10623 384 192K Linux filesystem</p> <p>/dev/sda7 12288 13439 1152 576K Linux filesystem</p> <p>/dev/sda8 14336 14463 128 64K Linux filesystem</p> <p>/dev/sda9 16384 17663 1280 640K Linux filesystem</p> <p>/dev/sda10 18432 19327 896 448K Linux filesystem</p> <p>/dev/sda11 20480 20735 256 128K Linux filesystem</p> <p>/dev/sda12 22528 22687 160 80K Linux filesystem</p> <p>On my case, the 64G uSD was mounted on /dev/sda</p> <p>example on the command line for making an image of the uSD card mounted</p> <p>on a Linux machines as /dev/sda</p> <p>sudo dd bs=4M if=/dev/sda of=ucsd_robocar_image_25sep19.img status=progress</p> <p>Lets compress the image using Zip</p> <p>zip ucsd_robocar_image_25sep19.zip ucsd_robocar_image_25sep19.img</p> <p>Example using MacOS</p> <p>diskutil list</p> <p>/dev/disk6 (external, physical):</p> <p>: TYPE NAME SIZE IDENTIFIER</p> <p>0: GUID_partition_scheme *128.0 GB disk6</p> <p>1: Linux Filesystem 127.6 GB disk6s1</p> <p>2: Linux Filesystem 67.1 MB disk6s2</p> <p>3: Linux Filesystem 67.1 MB disk6s3</p> <p>4: Linux Filesystem 458.8 KB disk6s4</p> <p>5: Linux Filesystem 458.8 KB disk6s5</p> <p>6: Linux Filesystem 66.1 MB disk6s6</p> <p>7: Linux Filesystem 524.3 KB disk6s7</p> <p>8: Linux Filesystem 262.1 KB disk6s8</p> <p>9: Linux Filesystem 262.1 KB disk6s9</p> <p>10: Linux Filesystem 104.9 MB disk6s10</p> <p>11: Linux Filesystem 134.2 MB disk6s11</p> <p>sudo dd if=/dev/disk6 of=ucsdrobocar-xxx-yy-v1.0.img</p> <p>Alternatively you can use MS Windows, use a software called Win32. [Search the web]{.underline}</p> <p>[for instructions on using Win32]{.underline}</p> <p>\"</p> <p>Using Windows</p> <p>Once you open Win32 Disk Imager, use the blue folder icon to choose the location and the name of the backup you want to take, and then choose the drive letter for your SD card. Click on the Read button. The card will then be backed up to your PC.Mar 18, 2015</p> <p>[Backing up and Restoring your Raspberry Pi\\'s SD Card-- The ...]{.underline}</p> <p>\"</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#if-needed-we-have-an-jtn-usd-card-image-ready-for-plug-and-play","title":"If needed, we have an JTN uSD Card Image Ready for Plug and Play","text":"<p>If you run out of time trying to make the compilation and build OpenCV, and install the Donkey AI</p> <p>framework, here is a link for a 64G Bytes uSDcard that is ready to go. [Get recovery]{.underline} [image here]{.underline}. []{.mark}</p> <p>The recovery image already has all the software installed.</p> <p>Connect a uUSB cable between the PC and the JTN, or connect the JTN to the access point</p> <p>using a network cable.</p> <p>Boot the JTN, wait 1\\~2 minutes for the software to finish loading</p> <p>You can ssh to the JTN</p> <p>ssh jetson@192.168.55.1</p> <p>or</p> <p>ssh jetson@ucsdrobocar-xxx-yy.local</p> <p>enter password</p> <p>jetsonucsd</p> <p>You need to change the host name and change the default password</p> <p>Connect the JTN to a WiFi network</p> <p>see steps earlier in this document</p> <p>ex:</p> <p>sudo nmcli device wifi connect UCSDRoboCar5GHz password UCSDrobocars2018</p> <p>shutdown the JTN</p> <p>sudo shutdown now</p> <p>If you are using the uUSB cable, remove it from the JTN</p> <p>Power on the JTN with the provided 5V power supply</p> <p>Wait 1\\~2 minutes for the JTN to complete the boot process</p> <p>Then use SSH to connect to the JTN</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#_24","title":"10docoriginal","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#ros-with-docker","title":"ROS with Docker","text":"<p>[Here is the link for getting your robot set up with ROS using our docker images]{.underline}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#supporting-material","title":"Supporting material","text":"<p>How to show what Linux we have installed</p> <p>dmesg | head -1</p> <p>How to show the distribution we are running</p> <p>lsb_release -a</p> <p>This session is in-process, just a placeholder for now</p> <p>Jetson Nano ROS based on NVIDIA</p> <p>[https://github.com/dusty-nv/jetbot_ros]{.underline}</p> <p>Here is what I changed on myconfig.py</p> <p>CAMERA_TYPE = \\\"WEBCAM\\\" (PICAM|WEBCAM|CVCAM|CSIC|V4L|MOCK)</p> <p>PCA9685_I2C_BUSNUM = 1 None will auto detect, which is fine on the pi. But other platforms should specify the bus number.</p> <p>STEERING_CHANNEL = 1 channel on the 9685 pwm board 0-15</p> <p>STEERING_LEFT_PWM = 290 pwm value for full left steering</p> <p>STEERING_RIGHT_PWM = 490 pwm value for full right steering</p> <p>THROTTLE_CHANNEL = 2 channel on the 9685 pwm board 0-15</p> <p>THROTTLE_FORWARD_PWM = 490 pwm value for max forward throttle</p> <p>THROTTLE_STOPPED_PWM = 380 pwm value for no movement</p> <p>THROTTLE_REVERSE_PWM = 300 pwm value for max reverse throttle</p> <p>USE_JOYSTICK_AS_DEFAULT = True when starting the manage.py, when True, will not require a --js option to use the joystick</p> <p>CONTROLLER_TYPE=\\'F710\\' (ps3|ps4|xbox|nimbus|wiiu|F710)</p> <p>JOYSTICK_DEADZONE = 0.01 when non zero, this is the smallest throttle before recording triggered.</p> <p>--</p> <p>--</p> <p>To use TensorflowRT on the JetsonNano</p> <p>http://docs.donkeycar.com/guide/robot_sbc/tensorrt_jetson_nano/</p> <p>Updating the Donkey AI framework and or using the master release fork</p> <p>\"</p> <p>Tawn 2:47 PM</p> <p>3.1.0 released now. TensorRT support on Nano! TFlite support (w TF 2.0+). Better support for cropping across more tools. Please update like:</p> <p>cd projects/donkeycar</p> <p>git checkout master</p> <p>git pull</p> <p>pip install -e .[pi] or .[nano] or .[pc] depending where you are installing it</p> <p>cd \\~/mycar</p> <p>donkey update</p> <p>Important Note: Your old models will not work with the new code. We\\'ve changed how cropping and normalization work to support TensorRT/TFLite. So please, RE-TRAIN your models after updating to\\ \\ docs to help get you started w TensorRT: https://docs.donkeycar.com/guide/robot_sbc/tensorrt_jetson_nano/</p> <p>Installing TensorRT on Ubuntu18.04</p> <p>After installing Tensorflow on your virtual environment</p> <p>ex: conda install tensorflow-gpu==1.13.1</p> <p>Testing the Tensorflow Install</p> <p>python</p> <p>Enter the following txt, you can cut and paste</p> <p>Python</p> <p>import tensorflow as tf</p> <p>hello = tf.constant(\\'Hello, TensorFlow!\\')</p> <p>sess = tf.Session()</p> <p>print(sess.run(hello))</p> <p>[https://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.htmlinstalling-tar]{.underline}</p> <p>[https://developer.nvidia.com/nvidia-tensorrt-5x-download]{.underline}</p> <p>{width=\"1.9798939195100613in\" height=\"1.828125546806649in\"}</p> <p>{width=\"3.5781255468066493in\" height=\"0.36722878390201225in\"}</p> <p>Open a terminal window and navigate to the directory where you saved the file</p> <p>Now let's expand the file</p> <p>{width=\"6.875in\" height=\"0.4444444444444444in\"}</p> <p>Note that the file above expects to work with cudnn7.5</p> <p>The Cuda install will need to match that version, cudnn7.5</p> <p>e.g. \\~/projects/TensorRT-5.1.5.0/</p> <p>cd \\~/projects/TensorRT-5.1.5.0</p> <p>tar xzvf TensorRT-5.1.5.0.Ubuntu-18.04.2.x86_64-gnu.cuda-10.1.[cudnn7.5]{.mark}.tar.gz</p> <p>export LD_LIBRARY_PATH=\\$LD_LIBRARY_PATH:\\~/projects/TensorRT-5.1.5.0/lib</p> <p>Activate the virtual environment you want TensorflowRT installed</p> <p>e.g. source \\~/projects/env/bin/activate</p> <p>cd python</p> <p>If using python 3.6</p> <p>pip install tensorrt-5.1.5.0-cp36-none-linux_x86_64.whl</p> <p>or if using python3.7</p> <p>pip install tensorrt-5.1.5.0-cp37-none-linux_x86_64.whl</p> <p>cd ..</p> <p>cd uff</p> <p>pip install uff-0.6.3-py2.py3-none-any.whl</p> <p>which convert-to-uff</p> <p>/home/jack/projects/env/bin/convert-to-uff</p> <p>or</p> <p>which convert-to-uff</p> <p>/home/jack/miniconda3/envs/donkey/bin/convert-to-uff</p> <p>cd ..</p> <p>cd graphsurgeon</p> <p>pip install graphsurgeon-0.4.1-py2.py3-none-any.whl</p> <p>cd..</p> <p>apt-get install tree</p> <p>tree -d</p> <p>dpkg -l | grep nvinfer</p> <p>Testing TensorRT</p> <p>python</p> <p>import tensorrt as trt</p> <p>exit()</p> <p>Example after training a model</p> <p>You end up with a Linear.h5 in the models folder</p> <p>python manage.py train --model=./models/Linear.h5 --tub=./data/tub_1_19-06-29,...</p> <p>Freeze model using freeze_model.py in donkeycar/scripts</p> <p>The frozen model is stored as protocol buffers.</p> <p>This command also exports some metadata about the model which is saved in ./models/Linear.metadata</p> <p>python freeze_model.py --model=./models/Linear.h5 --output=./models/Linear.pb</p> <p>Convert the frozen model to UFF. The command below creates a file ./models/Linear.uff</p> <p>convert-to-uff ./models/Linear.pb</p> <p>cd \\~/projects/d3_trax_rally</p> <p>python \\~/projects/donkeycar/scripts/freeze_model.py --model=./models/24aug19_ucsd_booker_linear_1_320_240.h5 --output=./models/24aug19_ucsd_booker_linear_1_320_240.pb</p> <p>convert-to-uff ./models/24aug19_ucsd_booker_linear_1_320_240.pb</p> <p>For the RPI</p> <p>If you would like to try tflite support, you will need a newer version of Tensorflow on the pi. You can download and install this version:</p> <p>wget https://tawn-train.s3.amazonaws.com/tf/tensorflow-2.0.0a0-cp35-cp35m-linux_armv7l.whl</p> <p>pip install tensorflow-2.0.0a0-cp35-cp35m-linux_armv7l.whl</p> <p>Your host PC can stay at TF 1.13.1</p> <p>to train a TensorRT model, during training add the arg</p> <p>--type=tensorrt_linear</p> <p>And for TFlite support add the arg</p> <p>--type=tflite_linear</p> <p>Also use this flag when running your model on the car. (edited)</p> <p>\"</p> <p>PS3 Controller Modes</p> <p>This is similar for the Logitech wireless controllers</p> <p>The default mode will be that [User]{.mark} is in Control. That is, the user controls Steering and Throttle.</p> <p>To switch to [Local Angle]{.mark} (software controls the Steering and uses the Throttle), you need to press the \\&lt;Select&gt; button in the Joystick.</p> <p>If you give Throttle the Robocar should drive around semi-autonomously.</p> <p>After few laps that you see that your model is good,</p> <p>[Please hold your robot with the wheels out of the floor]{.mark}</p> <p>you can press the \\&lt;Start&gt; button [and immediately press the \\&lt;left_DOWN_arrow&gt; button a few times to decrease the Throttle as needed.]{.mark} This is important so you slow down the Robocar for a constant Throttle.</p> <p>Press the \\&lt;left_UP_arrow&gt; to give it more Throttle as needed.</p> <p>Pressing \\&lt;[X&gt;]{.mark} will stop the robocar and go back to User mode (user is in control)</p> <p>You can change the driving modes by pressing the \\&lt;Select&gt; button. You should be able to see a message on your computer terminal that is SSH connected to the RoboCar RPI.</p> <p>The Local &amp; Angle mode (fully autonomous) is to be used after you see that you can do few laps with local angle</p> <p>Hit the Select button to toggle between three modes - User, Local Angle, and Local Throttle &amp; Angle.</p> <ul> <li> <p>User - User controls both steering and throttle with joystick</p> </li> <li> <p>Local Angle - Ai controls steering. User controls the throttle.</p> </li> <li> <p>Local Throttle &amp; Angle - Ai controls both steering and throttle</p> </li> </ul> <p>When the car is in Local Angle mode, the NN will steer. You must provide throttle...</p> <p>Ideally you will have \\~ 60 laps</p> <p>If you don't have a good working Auto-Pilot, get more data in 10 laps increments.</p> <p>In summary, you may want to start with 60 laps and then do 10\\~20 laps more to see if the model gets better.</p> <p>I would not worry much about a few bad spots when collecting data. Drive the car back to the track,</p> <p>then press Green_Triangle to delete the last 5s of data.</p> <p>Keep driving, you will develop good skills, you will get good data and better models. If you leave the track, just drive the RoboCar back to track. It may even learn how to get back to track.</p> <p>If you keep the data from the same track (ex: UCSD Track) in the d2t/data directory, as you add more files to it (e.g., tub_[5]{.mark}_17-10-13) it will help your model. At the same time it will take more time to train since your model will read all the data sets in the directory. You can use the transfer model to add new data to a current model.</p> <p>Incremental training using a previous model</p> <p>python train.py --tub \\~/projects/d2t/data/NAME_OF_NEW_TUBE_DATA --transfer=models/NAME_OF_PREVIOUS_MODEL.h5 --model=models/NAME_OF_NEW_MODEL.h5</p> <p>Some Advanced Tools</p> <p>The visualization tool is to be used on your PC. Please even if you can, please do not use the GPU Cluster Resources for this.</p> <p>https://docs.google.com/presentation/d/1oOF9qHh6qPwF-ocOwGRzmLIRXcIcguDFwa7x9EicCo8/editslide=id.g629a9e24fa_0_1149</p> <p>{width=\"6.875in\" height=\"4.083333333333333in\"}</p> <p>Visualizing the model driving the car vs. human driver</p> <p>Install OpenCV</p> <p>sudo apt-get install python-opencv</p> <p>pip3 install opencv-python</p> <p>donkey makemovie --tub=data\\tub_file --model=models\\model_name.h5 --limit=100 --salient --scale=2</p> <p>example</p> <p>donkey makemovie --tub=data/tub_9_19-01-19 --model=models/19jan19_oakland_5.h5 --start 1 --end 1000 --salient --scale=2</p> <p>tub_3_20-03-07</p> <p>07mar20_circuit_320x180_3.h5</p> <p>donkey makemovie --tub=data/tub_3_20-03-07 --model=models/07mar20_circuit_320x180_3.h5 --type=linear --start 1 --end 1000 --salient --scale=2</p> <p>[https://devtalk.nvidia.com/default/topic/1051265/jetson-nano/remote-access-to-jetson-nano/]{.underline}</p> <p>If you\\'re on the same wired local network, ssh -X works, but you should be aware that the graphics are being rendered on your local X server in such a case, so if you launch an OpenGL game for example, it\\'ll use your local virtual graphics hardware, which means your local *CPU* in most cases. Cuda, however, will be done on the nano.</p> <p>To launch a program remotely from a linux computer:</p> <p>ssh -X some_user@test-jetson -C gedit</p> <p>ssh -X ubuntu@tegra-ubuntu nautilus</p> <p>Where `gedit` is the program you wish to run. You can omit -C get straight to a ssh prompt with X support. Any graphical applications you launch will pop up on your screen automatically.</p> <p>Please note that X does not need to be running on the Nano, so if you want to save a whole bunch of memory while working remotely you can run `sudo systemctl isolate multi-user.target` to temporarily shut down the graphical environment on the Nano itself.</p> <p>To remotely access from windows, here are instructions on how to set up an X server on windows and connect it to Putty, but please note those instructions have an old download link. A new one is here.</p> <p>Raspberry PI (RPI or PI) Configuration</p> <p>04Nov19 - Added a link to a plug and play image</p> <p>03Nov19 - Updated instruction for Tensorflow 2.0</p> <p>28Aug19 - updated the instructions to include Raspberry PI 4B</p> <p>08Aug19 - Since we are using the Nvidia Jetson Nano, I am no longer maintaining the RPI</p> <p>instructions. If you got to this point and is using Raspberry PIs, please check the</p> <p>Donkey Docs for the latest updates... [http://docs.donkeycar.com]{.underline}</p> <p>In general if you are using a Linux distribution like Ubuntu in this course it will make</p> <p>your life is much easier. Initially, you will need access to a Linux to modify some files from</p> <p>a uSD card. You can ask a colleague, TA, or course instructor for help.</p> <p>Moreover, if you use Linux it will be another entry into your resume. Give it a try. You can</p> <p>make a dual boot in your computer or have a virtual machine, [VirtualBox is free]{.underline}.</p> <p>-------------</p> <p>We will start with an Operating System Image called Raspbian (Raspberry Debian ...)</p> <p>You can use your favorite disk image writer to have the disk image written to the uSD.</p> <p>The uSD card to be used on the RPI. Note, this is not a regular file copy operation.</p> <p>You can use Etcher [https://etcher.io/]{.underline}</p> <p>From your PC let\\'s prepare the Raspberry PI (RPI) uSD card</p> <p>Make sure your computer can access the Internet</p> <p>If you are using one of our WiFi Access Points in one of the labs or at one of the tracks, the first PC</p> <p>that connects to the WiFi Access point will need to accept the UCSD Wireless Visitor Agreement,</p> <p>just like when connecting directly to UCSD's Visitor WiFi.</p> <p>Get the Raspbian Lite uSD image [here.]{.underline}</p> <p>Etcher can use Zipped files, you don't need to Unzip the image file. If you are using Linux command</p> <p>lines to write the disk image to a uSD card you may need to extract the file first.</p> <p>This OS (disk) image is based on the Raspbian headless (no GUI). We will use command line on</p> <p>the RPI, to get to the RPI we will be using SSH (secure shell). Don't worry, these are just command</p> <p>line names. Mastering these will be good skills to have.</p> <p>If you don't know about SSH and the command line in Linux, you will learn enough in this course.</p> <p>Let\\'s write the disk image into the uSD Card</p> <p>Connect the provided uSD adapter into your PC</p> <p>Insert the provided uSD card (64 Gbytes) into the uSD adapter</p> <p>Install and run Etcher [https://etcher.io/]{.underline}</p> <p>Start Etcher, chose the Zipped file with the Disk Image you downloaded,</p> <p>pay attention when choosing the drive with the uSD card on it (e.g., 64 Gbytes)</p> <p>write the image to uSD card.</p> <p>If you are using Linux, after you finish writing the disk image to the uSD card</p> <p>you should see two partitions in your computer file system</p> <p>[boot and rootfs]{.mark}</p> <p>[If you are not seeing these partitions in your computer, try removing the uSD card from]{.mark}</p> <p>[your computer then insert it again. If that does not work, try using a]{.mark}</p> <p>[computer running Linux.]{.mark}</p> <p>Prepare the network configuration file.</p> <p>We will create a text file with the WiFi configurations.</p> <p>You can use the nano or gedit on Linux (e.g.,Ubuntu).</p> <p>On other OSes make sure the file you are creating is a plain text file.</p> <p>Don't use MS Word or other Apps that may save the file with different file formats</p> <p>and hidden text format characters.</p> <p>Note: If you are using a MAC with the SD to uSD adapter, MAC OS may not be able to</p> <p>write the Donkey Image disk \"boot\". Use the USB uSD card adapter provider for each</p> <p>Team.</p> <p>At the root partition of the uSD card, will name the file as wpa_supplicant.conf</p> <p>Using Linux or a Mac terminal, the command line you will use is</p> <p>sudo nano wpa_supplicant.conf</p> <p>Navigate to the drive that you created the RPI image.</p> <p>Look for a boot partition</p> <p>Edit and save the file with this name wpa_supplicant.conf</p> <p>Open a terminal ex:</p> <p>cd /media/UserID/boot</p> <p>ex:</p> <p>cd /media/jack/boot</p> <p>sudo nano wpa_supplicant.conf</p> <p>or</p> <p>sudo nano /media/UserID/boot/wpa_supplicant.conf</p> <p>You will need to enter your password</p> <p>ex: nano /media/jack/boot/wpa_supplicant.conf</p> <p>on a mac</p> <p>cd /Volumes/boot</p> <p>nano wpa_supplicant.conf</p> <p>Here is the content of wpa_supplicant.conf</p> <p>You can copy and paste it</p> <p>ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev</p> <p>update_config=1</p> <p>country=US</p> <p>network={</p> <p>ssid=\\\"SD-DIYRoboCar5GHz\\\"</p> <p>key_mgmt=WPA-PSK</p> <p>psk=\\\"SDrobocars2017\\\"</p> <p>priority=100</p> <p>id_str=\\\"SD-DIYRoboCars5GHz\\\"</p> <p>}</p> <p>network={</p> <p>ssid=\\\"UCSDRoboCar5GHz\\\"</p> <p>key_mgmt=WPA-PSK</p> <p>psk=\\\"UCSDrobocars2018\\\"</p> <p>priority=90</p> <p>id_str=\\\"ucsdrobocar5G\\\"</p> <p>}</p> <p>network={</p> <p>ssid=\\\"SD-DIYRoboCar\\\"</p> <p>key_mgmt=WPA-PSK</p> <p>psk=\\\"SDrobocars2017\\\"</p> <p>priority=80</p> <p>id_str=\\\"SD-DIYRoboCars2.4GHz\\\"</p> <p>}</p> <p>network={</p> <p>ssid=\\\"UCSDRoboCar\\\"</p> <p>key_mgmt=WPA-PSK</p> <p>psk=\\\"UCSDrobocars2018\\\"</p> <p>priority=70</p> <p>id_str=\\\"ucsdrobocar2.4GHz\\\"</p> <p>}</p> <p>[If you are using a PC running Linux to edit the file in a later time,]{.mark}</p> <p>[place the uSD card into the PC]{.mark}</p> <p>[sudo nano /media/user_id/rootfs/etc/wpa_supplicant/wpa_supplicant.conf]{.mark}</p> <p>On first boot, this file will be moved to the path below where it may be edited later</p> <p>as needed.</p> <p>To edit current WiFi connections or to add WiFi Connections</p> <p>You will need to ssh to to RPI and edit the following file</p> <p>/etc/wpa_supplicant/wpa_supplicant.conf</p> <p>From a terminal where you SSH to the RPI</p> <p>sudo nano /etc/wpa_supplicant/wpa_supplicant.conf</p> <p>[Please change the hostname and the password of your RPI\\ so other teams don\\'t connect to your RPI by mistake.]{.mark}</p> <p>[Change the Hostname (HostID)]{.mark}</p> <p>[Using Linux you will be able to edit two files that are in the following directory]{.mark}</p> <p>[When using MacOS you may not see ext3/ext4 partitions that Linux (Raspbian) uses]{.mark}</p> <p>[hostname]{.mark}</p> <p>[and]{.mark}</p> <p>[hosts]{.mark}</p> <p>[Replace raspberrypi with ucsdrobocar-xxx-yy]{.mark}</p> <p>[The hostnames should follow ucsdrobocar-xxx-yy]{.mark}</p> <p>[xx= Team number]{.mark}</p> <p>[For team 07, replace raspberrypi by ucsdrobocar07]{.mark}</p> <p>[For COMOS]{.mark}</p> <p>[For team 07, replace raspberrypi by ucsdcosmos11t07]{.mark}</p> <p>[sudo nano /media/UserID/rootfs/etc/hostname\\ sudo nano /media/UserID/rootfs/etc/hosts\\ ]{.mark}</p> <p>[ex:]{.mark}</p> <p>[sudo nano /media/jack/rootfs/etc/hostname]{.mark}</p> <p>[sudo nano /media/jack/rootfs/etc/hosts\\ ]{.mark}</p> <p>[When editing from the RPI]{.mark}</p> <p>[sudo nano /etc/hostname]{.mark}</p> <p>[sudo nano /etc/hosts]{.mark}</p> <p>[If you have not changed the RPI host name, you need to make sure that only your RPI is]{.mark}</p> <p>[connected to the WiFi router until you can change its hostname and password.]{.mark}</p> <p>[Your computer and the RPI need to be on the same network so you can connect to it.]{.mark}</p> <p>[Said so, in our case we have 5GHz WiFi too, you should connect your computer to]{.mark}</p> <p>[the 5GHz Access Point. There is a bridge between]{.mark}</p> <p>[the 5GHz and 2.4GHz networks in the Access Points we use.]{.mark}</p> <p>[Therefore you computer will be able to connect to the]{.mark}</p> <p>[RPI.]{.mark}</p> <p>[Enable SSH on boot\\ Create a text file and name it ssh in the boot partition]{.mark}</p> <p>[On Linux]{.mark}</p> <p>[sudo nano /media/UserID/boot/ssh]{.mark}</p> <p>[ex: sudo nano /media/jack/boot/ssh]{.mark}</p> <p>[On MacOS]{.mark}</p> <p>[/Volumes/boot]{.mark}</p> <p>[You can add some characters into the file to force nano to write it.]{.mark}</p> <p>[Type any short text such as test]{.mark}</p> <p>[ex: 123]{.mark}</p> <p>[Close any terminal that may be using the uSD card]{.mark}</p> <p>[Use the eject equivalent function in your computer to eject the boot and roofs uSD]{.mark}</p> <p>[partitions]{.mark}</p> <p>[Remove the uSD card from your computer, carefully insert it into your RPI]{.mark}</p> <p>[Power-up your RPI. You can use a uUSB power adapter or plug it into a PC USB port.]{.mark}</p> <p>[Connecting to your RPI using SSH]{.mark}</p> <p>[SSH is installed by default on Linux, MacOS, and Windows 10.]{.mark}</p> <p>[On MS Windows 7 or older you will need to get software/app that supports SSH.]{.mark}</p> <p>[During the first boot it may take a few minutes for the RPI to be ready.]{.mark}</p> <p>[Watch for the RPI green LED flashing, it indicates uSD access.]{.mark}</p> <p>[During the first boot Raspbian extend the file system. It takes more time than the next\\ following OS boot.]{.mark}</p> <p>[To connect to your RPI you will need to run a computer terminal and issue a ssh command]{.mark}</p> <p>ssh pi@rpiname.local</p> <p>[ex: ssh pi@ucsdrobocar07.local]{.mark}</p> <p>[password: raspberry]{.mark}</p> <p>[If you have changed the hostname of your RPI or you can not see it in the network,]{.mark}</p> <p>[maybe because you have incorrect WiFi info, coordinate with other teams]{.mark}</p> <p>[that you are the only one doing the initial configuration while connected to the WiFi or direct]{.mark}</p> <p>[to the router using a network cable.]{.mark}</p> <p>[Otherwise you will be connecting to another Team's RPI.]{.mark}</p> <p>[Connect a network cable to your RPI and the WiFi access point. Boot it...]{.mark}</p> <p>[Alternatively, remove the uSD card and put it back in your PC. Read on the net about]{.mark}</p> <p>[Raspbian / Linux (Debian) on changing hostname at]{.mark}</p> <p>[the OS level at the uSD card on your PC before placing it in RPI.]{.mark}</p> <p>[Hint]{.mark}</p> <p>[sudo nano]{.mark}</p> <p>[/etc/hostname\\ /etc/hosts\\ ]{.mark}</p> <p>[Please change the hostname and the password so other]{.mark}</p> <p>[teams don\\'t work on your RPI.]{.mark}</p> <p>[Again]{.mark}</p> <p>[If you are having difficulties connecting to you RPI using WiFi. Connect the RPI to the]{.mark}</p> <p>[router using a network cable (e.g., CAT5).]{.mark}</p> <p>[We should have at least one network cable in lab.]{.mark}</p> <p>SSH into the RPI</p> <p>ssh pi@ucsdrobocar-xxx-yy.local</p> <p>Lets update the repository content and upgrade the raspbian</p> <p>(RPI Linux OS based on Debian)</p> <p>If you did not configure the RPI WiFI correctly it will not connect to</p> <p>your WiFi Access Point (AP)</p> <p>If needed</p> <p>edit the wpa_supplicant.conf file</p> <p>sudo nano /etc/wpa_supplicant/wpa_supplicant.conf</p> <p>SSH into the RPI</p> <p>ssh pi@ucsdrobocar-xxx-yy.local</p> <p>Check that the date and time is correct at the RPI and instal ntp to auto-update date and time</p> <p>date</p> <p>Hint. In the case the clock on the RPI is not updating automatically by getting information</p> <p>from the Internet, you need to set the date and time manually or updates and install may fail</p> <p>Here is how you can do it</p> <p>ex: sudo date -s \\'2018-10-04 09:30:00\\'</p> <p>Once you set the country/location, and have an active Internet access,</p> <p>these should force a clock synchronization on boot/reboot</p> <p>Initially it may be off because you have not set the time zone yet. e.g. Pacific Time</p> <p>sudo apt-get update</p> <p>sudo apt-get install ntp</p> <p>sudo /etc/init.d/ntp stop</p> <p>sudo ntpd -q -g\\ sudo /etc/init.d/ntp start</p> <p>Assuming you RPI is connected to the Internet, you can always use</p> <p>sudo ntpd -q -g</p> <p>to get the time from a server in the Internet</p> <p>Reboot to check if the date / times are updating correctly</p> <p>sudo reboot now</p> <p>SSH into the RPI</p> <p>Check that the date and time are correct</p> <p>date</p> <p>Be patient, this may take a while depending on how many updates were released</p> <p>after the uSD card image that you are using</p> <p>sudo apt-get update</p> <p>sudo apt-get upgrade</p> <p>[then reboot the RPI]{.mark}</p> <p>[sudo reboot now]{.mark}</p> <p>[Connect to the RPI\\ As applicable, once you connect to your RPI run the following command]{.mark}</p> <p>[sudo raspi-config]{.mark}</p> <p>[to change the host name and other things.]{.mark}</p> <p>[On the RPI use this command]{.mark}</p> <p>[ex: For team 07, use hostname ucsdrobocar07]{.mark}</p> <p>[Change the password]{.mark}</p> <p>[sudo raspi-config]{.mark}</p> <p>{width=\"6.875in\" height=\"4.097222222222222in\"}</p> <p>User Password, Hostname (if needed), enable camera, enable I2C,</p> <p>locale-time zone to America/ US Pacific / Los Angeles</p> <p>WiFi country US, then expand the file system just in case</p> <p>it was not automatically done at boot.</p> <p>If changed the host name, your RPI will have the new hostname on</p> <p>the next boot</p> <p>sudo reboot now</p> <p>After you configure your RPI WiFi and Hostname correctly and reboot,</p> <p>Your RPI should be accessible using SSH from Linux, Mac, Windows</p> <p>Open a terminal</p> <p>ssh pi@ucsdrobocar-xxx-yy.local</p> <p>enter your password</p> <p>try this just in case you can not connect to the RPI on another WiFi Access point</p> <p>such as your Phone acting as a hotspot</p> <p>ssh pi@ucsdrobocar-xxx-yy (without .local)</p> <p>If you have not done so, change the default password</p> <p>on the RPI</p> <p>passwd</p> <p>To disable downloading translations, to save time,</p> <p>create a file named 99translations</p> <p>sudo nano /etc/apt/apt.conf.d/99translations</p> <p>Place the following line in the 99translations</p> <p>Acquire::Languages \\\"none\\\";</p> <p>reboot the RPI</p> <p>sudo reboot now</p> <p>Lets install some dependencies, libraries, and utilities</p> <p>sudo apt-get update</p> <p>This step may take some time, be patient</p> <p>sudo apt-get install build-essential python3 python3-dev python3-pip python3-virtualenv python3-numpy python3-picamera python3-pandas python3-rpi.gpio i2c-tools avahi-utils joystick libopenjp2-7-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev git ntp</p> <p>And install these to have the dependencies for OpenCV</p> <p>sudo apt-get install libilmbase-dev libopenexr-dev libgstreamer1.0-dev libjasper-dev libwebp-dev libatlas-base-dev libavcodec-dev libavformat-dev libswscale-dev libqtgui4 libqt4-test</p> <p>Installing OpenCV</p> <p>sudo apt install python3-opencv</p> <p>reboot the RPI</p> <p>sudo reboot now</p> <p>In case you want to use the (PCA9685) PWM controller from outside a virtual environment</p> <p>sudo apt-get install python3-pip</p> <p>pip3 install Adafruit_PCA9685</p> <p>and this if the Donkey framework complains about the RPI camera</p> <p>pip install \\\"picamera[array]\\\"</p> <p>Lets set a virtual environment and name it donkey</p> <p>We will create virtual environments to enable using different software library configurations without\\ having to install them at the root and user level (higher level)</p> <p>If you have not done so, lets create a directory to store our projects and one subdirectory</p> <p>to store virtual environments</p> <p>cd \\~</p> <p>mkdir projects</p> <p>cd projects</p> <p>mkdir envs</p> <p>cd envs</p> <p>pip3 install virtualenv</p> <p>python3 -m virtualenv -p python3 \\~/projects/envs/[donkey]{.mark} --system-site-packages</p> <p>this line will activate the virtual environment called donkey every time the</p> <p>user pi logs in a terminal</p> <p>echo \\\"source \\~/projects/envs/donkey/bin/activate\\\" &gt;&gt; \\~/.bashrc</p> <p>source \\~/.bashrc</p> <p>When a virtual environment is active, you should see (name_of_virtual_enviroment)</p> <p>in front of the terminal prompt</p> <p>ex:</p> <p>(donkey) pi@ucsdrobocar-xxx-yy:\\~\\$</p> <p>Testing to see if OpenCV is installed in the virtual env</p> <p>python</p> <p>import cv2</p> <p>cv2.__version__</p> <p>exit ()</p> <p>[GCC 8.2.0] on linux</p> <p>Type \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more information.</p> <p>&gt;&gt;&gt; import cv2</p> <p>&gt;&gt;&gt; cv2.__version__</p> <p>\\'3.2.0\\'</p> <p>&gt;&gt;&gt; exit ()</p> <p>Lets install the Donkeycar AI framework</p> <p>if not done, create a directory called projects</p> <p>mkdir projects</p> <p>cd projects</p> <p>Get the latest donkeycar from Github.</p> <p>git clone https://github.com/autorope/donkeycar</p> <p>cd donkeycar</p> <p>git checkout master</p> <p>This step may take some time, be patient</p> <p>pip install -e .[pi]</p> <p>Lets install Tensorflow</p> <p>single board computer (SBC) ...</p> <p>cd \\~/projects</p> <p>Type the command below all in one line</p> <p>wget https://github.com/PINTO0309/Tensorflow-bin/raw/master/tensorflow-2.0.0-cp37-cp37m-linux_armv7l.whl</p> <p>Lets install tensorflow. This step may take some time to install. You are using a low power</p> <p>pip install --upgrade tensorflow-2.0.0-cp37-cp37m-linux_armv7l.whl</p> <p>Quick test of the Tensorflow install</p> <p>python -c \\\"import tensorflow\\\"</p> <p>If no errors, you should be good.</p> <p>If you want to know what is running in your RPI while it is busy,</p> <p>open another terminal, or tab in the same terminal, SSH into the RPI</p> <p>then run the command top</p> <p>top</p> <p>ex: you can see that the CPU(s) is/are busy</p> <p>wait for the install to finish...</p> <p>{width=\"6.875in\" height=\"1.8611111111111112in\"}</p> <p>Try the Tensorflow install again few times if it fails</p> <p>If needed, upgrade pip</p> <p>pip install --upgrade pip</p> <p>Let's create a car on the path \\~/projects/d3</p> <p>donkey createcar --path \\~/projects/d3</p> <p>Your RPI directory should look like this</p> <p>list the content of a directory with ls</p> <p>d3 donkeycar envs tensorflow-2.0.0-cp37-cp37m-linux_armv7l.whl</p> <p>cd \\~/projects/d3</p> <p>ls</p> <p>The output should be something similar to</p> <p>config.py data logs manage.py models myconfig.py train.py</p> <p>Research what you have to do so when you log into your RPI you are working from</p> <p>the car directory like \\~/projects/d3 vs. having to issue the command cd /projects/d3</p> <p>all the time after you log into the RPI.</p> <p>What happens when the RPI boots in relation to the file .bashrc ( \\~/.bashrc) ?</p> <p>To enable the use of a WebCam (USB cameras)</p> <p>sudo apt-get install python3-dev python3-numpy libsdl-dev libsdl-image1.2-dev \\</p> <p>libsdl-mixer1.2-dev libsdl-ttf2.0-dev libsmpeg-dev libportmidi-dev \\</p> <p>libavformat-dev libswscale-dev libjpeg-dev libfreetype6-dev</p> <p>pip install pygame</p> <p>[Here is a link]{.underline} to a plug an play image with necessary software installed</p> <p>If you are at UCSD connected to one of the classes WiFi access points you can</p> <p>connect to the RPI with</p> <p>ssh pi@ucsdrobocar-xxx-yy.local</p> <p>raspberryucsd</p> <p>Then make sure to change the hostname and password. See instructions in this document.</p> <p>Installing the Pulse Width Modulation (PWM)</p> <p>https://docs.google.com/document/d/11nu6_ReReoIxA1KVq-sCy7Tczbk6io0izcItucrw7hI/edit</p> <p>{width=\"5.15625in\" height=\"5.447916666666667in\"}</p> <p>For reference, below is the Raspberry Pi Pinout.</p> <p>You will notice we connect to +3.3v, the two I2C pins (SDA and SCL) and ground:</p> <p>{width=\"6.8125in\" height=\"5.947916666666667in\"}</p> <p>After connecting the I2C PWM Controller lets test the communication</p> <p>Power the RPI, you can use the USB power adapter for this test</p> <p>sudo i2cdetect -y 1</p> <p>The output is something like</p> <p>0 1 2 3 4 5 6 7 8 9 a b c d e f</p> <p>00: -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>40: [40]{.mark} -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</p> <p>70: [70]{.mark} -- -- -- -- -- -- --</p> <p>(dk)pi@jackrpi02:\\~ \\$</p> <p>Look for a similar result above. 40 ...70 in this case is what we are looking for.</p> <p>Now that the RPI3 is talking to the I2C PWM board we can calibrate the Steering and Throttle.</p> <p>Lets get the EMO circuit working too in case you need to use it.</p> <p>For the UCSD RoboCars, an Emergency Stop Circuit is required.</p> <p>COSMOS RoboCars are not required to have the EMO</p> <p>After you charge your Lithium Polymer (LiPo) battery(ries) - [some info here]{.underline}</p> <p>Connect the battery(ries) and batteries monitor/alarm</p> <p>[Please do not use the batteries without the batteries monitor/alarms]{.mark}</p> <p>If you discharge a LiPO batteries below a threshold lets say 3.2 volts. It may not recover to be charged. That is why it is critical that you use the LiPo batteries alarm all the time.</p> <p>Connecting the Emergency Stop Circuit and Batteries into the Robot</p> <p>For this part of your robot, you will have to do some hacking. That is part of the class.</p> <p>The instructor will discuss the principle of the circuit and how to implement it with the component in your robot kit.</p> <p>Long story short, the PWM Controller we use has a disable pin. It the correct logic is applied to it for example Logic 1 or 0 (3.3V or 0V) it will disable the PWM signals and the UCSDRoboCar will stop.</p> <p>Think why one needs a separate EMO circuit vs. relying on the software, operating system and computer communicating with the PWM controller that then send PWM pulses to the actuators (steering servo and driving DC motor by the electronics speed controller)</p> <p>First search on the web and read the datasheet of the emergency stop switch (EMO) components provided with the robot kit and discuss them with your teammates how the EMO will work. You got two main components to build the a WireLess EMO switch:</p> <p>c)  A Wireless Relay with wireless remote controls</p> <p>d)  A Red/Blue high power LED. This is to help the user know if the car     is disable (Blue) or Enabled (Red).</p> <ul> <li> <p>What is the disable pin the PWM controller?</p> </li> <li> <p>Does it disable on logic 1 or 0?</p> </li> <li> <p>How to wire the wireless relay to provide the logic you need to     disable PWM controller? (1 or 0)</p> </li> <li> <p>How to connect the LEDs (Blue and Red) to indicate (RED -     hot/enabled), (BLUE - power is on / disabled).</p> </li> <li> <p>Note: The power to the PWM controller, that powers the LEDs, comes     from ESC (Electronics Speed Controller). Therefore, you have to     connect the robot batteries to the ESC and the ESC to the PWM     controller. We are using channel 2 for the ESC. Channel 1 for     the Steering.</p> </li> </ul> <p>After you see that the EMO is working, i.e. wireless remote control disable the PWM, and the LEDs light up as planned, then you need to document your work. Please use a schematic software such as Fritzing ([http://fritzing.org/home/]{.underline}) to document your electrical schematic.</p> <p>It may seem we did the opposite way; document after your build. In our case, you learned by looking at components information, thinking about the logic, and experimenting. Since you are an engineer you need to document even it if was a hack...</p> <p>Working in a company you may do fast prototyping first then document. On larger projects you make schematics, diagrams, drawings, work instructions, then build it. Keep that in mind!</p> <p>Calibration of the Throttle and Steering</p> <p>Before you develop code or you can use a someone\\'s code to control a robot, we need to calibrate the actuator/mechanism to find out its range of motion compared to the control / command a computer will send to the controller of the actuator/mechanism.</p> <p>The calibration is car specific. If you use the same platform, the numbers should be really close. Otherwise, you will need to calibrate your car.</p> <p>[We are following the standard from RC CAR world, Channel 1 for Steering, Channel 2 for Throttle]{.mark}</p> <p>Note:The default DonkeyCar build uses Channels 0 and 1</p> <p>The UCSDRoboCar has at least two actuators. A steering servo and the DC motor connected to an Electronics Speed Controller (ESC). These are controlled by PWM (Pulse Width Modulation).</p> <p>We use PWM Controller to generate the PWM signals, a device similar to the one in the picture below</p> <p>{width=\"4.015625546806649in\" height=\"2.804853455818023in\"}</p> <p>Shutdown the RPI if it is on type this command</p> <p>sudo shutdown -h now</p> <p>Connect the Steering Servo to the Channel 1</p> <p>Connect the Throttle (ESC) to Channel 2</p> <p>Observe the orientation of the 3 wires connector coming from the Steering Servo and ESC. Look for a the black or brown wire, that is the GND (-).</p> <p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p> <p>Follow the safety guidelines provided in person in the class. If something does not sound right, don't do it. Safety first.</p> <p>Power on the RPI</p> <p>Power the Electronic Speed Controller (ESC)</p> <p>Enable the robot using your EMO wireless control - The safety LED should be RED</p> <p>Lets run a Python command to calibrate Steering and Throttle</p> <p>The donkey commands need to be run from the directory you created for your car, i.e., [d2]{.mark}</p> <p>If you have not done so, SSH into the RPI</p> <p>If needed, change directory to d2t</p> <p>cd d2t</p> <p>The output</p> <p>(env) pi@jackrpi10:\\~/projects/d3 \\$</p> <p>donkey calibrate [--channel 1]{.mark}</p> <p>Try some values around 400 to center the steering</p> <p>Enter a PWM setting to test(100-600)370</p> <p>Enter a PWM setting to test(100-600)365</p> <p>Enter a PWM setting to test(100-600)360</p> <p>[In my case]{.mark}, 365 seems to center the steering</p> <p>Take note of the left max, right max,</p> <p>ex:</p> <p>365 - Center</p> <p>[285]{.mark} - Steering left max</p> <p>[440]{.mark} - Steering right max</p> <p>Note: When donkey calibrate times-out after few tries, just run it again</p> <p>donkey calibrate [--channel 1]{.mark}</p> <p>You can interrupt the calibration by typing CTRL-C</p> <p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p> <p>Let me say this one more time</p> <p>[MAKE SURE THE ROBOT IS ON THE RC CAR STAND]{.mark}</p> <p>Now to calibrate the Throttle Electronic Speed Controller ESC (THROTTLE)</p> <p>Following the standard for R/C cars. Throttle goes on channel [2]{.mark}</p> <p>donkey calibrate [--channel 2]{.mark}</p> <p>Enter a PWM setting to test(100-600)[370 (neutral)]{.mark}</p> <p>Enter a PWM setting to test(100-600)380</p> <p>Enter a PWM setting to test(100-600)390</p> <p>370 when power up the ESC seems to be the middle point (neutral),</p> <p>lets use 370 also to be compatible with Donkey.</p> <p>Neutral when power the ESC - [370]{.mark}</p> <p>Then go in increments of 10\\~20 until you can no longer hear increase on the speed</p> <p>of the car. Don't worry much about the max speed since we won't drive that fast autonomously</p> <p>and during training the speed will be limited.</p> <p>Max speed forward - [460]{.mark}</p> <p>Reverse on RC cars is a little tricky because the ESC needs to receive</p> <p>a reverse pulse, zero pulse, and again reverse pulse to start to go backwards.</p> <p>Use the same technique as above set the PWM [setting to your zero throttle (lets say 370)]{.mark}.</p> <p>[Enter the reverse value]{.mark}, then the [zero throttle (e.g., 370)]{.mark} value, then the [reverse value again]{.mark}.</p> <p>Enter values +/- 10 of the reverse value to find a reasonable reverse speed. Remember this reverse PWM value.</p> <p>(dk)pi@jackrpi02:\\~/projects/d3 \\$</p> <p>donkey calibrate [--channel 2]{.mark}</p> <p>...</p> <p>Enter a PWM setting to test(100-600)360</p> <p>Enter a PWM setting to test(100-600)[370]{.mark}</p> <p>Enter a PWM setting to test(100-600)360</p> <p>Enter a PWM setting to test(100-600)350</p> <p>Enter a PWM setting to test(100-600)340</p> <p>Enter a PWM setting to test(100-600)330</p> <p>Enter a PWM setting to test(100-600)320</p> <p>Enter a PWM setting to test(100-600)310</p> <p>Enter a PWM setting to test(100-600)300</p> <p>Enter a PWM setting to test(100-600)290</p> <p>---</p> <p>Neutral when power the ESC - [370]{.mark}</p> <p>Max speed forward - [460]{.mark}</p> <p>Max speed backwards - [280]{.mark}</p> <p>and from the steering calibration</p> <p>Center - 365</p> <p>Steering left max - [285]{.mark}</p> <p>Steering right max - [440]{.mark}</p> <p>---</p> <p>Now let\\'s write these values into the car configuration file</p> <p>(dk)pi@jackrpi02:\\~/projects/d3 \\$ ls</p> <p>config.py data logs manage.py models</p> <p>Edit the file config.py</p> <p>(dk)pi@jackrpi02:\\~/projects/d3 \\$</p> <p>nano config.py</p> <p>Change these values according to YOUR [calibration values]{.mark} and [where you have the Steering Servo and ESC]{.mark} connected</p> <p>...</p> <p>STEERING</p> <p>STEERING_CHANNEL = 1</p> <p>STEERING_LEFT_PWM = [285]{.mark}</p> <p>STEERING_RIGHT_PWM = [440]{.mark}</p> <p>THROTTLE</p> <p>THROTTLE_CHANNEL = [2]{.mark}</p> <p>THROTTLE_FORWARD_PWM = [460]{.mark}</p> <p>THROTTLE_STOPPED_PWM = [370]{.mark}</p> <p>THROTTLE_REVERSE_PWM = [280]{.mark}</p> <p>Also, if needed change these</p> <p>JOYSTICK</p> <p>USE_JOYSTICK_AS_DEFAULT = True</p> <p>JOYSTICK_MAX_THROTTLE = 0.4</p> <p>JOYSTICK_STEERING_SCALE = 1.0</p> <p>AUTO_RECORD_ON_THROTTLE = True</p> <p>CONTROLLER_TYPE=\\'ps3\\' (ps3|ps4)</p> <p>USE_NETWORKED_JS = False</p> <p>NETWORK_JS_SERVER_IP = \\\"192.168.0.1\\\"</p> <p>LED</p> <p>HAVE_RGB_LED = True</p> <p>LED_INVERT = False COMMON ANODE?</p> <p>board pin number for pwm outputs</p> <p>LED_PIN_R = 12</p> <p>LED_PIN_G = 10</p> <p>LED_PIN_B = 16</p> <p>LED status color, 0-100</p> <p>LED_R = 0</p> <p>LED_G = 0</p> <p>LED_B = 10</p> <p>Another example of configuration for the config.py, now with a joystick dead zone for neutral</p> <p>donkey calibrate --channel 1</p> <p>donkey calibrate --channel 2</p> <p>config.py settings</p> <p>STEERING</p> <p>STEERING_CHANNEL = 1</p> <p>STEERING_LEFT_PWM = 275</p> <p>STEERING_RIGHT_PWM = 440</p> <p>Center \\~ 360</p> <p>THROTTLE</p> <p>THROTTLE_CHANNEL = 2</p> <p>THROTTLE_FORWARD_PWM = 460</p> <p>THROTTLE_STOPPED_PWM = 370</p> <p>THROTTLE_REVERSE_PWM = 220</p> <p>JOYSTICK_MAX_THROTTLE = 0.8</p> <p>At the UCSD Outdoor Track my car was turning too much. I scaled the steering. May need to increase the number to have sharper turns.</p> <p>JOYSTICK_STEERING_SCALE = 0.8</p> <p>Because I have a particular joystick that the Throttle neutral was not centered, I had to</p> <p>use a deadzone value on config.py</p> <p>Try first with deadzone= 0</p> <p>JOYSTICK_DEADZONE = 0.02</p> <p>LED</p> <p>HAVE_RGB_LED = True</p> <p>board pin number for pwm outputs</p> <p>LED_PIN_R = 8</p> <p>LED_PIN_G = 10</p> <p>LED_PIN_B = 12</p> <p>LED status color, 0-100</p> <p>LED_R = 0</p> <p>LED_G = 0</p> <p>LED_B = 60</p> <p>Note: If you robot has too much power or not enough power when you start driving it, adjust the max_throttle</p> <p>JOYSTICK_MAX_THROTTLE = 0.5</p> <p>This also will be your starting power setting when using the constant throttle autopilot.</p> <p>Connecting the PS3 Keypad/Joystick</p> <p>[http://docs.donkeycar.com/parts/controllers/]{.underline}</p> <p>Bluetooth Setup</p> <p>[https://pythonhosted.org/triangula/sixaxis.html]{.underline}</p> <p>We need to install some bluetooth software in the RPI</p> <p>ssh to the RPI</p> <p>sudo apt-get update</p> <p>sudo apt-get install bluetooth libbluetooth3 libusb-dev</p> <p>sudo systemctl enable bluetooth.service</p> <p>sudo usermod -G bluetooth -a pi</p> <p>shutdown the RPI</p> <p>sudo shutdown now</p> <p>Remove the power from the RPI. Wait \\~ 5 s.</p> <p>Power on the RPI,</p> <p>Connect your PS3 Controller to the RPI using a mini USB Cable.</p> <p>SSH to the RPI</p> <p>Lets download, compile, and install some bluetooth configuration software</p> <p>wget http://www.pabr.org/sixlinux/sixpair.c</p> <p>gcc -o sixpair sixpair.c -lusb</p> <p>sudo ./sixpair</p> <p>Output should be something similar to this</p> <p>Current Bluetooth master: b8:27:eb:49:2d:8c</p> <p>Setting master bd_addr to b8:27:eb:49:2d:8c</p> <p>Execute the 'bluetoothctl' command</p> <p>Pay attention to your PS3 controller [mac address]{.mark}</p> <p>bluetoothctl</p> <p>the output should be something similar to this</p> <p>(env) pi@jackrpi10:\\~ \\$ bluetoothctl</p> <p>[NEW] Controller B8:27:EB:7A:12:18 jackrpi10 [default]</p> <p>[NEW] Device [64:D4:BD:03:CD:C9]{.mark} PLAYSTATION(R)3 Controller</p> <p>If you don't see the PS3 Controller, unplug and plug it in RPI again</p> <p>Type agent on</p> <p>[bluetooth]agent on</p> <p>Agent is already registered</p> <p>Type default-agent</p> <p>default-agent</p> <p>If you don't see the PS3 Controller, unplug and plug it in RPI again</p> <p>default-agent64:D4:BD:03:CD:C9</p> <p>[bluetooth]default-agent</p> <p>Default agent request successful</p> <p>[NEW] Device 64:D4:BD:03:CD:C9 Sony PLAYSTATION(R)3 Controller</p> <p>Authorize service</p> <p>[agent] Authorize service 00001124-0000-1000-8000-00805f9b34fb ([yes]{.mark}/no):</p> <p>Type yes</p> <p>Yes</p> <p>[CHG] Device 64:D4:BD:03:CD:C9 Trusted: yes</p> <p>[CHG] Device 64:D4:BD:03:CD:C9 UUIDs: 00001124-0000-1000-8000-00805f9b34fb</p> <p>[NEW] Device [64:D4:BD:03:CD:C9]{.mark} Sony PLAYSTATION(R)3 Controller</p> <p>Authorize service</p> <p>Yes</p> <p>Type trust and the MAC address for the PS3 controller</p> <p>ex: [bluetooth]trust 64:D4:BD:03:CD:C9</p> <p>trust THE_MAC_ADDRESS_PS3_CONTROLLER</p> <p>Changing 64:D4:BD:03:CD:C9 trust succeeded</p> <p>[CHG] Device 64:D4:BD:03:CD:C9 Trusted: yes</p> <p>[bluetooth]quit</p> <p>Agent unregistered</p> <p>Lets check what input devices are available</p> <p>ls /dev/input</p> <p>Output</p> <p>by-id by-path event0 event1 [js0]{.mark} mice</p> <p>you should see a [js0]{.mark}</p> <p>If connected by the USB cable, disconnect the PS3 controller from the RPI.</p> <p>Lets list the input devices again</p> <p>ls /dev/input</p> <p>Output</p> <p>mice</p> <p>Now press the \\&lt;PS&gt; button at the PS3 controller,</p> <p>the lights on the front of the controller should flash for a couple of seconds then stop</p> <p>The LED 1 should then stay one or flash then goes off. Since we are not using a PS3 game</p> <p>console, the LEDS on the controller showing the connections are not reliable.</p> <p>Again, we are not connecting the PS3 controller to a PS3 game console...). As long as you see a</p> <p>js0 listed on your input devices you are good to go. See below.</p> <p>ls /dev/input</p> <p>Output</p> <p>event0 event1 [js0]{.mark} mice</p> <p>Please keep your PS3 Controller Off when you are not using it. Press and hold the \\&lt;PS&gt;</p> <p>button for \\~10s. The LEDs at the controller will flash then go off.</p> <p>If the PS3 controller connection is intermittent, try</p> <p>sudo rpi-update</p> <p>To remove a device, lets say another JoyStick that you don't use anymore</p> <p>bluetoothctl</p> <p>paired-devices</p> <p>remove THE_MAC_ADDRESS</p> <p>[NEW] Controller B8:27:EB:72:95:A6 rpimine01 [default]</p> <p>[NEW] Device 00:1E:3D:D8:EA:15 PLAYSTATION(R)3 Controller</p> <p>[NEW] Device 00:16:FE:74:12:B7 PLAYSTATION(R)3 Controller</p> <p>[bluetooth]remove 00:1E:3D:D8:EA:15</p> <p>[DEL] Device 00:1E:3D:D8:EA:15 PLAYSTATION(R)3 Controller</p> <p>Device has been removed</p> <p>[bluetooth]</p> <p>If you are having problems pairing the PS3 controller, please reset it.</p> <p>There is \"small\" reset button at the back. Look up at the Internet if you can't find it.</p> <p>This is another method that worked for me with the the RaspbianBuster (Sep 2019)</p> <p>[sudo apt-get install bluetooth libbluetooth3 libusb-dev]{.mark}</p> <p>[sudo systemctl enable bluetooth.service]{.mark}</p> <p>[sudo bluetoothctl]{.mark}</p> <p>[agent on]{.mark}</p> <p>[default-agent]{.mark}</p> <p>[scan on]{.mark}</p> <p>Plug the PS3 Controller in the RPI using a Mini USB Cable</p> <p>Trust when asked</p> <p>Disconnect the PS3 Controller</p> <p>Press the PS Button on the Controller. See if it will pair</p> <p>Now you can go drive you robot to collect data. Make sure to keep the</p> <p>EMO handy and use when needed!</p> <p>Also the \\&lt;X&gt; on your controller is like an emergency break.</p> <p>Driving the Robot to Collect data</p> <p>Remember you are driving at max of x (0.x) Throttle power based on the</p> <p>config.py that you edited.</p> <p>To reverse you may have to reverse, stop, reverse. This is a feature of the</p> <p>ESC using on RC cars to prevent damaging gears when changing from forward to reverse.</p> <p>On the RPI</p> <p>cd d2t</p> <p>(env) pi@jackrpi10:\\~/projects/d3 \\$</p> <p>[python manage.py drive]{.mark}</p> <p>Note: CTRL-C stop the manage.py drive</p> <p>Output below</p> <p>---</p> <p>(env) pi@jackrpi10:\\~/projects/d3 \\$ python manage.py drive</p> <p>using donkey v2.5.0t ...</p> <p>loading config file: /home/pi/d2t/config.py</p> <p>config loaded</p> <p>cfg.CAMERA_TYPE PICAM</p> <p>PiCamera loaded.. .warming camera</p> <p>Adding part PiCamera.</p> <p>Adding part PS3JoystickController.</p> <p>Adding part ThrottleFilter.</p> <p>Adding part Lambda.</p> <p>Adding part Lambda.</p> <p>Adding part Lambda.</p> <p>Init ESC</p> <p>Adding part PWMSteering.</p> <p>Adding part PWMThrottle.</p> <p>Tub does NOT exist. Creating new tub...</p> <p>New tub created at: /home/pi/d2t/data/tub_2_18-09-25</p> <p>Adding part TubWriter.</p> <p>You can now move your joystick to drive your car.</p> <p>Starting vehicle...</p> <p>Opening /dev/input/js0...</p> <p>Device name: PLAYSTATION(R)3 Controller</p> <p>/home/pi/env/lib/python3.5/site-packages/picamera/encoders.py:544: PiCameraResolutionRounded: frame size rounded up from 160x120 to 160x128</p> <p>width, height, fwidth, fheight)))</p> <p>---</p> <p>If you get this error trying to drive it is because the PS3 game controller is off</p> <p>or not connected</p> <p>Output</p> <p>/dev/input/js0 is missing</p> <p>ls</p> <p>config.py data logs manage.py models sixpair sixpair.c</p> <p>ls data</p> <p>tub_1_17-10-13</p> <p>If you want to wipe clean the data collected</p> <p>Remove the content of the \\~./d2t/data directory. It should be tub_......</p> <p>You can delete the entire directory then create it again.</p> <p>(dk)pi@jackrpi02:\\~/projects/d3 \\$</p> <p>rm -rf data</p> <p>mkdir data</p> <p>Donkey Install Using Linux - Ubuntu</p> <p>It is highly recommended that you use Ubuntu in this class. Ideally you can make your computer</p> <p>dual boot to Ubuntu. Look for instructions at the Internet for that. PLEASE backup your computer</p> <p>first.</p> <p>Alternatively, you can install a Virtual Machine management software. e.g., VirtualBox.</p> <p>Then install Ubuntu.</p> <p>These first instructions wont have GPU support.</p> <p>Nvidia GPU support documentation is provided later in this document.</p> <p>Installation of GPU features won't be supported in the class.</p> <p>You will have access to a GPU Cluster from the UC San Diego Supercomputer Center.</p> <p>Lets refresh the Ubuntu repositories and installed software</p> <p>sudo apt-get update</p> <p>sudo apt-get upgrade</p> <p>First lets create a directory to store your projects</p> <p>mkdir projects</p> <p>cd projects</p> <p>Install some necessary software and create a virtual environment</p> <p>sudo apt-get install virtualenv build-essential python3-dev gfortran libhdf5-dev libatlas-base-dev</p> <p>virtualenv env -p python3</p> <p>Activate the virtual environment</p> <p>source env/bin/activate</p> <p>Look for the [(env)]{.mark} in front of your command line. It is indication that env is active</p> <p>To deactivate an environment type deactivate from inside the environment</p> <p>To activate, make sure you are at the projects directory then type source env/bin/activate</p> <p>To install specific versions of Keras and Tensorflow (non-GPU)</p> <p>Install Keras</p> <p>pip install keras==2.2.2</p> <p>Install tensorflow</p> <p>pip install tensorflow==1.10.0</p> <p>As 03Feb19, the latest version of tensorflow is 1.12</p> <p>Keep the same version of Tensorflow as close as the Tensorflow in the robocar</p> <p>(Raspberry) as feasible.</p> <p>pip install tensorflow==1.12</p> <p>if needed, install keras 2.2.4</p> <p>pip install keras==2.2.4</p> <p>Run a short TensorFlow program to test it</p> <p>Invoke python from your shell as follows:</p> <p>\\$ python</p> <p>Enter the following short program inside the python interactive shell:</p> <p>Python</p> <p>import tensorflow as tf</p> <p>hello = tf.constant(\\'Hello, TensorFlow!\\')</p> <p>sess = tf.Session()</p> <p>print(sess.run(hello))</p> <p>Ctr-D gets out of the Python Tensorflow test.</p> <p>Install Donkey on your Ubuntu Linux Computer</p> <p>cd projects</p> <p>If you want to create another virtual environment or env was not created yet</p> <p>Install some necessary software and create a virtual environment</p> <p>sudo apt-get install virtualenv build-essential python3-dev gfortran libhdf5-dev libatlas-base-dev</p> <p>virtualenv env -p python3</p> <p>Activate the virtual environment</p> <p>source env/bin/activate</p> <p>Look for the [(env)]{.mark} in front of your command line. It is indication that env is active</p> <p>To deactivate an environment type deactivate from inside the environment</p> <p>To activate, make sure you are at the projects directory then type source env/bin/activate</p> <p>This is like you did on RPI but I had my car under the \\~/projects directory</p> <p>Lets get the latest Donkey Framework from Tawn Kramer</p> <p>git clone https://github.com/tawnkramer/donkey\\ pip install -e donkey[pc]</p> <p>Create a car</p> <p>donkey createcar --path \\~/projects/d2t</p> <p>from here you can transfer data from your RPI,and then train</p> <p>create a model (autopilot), transfer the model to the RPI, test it.</p> <p>Using the GPU Cluster to Train the Auto Pilots</p> <p>27Aug18 - Under development - Please provide feedback</p> <p>We are following the instruction provide by out IT Team</p> <p>[https://docs.google.com/document/d/e/2PACX-1vTe9sehl7izNJJNypsDNABD4wg-F-AClAi0cYV3pIIRGpCknD7SEWQPEGqy_5DBRmFQtkulLkHkLxEm/pub]{.underline}</p> <p>We will be using the UCSD Supercomputer GPU Cluster. There are few steps that you need to follow. Pay attention to not leave machines running on the cluster because it is a share resource.</p> <p>You should be able to log with you student credentials.</p> <p>Pay attention to not use a GPU machine to do work that does not require GPU...</p> <p>To begin, login to your account using an SSH client. Most students will use their standard email username to sign on</p> <p>Create Car Project</p> <p>ssh [YOUR_USER_ID@ieng6.ucsd.edu]{.underline}</p> <p>prep me148f</p> <p>We will launch a container (similar to a virtual machine) without GPU. Tensorflow is not installed on it.</p> <p>Lets launch a machine to enable us to create a robocar using the Donkey Framework</p> <p>[No GPU]{.mark} to save resources</p> <p>launch-py3torch.sh</p> <p>Attempting to create job (\\'pod\\') with 4 CPU cores, 16 GB RAM, and [0 GPU units]{.mark}.</p> <p>To keep the car name the same as in the RPI I created a d2t car</p> <p>source activate /datasets/conda-envs/me148f-conda/</p> <p>donkey createcar --path projects/[d2t]{.mark}/</p> <p>The car was created under projects</p> <p>cd \\~/projects/d2t</p> <p>Lets exit the container you used to create the Car</p> <p>exit</p> <p>Lets check that we don't have any container running under our user</p> <p>kubectl get pods</p> <p>No resources found.</p> <p>If there is nothing still running, lets get out of the ieng6 machine</p> <p>logout</p> <p>If you see a container still running,</p> <p>f \"kubectl get pods\" shows leftover containers still running, you may kill them as follows:</p> <p>[ee148vzz@ieng6-201]:\\~:505\\$ [kubectl get pods]{.mark}</p> <p>NAME READY STATUS RESTARTS AGE</p> <p>ee148vzz-24313 1/1 Running 0 9m</p> <p>[ee148vzz@ieng6-201]:\\~:506\\$ [kubectl delete pod agt-24313]{.mark}</p> <p>pod \\\"ee148vzz-24313\\\" deleted</p> <p>[ee148vzz@ieng6-201]:\\~:507\\$ [kubectl get pods]{.mark}</p> <p>No resources found.</p> <p>[ee148vzz@ieng6-201]:\\~:508\\$</p> <p>If there is no container running, lets get out of the ieng6 machine</p> <p>logout</p> <p>Now, let's bring some data in so we can train on it</p> <p>Use rsync command below in one line</p> <p>You can rsync direct from the RPI. SSH to the RPI the issue the rsync command to rsync direct to the GPU cluster. Or you can rsync the data to your computer then rsync to the GPU cluster</p> <p>Example of rsync one particular Tub directory</p> <p>rsync -avr -e ssh --rsync-path=cluster-rsync tub_1_17-11-18 [YOUR_USER_ID]{.mark}\\@ieng6.ucsd.edu:projects/d2/data/</p> <p>Example of rsync the entire data directory\\ Sending data to the UCSD GPU Cluster direct from the RPI\\ rsync -avr -e ssh --rsync-path=cluster-rsync \\~/projects/d3/data/* [YOUR_USER_ID]{.mark}\\@ieng6.ucsd.edu:projects/[d2]{.mark}/data/\\ \\ or if you have the car created at d2t at the RPI and GPU Cluster\\ \\ rsync -avr -e ssh --rsync-path=cluster-rsync \\~/projects/d3/data/* YOUR_USER_ID@ieng6.ucsd.edu:projects/[d2t]{.mark}/data/</p> <p>To enable constant rsync and train while you collect data (drive) we need to exchange a key between the two computers we will rsync. e.g., GPU Cluster and RPI, your computer and RPI, your computer and the GPU Cluster</p> <p>To Enable Continuous Training - And to not have to type password for SSH and RSYNC</p> <p>At the PC or RPI</p> <p>Generate the Private and Public Keys</p> <p>cd \\~</p> <p>sudo apt-get install rsync</p> <p>ssh-keygen -t rsa</p> <p>You may see a warning if you already have a key in your system. Just keep or replace it as you</p> <p>wish. Keep in mind that if you replace the key your previous trust connections will be lost.</p> <p>Linux Ubuntu</p> <p>cat .ssh/id_rsa.pub</p> <p>macos</p> <p>cat \\~/.ssh/id_rsa.pub</p> <p>Copy the output of the command into a buffer (e.g., Ctrl-C)</p> <p>You can use two terminals to easy the copy and paste the information</p> <p>Leave a terminal open with the key you generated</p> <p>From your PC to the RPI</p> <p>Try this first. If does not work try the steps below this line</p> <p>cat \\~/.ssh/id_rsa.pub | ssh pi@YOUR_PI_NAME.local \\'cat &gt;&gt; .ssh/authorized_keys\\'</p> <p>or</p> <p>From the PC to the RPI or PC to the Cluster</p> <p>At the RPI or GPU Cluster</p> <p>cd \\~</p> <p>mkdir .ssh</p> <p>If the directory exists, just ignore this error if you see it</p> <p>&gt; mkdir: cannot create directory '.ssh': File exists</p> <p>chmod 700 .ssh</p> <p>chown pi:pi .ssh</p> <p>nano .ssh/authorized_keys</p> <p>then copy and paste the information from the host id_rsa.pub you generated earlier at the PC or RPI</p> <p>I was able to ssh to the RPI without asking for the password</p> <p>ex: ssh pi@jackrpi05.local</p> <p>Here is what I have done to ssh and use rsync from my computer to the Cluster</p> <p>without having to type the user password all the time</p> <p>for user me148f</p> <p>SSH to ieng6</p> <p>ssh me148f@ieng6.ucsd.edu</p> <p>[me148f@ieng6-201]:\\~:44\\$ cd \\~</p> <p>[me148f@ieng6-201]:\\~:45\\$ mkdir .ssh</p> <p>[me148f@ieng6-201]:\\~:46\\$ chmod 700 .ssh</p> <p>[me148f@ieng6-201]:\\~:48\\$ chown me148f:me148f .ssh</p> <p>Just ignore the error below</p> <p>chown: invalid group: \\'me148f:me148f\\'</p> <p>[me148f@ieng6-201]:\\~:49\\$ nano .ssh/authorized_keys</p> <p>Now copy and paste the Key you created on your computer.</p> <p>You can use this same process to ssh / rsync from the RPI to the Cluster</p> <p>without having to type the user password all the time.</p> <p>Now you should be able to ssh and rsync to the RPI or Cluster without typing the user password</p> <p>ex: ssh [YOUR_USER_ID@ieng6.ucsd.edu]{.underline}</p> <p>If you can not ssh to the RPI or Cluster, fix the problem before continue</p> <p>Constant rsync - sync data while you drive the car</p> <p>Lets make rsync run periodically</p> <p>We will be using some Python Code</p> <p>rsync data continuously to the data directory under where the code was called from</p> <p>ex: \\~/projects/d2t</p> <p>From the PC to the RPI</p> <p>At the PC, create a file name continous_data_rsync.py</p> <p>nano continous_data_rsync.py</p> <p>import os</p> <p>import time</p> <p>while True:</p> <p>command = \\\"rsync -aW --progress %s@%s:%s/data/ ./data/ --delete\\\" %\\</p> <p>(\\'pi\\', \\'jackrpi04.local\\', \\'\\~/projects/d3\\')</p> <p>os.system(command)</p> <p>time.sleep(5)</p> <p>call the python code with</p> <p>python continous_data_rsync.py</p> <p>From the RPI to the GPU Cluster</p> <p>Since the GPU cluster can not see your RPI on the network,</p> <p>we need to RSYNC from the RPI to the GPU Cluster</p> <p>At the RPI create a file name continous_data_rsync.py</p> <p>nano continous_data_rsync.py</p> <p>import os</p> <p>import time</p> <p>while True:</p> <p>command = \\\"rsync -aW --progress %s@%s:%s/data/ ./data/ --delete\\\" %\\</p> <p>(\\'pi\\', \\'jackrpi04.local\\', \\'\\~/projects/d3\\')</p> <p>os.system(command)</p> <p>time.sleep(5)</p> <p>call the python code with</p> <p>python continous_data_rsync.py</p> <p>Continuous Train\\ This command fires off the keras training in a mode where it will continuously look for new</p> <p>data at the end of every epoch.\\ \\ Usage:\\ donkey contrain [--tub=\\&lt;data_path&gt;] [--model=\\&lt;path to model&gt;] [--transfer=\\&lt;path to model&gt;] [--type=\\&lt;linear|categorical|rnn|imu|behavior|3d&gt;] [--aug]</p> <p>example</p> <p>training for a particular tub</p> <p>donkey contrain --tub=\\~/projects/d2t/data/tub_1_18-08-20 --model=20aug18_just_testing.h5</p> <p>training for all tub files</p> <p>donkey contrain --model=24aug18_just_testing.h5</p> <p>Continuous Training from a PC</p> <p>Youetter have a GPU for training, if not this will take a long time. It wont work.</p> <p>If you don't have a NVIDIA GPU on your PC with CUDA</p> <p>Here what I have done for continuous training using a PC</p> <p>For continuous training</p> <p>Read instructions and do the security keys exchange first</p> <p>After exchanging the security keys to not have to type the password on ssh and RSYNC</p> <p>Create a file name continous_data_rsync.py</p> <p>nano continous_data_rsync.py</p> <p>import os</p> <p>import time</p> <p>while True:</p> <p>command = \\\"rsync -aW --progress %s@%s:%s/data/ ./data/ --delete\\\" %\\</p> <p>(\\'pi\\', \\'[jackrpi01]{.mark}.local\\', \\'\\~/projects/d3\\')</p> <p>os.system(command)</p> <p>time.sleep(10)</p> <p>To start the continuous RSYNC</p> <p>python continous_data_rsync.py</p> <p>Edit the config.py at your computer so the continuous training works</p> <p>Continuous Training sends models to the PI when there is new model created.</p> <p>No need to have an RSYNC for models.</p> <p>nano config.py</p> <p>pi information</p> <p>PI_USERNAME = \\\"pi\\\"</p> <p>PI_PASSWD = \\\"[Your_RPI_PassWord_Here]{.mark}\\\"</p> <p>PI_HOSTNAME = \\\"[jackrpi01.local]{.mark}\\\"</p> <p>PI_DONKEY_ROOT = \\\"[d2t]{.mark}\\\"</p> <p>SEND_BEST_MODEL_TO_PI = True</p> <p>save the config.py</p> <p>Continuous Training</p> <p>training for all tub files</p> <p>python train.py --continuous --model=models/date_modelName.h5</p> <p>training for a particular tub</p> <p>python train.py --continuous --tub=\\~/projects/d2t_pmm01/data/tub_1_18-08-20 --model=20aug18_just_testing.h5</p> <p>If set on config.py continuous Training automatically sends the new model to the RPI.</p> <p>SEND_BEST_MODEL_TO_PI = True</p> <p>When using the UCSD's GPU Cluster, you will need an rsync for the models</p> <p>since the GPU Cluster can not see the RPI</p> <p>When using the GPU Cluster, rsync works from the RPI to the GPU Cluster</p> <p>rsync models to the RPI continuously from the models directory where the code was called from</p> <p>Create a file name continous_models_rsync.py</p> <p>nano continous_models_rsync.py</p> <p>import os</p> <p>import time</p> <p>while True:</p> <p>command = \\\"rsync -aW --progress ./models/ %s@%s:%s/models/ --delete\\\" %\\</p> <p>(\\'pi\\', \\'[jackrpi01.local]{.mark}\\', \\'\\~/projects/d3\\')</p> <p>os.system(command)</p> <p>time.sleep(30)</p> <p>To start the continuous RSYNC</p> <p>python continous_models_rsync.py</p> <p>Issue the drive command with the model name.</p> <p>You need a new model name before issuing the command to drive with a mode.</p> <p>So lets create a blank model file. Use the same name for the model you will create with</p> <p>continuous training</p> <p>touch \\~/projects/d3/[24nov18_ucsd01.json]{.mark}</p> <p>Load the model_name.json because it loads faster than model_name.h5</p> <p>python manage.py drive --model=./models/[24nov18_ucsd01.json]{.mark}</p> <p>Security Keys Exchange</p> <p>It can save you some time by allowing you log to the RPI without asking for the password,</p> <p>same for RSYNC</p> <p>Dont use this on mission critical computers</p> <p>At the PC</p> <p>Generate the Private and Public Keys</p> <p>Exchange security Keys</p> <p>cd \\~</p> <p>sudo apt-get install rsync</p> <p>ssh-keygen -t rsa</p> <p>ubuntu</p> <p>cat .ssh/id_rsa.pub</p> <p>mac</p> <p>cat \\~/.ssh/id_rsa.pub</p> <p>Copy the output of the command into the buffer (Ctrl-C or Command-C on a Mac)</p> <p>At the RPI</p> <p>cd \\~</p> <p>mkdir .ssh</p> <p>Ignore this message if you get it</p> <p>&gt; mkdir: cannot create directory '.ssh': File exists</p> <p>chmod 700 .ssh</p> <p>chown pi:pi .ssh</p> <p>nano .ssh/authorized_keys</p> <p>I was able to ssh to the RPI without asking for the password</p> <p>ex: ssh pi@jackrpi04.local</p> <p>Summary of commands after the robot built and software Installed</p> <p>[http://docs.donkeycar.com/guide/get_driving/]{.underline}</p> <p>You will need and Access Point (AP) to connect to your RoboCar to be able to SSH to it and give the commands:</p> <p>For that we have few options:</p> <ul> <li> <p>Move the WiFi AP to a place close to the track. You will need a     power outlet there to power the AP</p> </li> <li> <p>Use your phone as an AP. You will need to add an entry in the     RoboCar WiFi configuration so it connects to your Phone AP. See     instruction on RPI WiFi configuration. Please make sure it has a     lower priority than the UCSDRoboCar AP. e.g., 30 and below.     Moreover, you will need to connect your computer to the same phone     AP so you can SSH to your RoboCar RPI.</p> </li> <li> <p>You can create a direct connection between your computer and the     RoboCar. Need to search the net for that. Please make sure the     RoboCar still connects to the AP UCSDRoboCar after you are done     using the direct connection with your phone.</p> </li> </ul> <p>If you follow the instructions in this document, the PS3Keypad/Joystick will be used by default to drive the RoboCar.</p> <p>Summary of commands after the installs are complete</p> <p>[Jack's RPI]{.mark}</p> <p>On the PC</p> <p>Activate the virtual environment</p> <p>cd projects</p> <p>source \\~/env/bin/activate</p> <p>ssh pi@[jackrpi02.local]{.mark}</p> <p>On the RPI</p> <p>(env)pi@jackrpi02:\\~ \\$</p> <p>cd d2t</p> <p>python manage.py drive</p> <p>Drive the robot to collect data</p> <p>On the PC</p> <p>Get data from RPI</p> <p>(env) jack@virtlnx01:\\~/projects/d2t\\$</p> <p>rsync -a --progress pi@[jackrpi02.local]{.mark}:\\~/projects/d3/data \\~/projects/d2t</p> <p>(env) jack@virtlnx01:\\~/projects/d2t\\$</p> <p>ls data</p> <p>tub_1_17-10-12</p> <p>Train a model using all tubes in the data directory</p> <p>(env) jack@lnxmbp01:\\~/projects/d2t\\$</p> <p>python train.py --model=models/[date_model_name]{.mark}.h5</p> <p>To use on particular tube</p> <p>python train.py --tub \\~/projects/d2t/data/tub_1_18-01-07 --model=models/07jan18_coleman_tube1Test.h5</p> <p>To make an incremental training using a previous model</p> <p>python train.py --tub \\~/projects/d2t/data/NAME_OF_NEW_TUBE --transfer=models/NAME_OF_PREVIOUS_MODEL.h5 --model=models/NAME_OF_NEW_MODEL.h5</p> <p>To clean-up tubs</p> <p>(env) jack@LnxWS1:\\~/projects/d2t\\$</p> <p>donkey tubclean data</p> <p>using donkey v2.2.0 ...</p> <p>Listening on 8886...</p> <p>Open a browser and type</p> <p>[http://localhost:8886]{.underline}</p> <p>{width=\"5.744792213473316in\" height=\"3.2466776027996502in\"}</p> <p>You can clean-up your tub directories. Please make a backup of your data before you start to clean it up.</p> <p>On the mac if the training complains</p> <p>rm \\~/projects/d2t/data/.DS_Store</p> <p>If it complains about docopt, install it again. And I did not change anything from the previous day. Go figure...</p> <p>(env) jack@lnxmbp01:\\~/projects/d2\\$ pip list</p> <p>(env) jack@lnxmbp01:\\~/projects/d2\\$ pip install docopt</p> <p>Collecting docopt</p> <p>Installing collected packages: docopt</p> <p>Successfully installed docopt-0.6.2</p> <p>(env) jack@virtlnx01:\\~/projects/d2\\$</p> <p>python train.py --model=models/[12oct17_ucsd_day1.h5]{.mark}</p> <p>See the models here</p> <p>(env) jack@virtlnx01:\\~/projects/d2\\$</p> <p>ls models</p> <p>[ucsd_12oct17.h5]{.mark}</p> <p>Place Autopilot into RPI</p> <p>(env) jack@lnxmbp01:\\~/projects/d2\\$</p> <p>rsync -a --progress \\~/projects/d2t/models/ pi@[jackrpi02.local:]{.mark}\\~/projects/d3/models/</p> <p>(dk)pi@jackrpi02:\\~/d2 \\$</p> <p>ls models</p> <p>[ucsd_12oct17.h5]{.mark}</p> <p>On the RPI</p> <p>Run AutoPilot at the RPI</p> <p>(dk)pi@jackrpi02:\\~/d2 \\$</p> <p>python manage.py drive --model=./models/[ucsd_12oct17.h5]{.mark}</p> <p>...</p> <p>Using TensorFlow backend.</p> <p>loading config file: /home/pi/d2/config.py</p> <p>config loaded</p> <p>PiCamera loaded.. .warming camera</p> <p>Starting vehicle...</p> <p>/home/pi/env/lib/python3.4/site-packages/picamera/encoders.py:544: PiCameraResolutionRounded: frame size rounded up from 160x120 to 160x128</p> <p>width, height, fwidth, fheight)))</p> <p>...</p> <p>Check Tub</p> <p>This command allows you to see how many records are contained in any/all tubs. It will also open each record and ensure that the data is readable and intact. If not, it will allow you to remove corrupt records.</p> <p>Usage:</p> <p>[donkey tubcheck \\&lt;tub_path&gt; [--fix]\\ ]{.mark}</p> <ul> <li> <p>Run on the host computer or the robot</p> </li> <li> <p>It will print summary of record count and channels recorded for each     tub</p> </li> <li> <p>It will print the records that throw an exception while reading</p> </li> <li> <p>The optional [--fix]{.mark} will delete records that have problem</p> </li> </ul> <p>PS3 Controller Modes</p> <p>The default mode will be that [User]{.mark} is in Control. That is, the user controls Steering and Throttle.</p> <p>To switch to [Local Angle]{.mark} (software controls the Steering and user the Throttle), you need to press the \\&lt;Select&gt; button in the Joystick.</p> <p>If you give Throttle the Robocar should drive around semi-autonomously.</p> <p>After few laps that you see that your model is good,</p> <p>[Please hold your robot with the wheels out of the floor]{.mark}</p> <p>you can press the \\&lt;Start&gt; button [and immediately press the \\&lt;left_DOWN_arrow&gt; button few times to decrease the Throttle as needed.]{.mark} This is important so you slow down the Robocar for a constant Throttle.</p> <p>Press the \\&lt;left_UP_arrow&gt; to give it more Throttle as needed.</p> <p>Pressing \\&lt;[X&gt;]{.mark} will stop the robocar and go back to User mode (user is in control)</p> <p>You can change the driving modes by pressing the \\&lt;Select&gt; button. You should be able to see a message on your computer terminal that is SSH connected to the RoboCar RPI.</p> <p>The Local &amp; Angle mode (fully autonomous) is to be used after you see that you can do few laps with local angle</p> <p>Hit Select button to toggle between three modes - User, Local Angle, and Local Throttle &amp; Angle.</p> <ul> <li> <p>User - User controls both steering and throttle with joystick</p> </li> <li> <p>Local Angle - Ai controls steering. User controls throttle.</p> </li> <li> <p>Local Throttle &amp; Angle - Ai controls both steering and throttle</p> </li> </ul> <p>When the car is in Local Angle mode, the NN will steer. You must provide throttle...</p> <p>Ideally you will have \\~ 60 laps</p> <p>If you don't have a good working Auto-Pilot, get more data in 10 laps increments.</p> <p>In summary, you may want to start with 60 laps and then do 10\\~20 laps more to see if the model gets better.</p> <p>I would not worry much about few bad spots when collecting data. Drive the car back to the track,</p> <p>then press Green_Triangle to delete the last 5s of data.</p> <p>Keep driving, you will develop good skills, you will get good data and better models. If you leave the track, just drive the RoboCar back to track. It may even learn how to get back to track.</p> <p>If you keep the data from the same track (ex: UCSD Track) in the d2t/data directory, as you add more files to it (e.g., tub_[5]{.mark}_17-10-13) it will help your model. At the same time it will take more time to train since your model will read all the data sets in the directory. You can use transfer model to add new data to a current model.</p> <p>Incremental training using a previous model</p> <p>python train.py --tub \\~/projects/d2t/data/NAME_OF_NEW_TUBE_DATA --transfer=models/NAME_OF_PREVIOUS_MODEL.h5 --model=models/NAME_OF_NEW_MODEL.h5</p> <p>Some Advanced Tools</p> <p>The visualization tool is to be use on your PC. Please even if you can, please do not use the GPU Cluster Resources for this.</p> <p>Visualizing the model driving the car vs. human driver</p> <p>Install OpenCV</p> <p>sudo apt-get install python-opencv</p> <p>pip3 install opencv-python</p> <p>donkey makemovie --tub=data\\tub_file --model=models\\model_name.h5 --limit=100 --salient --scale=2</p> <p>example</p> <p>donkey makemovie --tub=data/tub_9_19-01-19 --model=models/19jan19_oakland_5.h5 --start 1 --end 1000 --salient --scale=2</p> <p>Here are installs with very limited or no support in this course.</p> <p>There are too many computers and OS variations to support...</p> <p>28Aug20</p> <p>The installation on Linux(ubuntu) is much easier now using Conda</p> <p>See docs.donkeycar.com</p> <p>[https://docs.donkeycar.com/guide/host_pc/setup_ubuntu/]{.underline}</p> <p>I will stop maintaining the installing of the donkeycar into the host computers.</p> <p>Ubuntu 18.04 - Tensorflow-GPU - CUDA Install - NVIDIA GPUs with CUDA Cores</p> <p>If your computer has a NVIDIA GPU with CUDA cores, you can take advantage of the GPU to</p> <p>accelerate the AI training. Based on the class experience, results varies between</p> <p>3\\~10 times faster when comparing to CPU training</p> <p>Installing CUDA related files and Tensorflow GPU</p> <p>If you want to try the easier way but not the latest files, we have a compressed Zip file</p> <p>with the necessary files for Linux Ubuntu 64 bits</p> <p>and Tensorflow 1.12 - GPU (latest version on Feb 2019) - cudnn-10.0-linux-x64-v7.4.1.5,</p> <p>libcudnn7_7.4.1.5-1+cuda10.0_amd64</p> <p>[Tensorflow1.12 with GPU CUDA10 cuddnn7.4.2.24 Compute Capabilties_3.0_5.2_6.1_7.0 for Linux]{.mark}</p> <p>[The link below has the Tensorflow and CUDA supporting files for Linux Ubuntu]{.mark}</p> <p>/* {width=\"3.7864588801399823in\" height=\"0.39585739282589677in\"}</p> <p>*/</p> <p>[https://drive.google.com/open?id=1xfbn_qy77SjXqpfWQWwti5JFWWtJVjIM]{.underline}</p> <p>Alternatively, you can download the files from the NVIDIA site as long as you use the same versions</p> <p>for cudnn and others libraries</p> <p>Or build Tensorflow from source with the version of the CUDA files you have installed.</p> <p>Building Tensorflow from source can take a few hours even on modern I7s CPU with SSD disk and</p> <p>lots of RAM...</p> <p>Skip these downloads if you are using the files included in the Zipped file listed above.</p> <p>Otherwise you can get the files from the NVIDIA site</p> <p>Download [http://developer.nvidia.com/cuda-downloads]{.underline}</p> <p>Download Installer for Linux Ubuntu 18.04 x86_64</p> <p>The base installer is available for download below.</p> <p>As of 17Apr20</p> <p>https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1804&amp;target_type=deblocal</p> <p>At the directory you download files to install</p> <p>mkdir cuda</p> <p>mkdir 10.2</p> <p>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin</p> <p>sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600</p> <p>wget http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb</p> <p>sudo dpkg -i cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb</p> <p>sudo apt-key add /var/cuda-repo-10-2-local-10.2.89-440.33.01/7fa2af80.pub</p> <p>sudo apt-get update</p> <p>sudo apt-get -y install cuda</p> <p>09Aug20</p> <p>https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=2004&amp;target_type=deblocal</p> <p>28Aug20</p> <p>https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=2004&amp;target_type=deblocal</p> <p>CUDA 11</p> <p>{width=\"2.604530839895013in\" height=\"3.0677088801399823in\"}</p> <p>Base Installer</p> <p>Installation Instructions:</p> <p>cd projects</p> <p>mkdir cuda</p> <p>cd cuda</p> <p>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin</p> <p>sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600</p> <p>wget https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda-repo-ubuntu2004-11-0-local_11.0.3-450.51.06-1_amd64.deb</p> <p>sudo dpkg -i cuda-repo-ubuntu2004-11-0-local_11.0.3-450.51.06-1_amd64.deb</p> <p>sudo apt-key add /var/cuda-repo-ubuntu2004-11-0-local/7fa2af80.pub</p> <p>sudo apt-get update</p> <p>sudo apt-get -y install cuda</p> <p>echo \\'export PATH=/usr/local/cuda/bin\\${PATH:+:\\${PATH}}\\' &gt;&gt; \\~/.bashrc</p> <p>reboot the machine</p> <p>As 28aug20 nvidia driver 450.51.06 is the latest</p> <p>After rebooted, nvcc-V and nvidia-smi worked</p> <p>nvidia-smi</p> <p>{width=\"4.557292213473316in\" height=\"2.899010279965004in\"}</p> <p>{width=\"3.494792213473316in\" height=\"1.6441896325459318in\"}</p> <p>[https://developer.nvidia.com/rdp/cudnn-download]{.underline}</p> <p>{width=\"2.2031255468066493in\" height=\"2.3837095363079617in\"}</p> <p>download both these files</p> <p>cuDNN Library for Linux (x86_64)</p> <p>Extract cudnn-11.0-linux-x64-v8.0.2.39.tgz</p> <p>Make sure you are in the directory where you downloaded the files</p> <p>tar -xf cudnn-11.0-linux-x64-v8.0.2.39.tgz</p> <p>cd cudnn-11.0-linux-x64-v8.0.2.39</p> <p>We have cuda 11.0 at</p> <p>/usr/local/cuda-11.0</p> <p>sudo cp cuda/include/cudnn.h /usr/local/cuda/include/</p> <p>sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/</p> <p>sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*</p> <p>sudo cp -R cuda/include/* /usr/local/cuda-11.0/include</p> <p>sudo cp -R cuda/lib64/* /usr/local/cuda-11.0/lib64</p> <p>Install libcudnn8_8.0.2.39-1+cuda11.0_amd64.deb</p> <p>sudo dpkg -i libcudnn8_8.0.2.39-1+cuda11.0_amd64.deb</p> <p>Install tensorflow with GPU support</p> <p>For a ubuntu server lets make tensor available without virtual env.</p> <p>For workstations and donkeycar, see below for virtualenv install</p> <p>sudo apt install python3-pip</p> <p>pip3 install --upgrade tensorflow-gpu</p> <p>Lets test the tensorflow-gpu install</p> <p>python3</p> <p>import tensorflow as tf</p> <p>tf.config.list_physical_devices(\\'GPU\\')</p> <p>exit()</p> <p>Optional</p> <p>Let's install NCCL. NCCL is NVIDIA optimization for multi-GPU use.</p> <p>Ex: Use GPU in the computer in one external such as eGPU.</p> <p>[https://developer.nvidia.com/nccl/nccl-download]{.underline}</p> <p>{width=\"3.7239588801399823in\" height=\"0.7391491688538933in\"}</p> <p>Download and install NCCL using Gdeb package install</p> <p>O/S agnostic local installer</p> <p>cd to the download directory</p> <p>cd \\~/Downloads/cuda/10.1</p> <p>tar -xf nccl_2.4.8-1+cuda10.1_x86_64.txz</p> <p>cd nccl_2.4.8-1+cuda10.1_x86_64</p> <p>sudo cp -R * /usr/local/cuda-10.1/targets/x86_64-linux/</p> <p>sudo ldconfig</p> <p>Based on the NVIDIA video driver and CUDA that I installed</p> <p>Pay attention to the version listed ex: CUDA 10.2</p> <p>Adapt 10.1 to 10.2 or whatever version you installed</p> <p>nvcc -V</p> <p>Command \\'nvcc\\' not found, but can be installed with:</p> <p>sudo apt install nvidia-cuda-toolkit</p> <p>This line should make nvcc -V work</p> <p>echo \\'export PATH=/usr/local/cuda/bin\\${PATH:+:\\${PATH}}\\' &gt;&gt; \\~/.bashrc</p> <p>adminlnx@lnxsrv6:\\~\\$ nvcc -V</p> <p>nvcc: NVIDIA (R) Cuda compiler driver</p> <p>Copyright (c) 2005-2020 NVIDIA Corporation</p> <p>Built on Wed_Jul_22_19:09:09_PDT_2020</p> <p>Cuda compilation tools, release 11.0, V11.0.221</p> <p>Build cuda_11.0_bu.TC445_37.28845127_0</p> <p>adminlnx@lnxsrv6:\\~\\$</p> <p>Double checking</p> <p>adminlnx@lnxsrv6:\\~\\$ cd Downloads</p> <p>adminlnx@lnxsrv6:\\~/Downloads\\$ cd cuda</p> <p>adminlnx@lnxsrv6:\\~/Downloads/cuda\\$ cd 11</p> <p>adminlnx@lnxsrv6:\\~/Downloads/cuda/11\\$ sudo apt-get -y install cuda</p> <p>[sudo] password for adminlnx:</p> <p>Reading package lists... Done</p> <p>Building dependency tree</p> <p>Reading state information... Done</p> <p>cuda is already the newest version (11.0.3-1).</p> <p>0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.</p> <p>We have the latest version installed...</p> <p>We will install the nvidia-cuda-toolkit next</p> <p>Easier way? But older cuDNN</p> <p>[https://linuxconfig.org/how-to-install-cuda-on-ubuntu-20-04-focal-fossa-linux]{.underline}</p> <p>Although you might not end up with the latest CUDA toolkit version, the easiest way to install CUDA on Ubuntu 20.04 is to perform the installation from Ubuntu\\'s standard repositories.</p> <p>To install CUDA execute the following commands:</p> <p>\\$ sudo apt update</p> <p>\\$ sudo apt install nvidia-cuda-toolkit</p> <p>{width=\"5.536458880139983in\" height=\"1.417668416447944in\"}</p> <p>All should be ready now. Check your CUDA version:</p> <p>If not, try this</p> <p>echo \\'export PATH=/usr/local/cuda/bin\\${PATH:+:\\${PATH}}\\' &gt;&gt; \\~/.bashrc</p> <p>\\$ nvcc --version</p> <p>nvcc: NVIDIA (R) Cuda compiler driver</p> <p>Copyright (c) 2005-2019 NVIDIA Corporation</p> <p>Built on Sun_Jul_28_19:07:16_PDT_2019</p> <p>Cuda compilation tools, release 10.1, V10.1.243</p> <p>As 09Ag20, 10.1 is not the latest version of CUDA toolkit.</p> <p>Let me install it manually</p> <p>[https://developer.nvidia.com/rdp/cudnn-download]{.underline}</p> <p>As of 23Apr20</p> <p>cuDNN7.6.5</p> <p>Adjust instructions accordingly</p> <p>{width=\"6.875in\" height=\"0.5694444444444444in\"}</p> <p>Download cuDNN Runtime Library for Ubuntu18.04 (Deb)</p> <p>libcudnn7_7.6.2.24-1+cuda10.1_amd64.deb</p> <p>Download cuDNN v7.5.1 for CUDA 10.1</p> <p>cuDNN Runtime Library for Ubuntu18.04 (Deb)</p> <p>get Gdebi package and install libcudnn using Gdebi</p> <p>install libcudnn7_7.6.2.24-1+cuda10.1_amd64.deb</p> <p>Type this to add the CUDA 10.1 to the path</p> <p>export PATH=/usr/local/cuda-10.1/bin\\${PATH:+:\\${PATH}}</p> <p>export PATH=/usr/local/cuda-10.2/bin\\${PATH:+:\\${PATH}}</p> <p>Then nvcc -V worked</p> <p>{width=\"5.307292213473316in\" height=\"0.860424321959755in\"}</p> <p>note release 10.1, adjust for 10.2 or other version you may be installing</p> <p>Now that it works, let\\'s add the path to be persistent\\ add at the end of the file \\~./profile\\ nano \\~/.profile</p> <p>Add the PATH to include cuda-10.1 ...</p> <p>set PATH so it includes user\\'s private bin if it exists</p> <p>if [ -d \\\"\\$HOME/.local/bin\\\" ] ; then</p> <p>PATH=\\\"\\$HOME/.local/bin:\\$PATH\\\"</p> <p>fi</p> <p>PATH=/usr/local/cuda-10.2/bin\\${PATH:+:\\${PATH}}</p> <p>reboot to test nvidia-smi and nvcc -V</p> <p>when installing CUDA it seems it reverted my NVIDIA driver to 4.10</p> <p>let me install version 430 (the latest as 11May19), I used the</p> <p>Ubuntu software update / additional drivers - then I tried again nvidia-smi and nvcc -V</p> <p>{width=\"3.9140627734033244in\" height=\"2.6093755468066493in\"}</p> <p>{width=\"5.380208880139983in\" height=\"2.796078302712161in\"}</p> <p>[https://developer.nvidia.com/rdp/cudnn-download]{.underline}</p> <p>{width=\"5.609375546806649in\" height=\"1.1643700787401574in\"}</p> <p>cudnn-10.1-linux-x64-v7.6.2.24.tgz</p> <p>cudnn-10.2-linux-x64-v7.6.5.32.tgz</p> <p>Make sure you are in the directory where you downloaded the files</p> <p>tar -xf cudnn-10.1-linux-x64-v7.6.2.24.tgz</p> <p>for cudnn 10.2</p> <p>tar -xf cudnn-10.2-linux-x64-v7.6.5.32.tgz</p> <p>sudo cp -R cuda/include/* /usr/local/cuda-10.2/include\\ sudo cp -R cuda/lib64/* /usr/local/cuda-10.2/lib64</p> <p>Optional</p> <p>Let's install NCCL. NCCL is NVIDIA optimization for multi-GPU use.</p> <p>Ex: Use GPU in the computer in one external such as eGPU.</p> <p>[https://developer.nvidia.com/nccl/nccl-download]{.underline}</p> <p>{width=\"3.7239588801399823in\" height=\"0.7391491688538933in\"}</p> <p>Download and install NCCL using Gdeb package install</p> <p>O/S agnostic local installer</p> <p>cd to the download directory</p> <p>cd \\~/Downloads/cuda/10.1</p> <p>tar -xf nccl_2.4.8-1+cuda10.1_x86_64.txz</p> <p>cd nccl_2.4.8-1+cuda10.1_x86_64</p> <p>sudo cp -R * /usr/local/cuda-10.1/targets/x86_64-linux/</p> <p>sudo ldconfig</p> <p>29Aug20</p> <p>TensorFlow 2.3 on Ubuntu 20.04 LTS with CUDA 11.0 and CUDNN 8.0</p> <p>[https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03]{.underline}</p> <p>[https://medium.com/@cwbernards/tensorflow-2-3-on-ubuntu-20-04-lts-with-cuda-11-0-and-cudnn-8-0-fb136a829e7f]{.underline}</p> <p>After installing NVIDIA drivers and cuDNN or here in this same document I have examples</p> <p>Please refer to my instructions here.</p> <p>Now lets install Tensorflow</p> <p>If you don't have the python3 virtual environment, create one</p> <p>I am using \\~/projects/envs/env1</p> <p>sudo apt-get update</p> <p>sudo apt-get install virtualenv</p> <p>virtualenv --system-site-packages -p python3 \\~/projects/envs/env1</p> <p>Activate your virtual environment</p> <p>source \\~/projects/envs/env1/bin/activate</p> <p>(env) jack@lnxmbp01:\\~/projects\\$</p> <p>easy_install -U pip</p> <p>To install the latest version of tensorflow with GPU support, you can use</p> <p>pip3 install --upgrade tensorflow-gpu</p> <p>The version on the Zipped file is already old. Left available for testing only</p> <p>(if you have new CPUs like Intel I5 and I7), you can use the tensorflow file included</p> <p>in the downloaded Zip file earlier in these instructions.</p> <p>{width=\"4.359375546806649in\" height=\"0.45575240594925637in\"}</p> <p>tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl</p> <p>[Or alternatively if you believe the released tensorflow use the same CUDA install you have]{.mark}</p> <p>[running in your computer, you can use]{.mark}</p> <p>[pip3 install --upgrade tensorflow-gpu]{.mark}</p> <p>[Note on using the latest CUDA version. The packages available at the standard repository may]{.mark}</p> <p>[not work with the latest CUDA such as CUDA 10]{.mark}</p> <p>[You need to download Tensorflow that was built for the version of CUDA and relate]{.mark}</p> <p>[software you installed]{.mark}</p> <p>[You can install a particular version of Tensorflow such as 1.4]{.mark}</p> <p>[pip3 install --upgrade tensorflow-gpu==1.4]{.mark}</p> <p>[or 1.3.1]{.mark}</p> <p>[pip3 install --upgrade tensorflow-gpu==1.3.0]{.mark}</p> <p>To install a particular Tensorflow such as the *.whl file included at the Zipped</p> <p>file you have downloaded</p> <p>use the Tensorflow file name you downloaded</p> <p>pip3 install --upgrade tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl</p> <p>you need to be in the same directory where the file is or specify the complete path.</p> <p>I usually copy over the tensorflow .whl file to my projects directory and install it from there.</p> <p>After a successful install</p> <p>Run a short Python program to test the Tensorflow install</p> <p>(env) jack@lnxmbp01:\\~/projects\\$</p> <p>python</p> <p>Enter the following txt, you can cut and paste</p> <p>Python</p> <p>import tensorflow as tf</p> <p>hello = tf.constant(\\'Hello, TensorFlow!\\')</p> <p>sess = tf.Session()</p> <p>print(sess.run(hello))</p> <p>It worked.</p> <p>*2018-11-28 03:33:42.219251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 913 MB memory) -&gt; physical GPU (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0, compute capability: 3.0)\\ &gt;&gt;&gt; print(sess.run(hello))\\ b\\'Hello, TensorFlow!\\'\\ &gt;&gt;&gt;\\ *</p> <p>end of the Tesnforflow GPU install on Linux Ubuntu</p> <p>Some previous Instructions and for older GPUs</p> <p>How to install Tenforflow with GPU support</p> <p>Note the instructions below are for an older NVIDIA GPUs. It will work for modern NVIDIA GPUs</p> <p>but it wont take advantage of the new features and all its performance</p> <p>You can adapt the instructions to have the latest CUDA, cuDNN, and latest Tensorflow with GPU</p> <p>support.</p> <p>On some Macbook Pro (\\~2013) you may have NVIDIA dGPU (dedicated GPU).</p> <p>It may require CUDA 8</p> <p>Make sure you have the NVIDIA driver installed</p> <p>{width=\"6.5in\" height=\"1.4444444444444444in\"}</p> <p>{width=\"6.5in\" height=\"4.291666666666667in\"}</p> <p>Download the CUDA files from here</p> <p>[https://developer.nvidia.com/cuda-toolkit-archive]{.underline}</p> <p>{width=\"6.5in\" height=\"2.7222222222222223in\"}</p> <p>Download cuDNN v6.0 for CUDA 8 from here</p> <p>[https://developer.nvidia.com/rdp/cudnn-archive]{.underline}</p> <p>{width=\"6.5in\" height=\"1.0277777777777777in\"}</p> <p>{width=\"6.5in\" height=\"0.625in\"}</p> <p>{width=\"6.5in\" height=\"1.6666666666666667in\"}</p> <p>Open a terminal and navigate to where you saved the files</p> <p>Then issue the following commands</p> <p>sudo apt-get update</p> <p>sudo apt-get install gdebi</p> <p>sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb</p> <p>sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-cublas-performance-update_8.0.61-1_amd64.deb</p> <p>sudo dpkg -i libcudnn6_6.0.21-1+cuda8.0_amd64.deb</p> <p>sudo apt-get update</p> <p>sudo apt-get install cuda</p> <p>Let it install. It may take a while.</p> <p>...</p> <p>Setting up cuda-toolkit-8-0 (8.0.61-1) ...</p> <p>Setting up cuda-drivers (375.26-1) ...</p> <p>Setting up cuda-runtime-8-0 (8.0.61-1) ...</p> <p>Setting up cuda-demo-suite-8-0 (8.0.61-1) ...</p> <p>Setting up cuda-8-0 (8.0.61-1) ...</p> <p>Setting up cuda (8.0.61-1) ...</p> <p>Processing triggers for initramfs-tools (0.130ubuntu3.5) ...</p> <p>update-initramfs: Generating /boot/initrd.img-4.15.0-36-generic</p> <p>Processing triggers for libc-bin (2.27-3ubuntu1) ...</p> <p>....</p> <p>Lets test the install</p> <p>nvcc --version</p> <p>Command \\'nvcc\\' not found, but can be installed with:</p> <p>export PATH=/usr/local/cuda-8.0/bin\\${PATH:+:\\${PATH}}</p> <p>nvcc --version</p> <p>nvcc: NVIDIA (R) Cuda compiler driver</p> <p>Copyright (c) 2005-2016 NVIDIA Corporation</p> <p>Built on Tue_Jan_10_13:22:03_CST_2017</p> <p>Cuda compilation tools, release 8.0, V8.0.61</p> <p>nano \\~/.profile</p> <p>Add this line to the end of the file</p> <p>export PATH=/usr/local/cuda-8.0/bin\\${PATH:+:\\${PATH}}</p> <p>sudo reboot now</p> <p>Open a terminal and run again</p> <p>nvcc --version</p> <p>nvidia-smi</p> <p>{width=\"6.5in\" height=\"3.736111111111111in\"}</p> <p>Now lets install Tensorflow-GPU and test it</p> <p>sudo apt-get update</p> <p>sudo apt-get install virtualenv build-essential python3-dev gfortran libhdf5-dev libatlas-base-dev\\ virtualenv env -p python3\\ source env/bin/activate\\ pip install keras==2.2.2</p> <p>pip install tensorflow-gpu==1.3.0</p> <p>Lets Test Tensorflow\\ python</p> <p>Enter the following lines, it is a short program, inside the python interactive shell:</p> <p>Python\\ import tensorflow as tf\\ hello = tf.constant(\\'Hello, TensorFlow!\\')\\ sess = tf.Session()\\ print(sess.run(hello))</p> <p>{width=\"6.5in\" height=\"5.736111111111111in\"}</p> <p>It worked. Note that Tensorflow can see the dGPU</p> <p>Also try this when inquiring about the Tensorflow version you have installed</p> <p>pip3 show tensorflow</p> <p>Install CUDA to enable GPU Support on Ubuntu 16.04 - Not supported in this course</p> <p>[http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.htmlpost-installation-actions]{.underline}</p> <p>Installing CUDA on Ubuntu 16.04 to enable using GPU computation such as TensorFlow.</p> <p>First I installed the latest NVIDIA Driver from the Ubuntu setup 3rd party driver</p> <p>System Settings/Software &amp; Updates/Additional Drivers</p> <p>NVIDIA xxx Proprietary Tested</p> <p>CUDA Install on Ubuntu 16.04</p> <p>Note: Initially I installed the latest CUDA (9), I had to revert to CUDA 8 install because Tensorflow still looks for it.</p> <p>In the future it may work with CUDA 9</p> <p>Installing CUDA on Ubuntu 16.04 to enable using GPU computation such as TensorFlow.</p> <p>First I installed the latest NVIDIA Driver from the Ubuntu setup 3rd party driver</p> <p>System Settings/Software &amp; Updates/Additional Drivers</p> <p>NVIDIA xxx Proprietary Tested</p> <p>To remove other CUDA and NVIDIA installs</p> <p>[https://devtalk.nvidia.com/default/topic/903867]{.underline}</p> <p>[sudo apt-get remove --purge nvidia-*]{.mark}</p> <p>After this command, you need to enable the 3rd party video driver again</p> <p>If needed, remove the repository that may be installing CUDA 9.</p> <p>[https://askubuntu.com/questions/43345/how-to-remove-a-repository]{.underline}</p> <p>Download Installers for Linux Ubuntu 16.04 x86_64</p> <p>[https://developer.nvidia.com/cuda-80-ga2-download-archive]{.underline}</p> <p>Get the base installer and patch</p> <p>Get CUDA 8 (because Tensorflow was/is not compatible with CUDA 9 yet)</p> <p>jack@lnxmbp01:\\~/Downloads/NVIDIA\\$</p> <p>sudo apt-get update</p> <p>sudo apt-get install gdebi</p> <p>sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb</p> <p>sudo dpkg -i cuda-repo-ubuntu1604-8-0-local-cublas-performance-update_8.0.61-1_amd64.deb</p> <p>jack@lnxmbp01:\\~/Downloads/NVIDIA\\$ sudo apt-get update</p> <p>jack@lnxmbp01:\\~/Downloads/NVIDIA\\$ sudo apt-get install cuda</p> <p>Let it install. It may take a while.</p> <p>jack@lnxmbp01:\\~\\$ [nvcc --version]{.mark}</p> <p>The program \\'nvcc\\' is currently not installed. You can install it by typing:</p> <p>sudo apt install nvidia-cuda-toolkit</p> <p>Cuda8</p> <p>jack@lnxmbp01:\\~\\$ [export PATH=/usr/local/cuda-8.0/bin\\${PATH:+:\\${PATH}}]{.mark}</p> <p>jack@lnxmbp01:\\~\\$ nvcc -V</p> <p>nvcc: NVIDIA (R) Cuda compiler driver</p> <p>Copyright (c) 2005-2016 NVIDIA Corporation</p> <p>Built on Tue_Jan_10_13:22:03_CST_2017</p> <p>Cuda compilation tools, release 8.0, V8.0.61</p> <p>If you see an error with libcudnn, check versions. CUDA and Tensorflow need to play nice together with particular versions specially for older GPUs.</p> <p>ImportError: [libcudnn.so.6]{.mark}: cannot open shared object file: No such file or directory\\ Fixed it! When I installed CUDA 8, I originally I had cudnn7 I had to download cudnn6 from here [https://developer.nvidia.com/rdp/cudnn-downloada-collapse6-8]{.underline}</p> <p>[Download cuDNN v6.0 (April 27, 2017), for CUDA 8.0]{.underline}://developer.nvidia.com/rdp/cudnn-download\\ Need to install cuDNN V6.</p> <p>Verify CUDA installation</p> <p>Reboot, if not the nvidia-smi command may not work</p> <p>jack@lnxmbp01:\\~\\$ nvcc --version</p> <p>The program \\'nvcc\\' is currently not installed. You can install it by typing:</p> <p>sudo apt install nvidia-cuda-toolkit</p> <p>jack@lnxmbp01:\\~\\$ export PATH=/usr/local/cuda-9.0/bin\\${PATH:+:\\${PATH}}</p> <p>jack@lnxmbp01:\\~\\$ [nvcc --version]{.mark}</p> <p>nvcc: NVIDIA (R) Cuda compiler driver</p> <p>Copyright (c) 2005-2017 NVIDIA Corporation</p> <p>Built on Fri_Sep__1_21:08:03_CDT_2017</p> <p>Cuda compilation tools, release 9.0, V9.0.176</p> <p>jack@lnxmbp01:\\~\\$ [nvidia-smi]{.mark}</p> <p>Sat Sep 30 13:22:52 2017</p> <p>+-----------------------------------------------------------------------------+</p> <p>| NVIDIA-SMI 384.81 Driver Version: 384.81 |</p> <p>|-------------------------------+----------------------+----------------------+</p> <p>| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |</p> <p>| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |</p> <p>|===============================+======================+======================|</p> <p>| 0 GeForce GT 750M Off | 00000000:01:00.0 N/A | N/A |</p> <p>| N/A 67C P0 N/A / N/A | 403MiB / 1998MiB | N/A Default |</p> <p>+-------------------------------+----------------------+----------------------+</p> <p>+-----------------------------------------------------------------------------+</p> <p>| Processes: GPU Memory |</p> <p>| GPU PID Type Process name Usage |</p> <p>|=============================================================================|</p> <p>| 0 Not Supported |</p> <p>+-----------------------------------------------------------------------------+</p> <p>For CUDA 9.2</p> <p>jack@lnx01:\\~\\$ nvcc -V</p> <p>Command \\'nvcc\\' not found, but can be installed with:</p> <p>sudo apt install nvidia-cuda-toolkit</p> <p>jack@lnx01:\\~\\$ export PATH=/usr/local/cuda-9.2/bin\\${PATH:+:\\${PATH}}</p> <p>jack@lnx01:\\~\\$ nvcc -V</p> <p>nvcc: NVIDIA (R) Cuda compiler driver</p> <p>Copyright (c) 2005-2018 NVIDIA Corporation</p> <p>Built on Tue_Jun_12_23:07:04_CDT_2018</p> <p>Cuda compilation tools, release 9.2, V9.2.148</p> <p>Now that ii works, lets add the path to be persistent</p> <p>jack@lnxmbp01:\\~\\$ nano \\~/.profile</p> <p>..</p> <p>set PATH so it includes user\\'s private bin directories</p> <p>PATH=\\\"\\$HOME/bin:\\$HOME/.local/bin:\\$PATH\\\"</p> <p>[PATH=\\\"/usr/local/cuda-8.0/bin\\${PATH:+:\\${PATH}}\\\"]{.mark}</p> <p>PATH=/usr/local/cuda-9.2/bin\\${PATH:+:\\${PATH}}</p> <p>Installing TensorFlow with GPU Support on Ubuntu 16.04- not supported in this course</p> <p>[CUDA needs to be installed and tested first]{.mark} - search for Install CUDA in this document.</p> <p>[https://www.tensorflow.org/install/install_linuxdetermine_which_tensorflow_to_install]{.underline}</p> <p>[https://www.tensorflow.org/install/install_linuxInstallingVirtualenv]{.underline}</p> <p>[https://github.com/mind/wheels/releases/]{.underline}</p> <p>sudo apt-get update</p> <p>sudo apt-get upgrade</p> <p>sudo apt-get install virtualenv</p> <p>mkdir projects</p> <p>jack@lnxmbp01:\\~/projects\\$</p> <p>virtualenv --system-site-packages -p python3 \\~/projects/env</p> <p>jack@lnxmbp01:\\~/projects\\$</p> <p>source \\~/projects/env/bin/activate</p> <p>(env) jack@lnxmbp01:\\~/projects\\$</p> <p>(env) jack@lnxmbp01:\\~/projects\\$</p> <p>easy_install -U pip</p> <p>(env) jack@lnxmbp01:\\~/projects\\$</p> <p>pip3 install --upgrade tensorflow[-gpu]{.mark}</p> <p>Successfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.6.9 numpy-1.13.3 protobuf-3.4.0 six-1.11.0 tensorflow-gpu-1.3.0 tensorflow-tensorboard-0.1.8 werkzeug-0.12.2</p> <p>To install a particular version of tensorflow, example</p> <p>[pip3 install --upgrade tensorflow-gpu==1.4]{.mark}</p> <p>[(env) jack@lnxmbp01:\\~/projects\\$]{.mark}</p> <p>[pip3 install --upgrade tensorflow-gpu==1.3.0]{.mark}</p> <p>Run a short TensorFlow program to test it</p> <p>(env) jack@lnxmbp01:\\~/projects\\$</p> <p>python</p> <p>Python 3.5.2 (default, Sep 14 2017, 22:51:06)</p> <p>[GCC 5.4.0 20160609] on linux</p> <p>Type \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more information.</p> <p>&gt;&gt;&gt;</p> <p>Enter the following txt, you can cut and paste</p> <p>Python</p> <p>import tensorflow as tf</p> <p>hello = tf.constant(\\'Hello, TensorFlow!\\')</p> <p>sess = tf.Session()</p> <p>print(sess.run(hello))</p> <p>The result should have this at the end.</p> <p>...</p> <p>&gt;&gt;&gt; print(sess.run(hello))</p> <p>b\\'Hello, TensorFlow!\\'</p> <p>&gt;&gt;&gt;</p> <p>Also try this when inquiring about the Tensorflow version you have installed</p> <p>pip3 show tensorflow</p> <p>Install TensorFlow on MacOS - not supported in this course</p> <p>[From Tawn Kramer Instructions]{.underline}</p> <p>[Install miniconda Python 3.6 64 bit]{.underline}</p> <p>[https://conda.io/docs/user-guide/tasks/manage-python.html]{.underline}</p> <p>[https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/20/conda/]{.underline}</p> <p>[https://conda.io/docs/commands.htmlconda-general-commands]{.underline}</p> <p>[Install git 64 bit]{.underline}\\ \\ Start Terminal\\ \\ cd \\~\\ mkdir projects\\ cd projects</p> <p>Download the latest version of miniconda for macos 64 bits from [https://conda.io/miniconda.html]{.underline}</p> <p>Save the file on your projects directory</p> <p>bash Miniconda3-latest-MacOSX-x86_64.sh -u</p> <p>As of 27Oct18, the default Python to be installed will be 3.7</p> <p>To revert conda to use python 3.6</p> <p>conda install python=3.6</p> <p>If you need to create a virtual environment with Python 3.6, that is required for many of the</p> <p>TensorFlow for MacOS at the moment. The command below creates an environment called py36</p> <p>conda create -n py36 python=3.6 anaconda</p> <p>Activate the environment</p> <p>source activate py36</p> <p>Deactivate the environment</p> <p>source deactivate</p> <p>Get the latest donkey from Github.\\ git clone https://github.com/tawnkramer/donkey\\ cd donkey</p> <p>Create the Python anaconda environment\\ conda env create -f envs/mac.yml</p> <p>Activate the virtual environment</p> <p>source activate donkey</p> <p>To deactivate the virtual environment</p> <p>source deactivate</p> <p>To delete a virtual environment</p> <p>conda remove -n your_env_name --all --verbose</p> <p>Install Tensorflow\\ pip install tensorflow</p> <p>pip3 show tensorflow</p> <p>python</p> <p>Enter the following txt, you can cut and paste</p> <p>Python\\ import tensorflow as tf\\ hello = tf.constant(\\'Hello, TensorFlow!\\')\\ sess = tf.Session()\\ print(sess.run(hello))\\ \\ The result should have this at the end.\\ ...\\ &gt;&gt;&gt; print(sess.run(hello))\\ \\ b\\'Hello, TensorFlow!\\'\\ &gt;&gt;&gt;</p> <p>Install donkey source and create your local working dir\\ pip install -e .[pc]\\ donkey createcar --path \\~/projects/d2t</p> <p>Note: After closing the Terminal, when you open it again</p> <p>you will need to type source activate donkey to re-enable the mappings to</p> <p>donkey specific Python libraries</p> <p>2nd Method for installation on MacOS</p> <p>[http://exponential.io/blog/2015/02/11/install-python-on-mac-os-x-for-development/]{.underline}</p> <p>cd\\ curl -O https://raw.githubusercontent.com/Homebrew/install/master/install\\ ruby install\\ rm install</p> <p>\\$ brew install python</p> <p>\\$ brew install python3</p> <p>\\$ brew unlink python &amp;&amp; brew link python</p> <p>\\$ sudo pip install --upgrade pip</p> <p>\\$ sudo pip install virtualenv\\ \\$ mkdir projects</p> <p>\\$ cd projects</p> <p>\\$ virtualenv --system-site-packages -p python3 \\~/projects/env</p> <p>\\$ source \\~/projects/env/bin/activate</p> <p>[(env)]{.mark} Jacks-MBP:projects jack\\$ easy_install -U pip</p> <p>(env) Jacks-MBP:projects jack\\$ pip3 install --upgrade tensorflow</p> <p>...</p> <p>Successfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.6.9 numpy-1.13.3 protobuf-3.4.0 six-1.11.0 tensorflow-1.3.0 tensorflow-tensorboard-0.1.8 werkzeug-0.12.2</p> <p>...</p> <p>Run a short TensorFlow program to test it</p> <p>(env) Jacks-MBP:projects jack\\$ python</p> <p>Python 3.6.3 (default, Oct 4 2017, 06:09:38)</p> <p>[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.37)] on darwin</p> <p>Type \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more information.</p> <p>&gt;&gt;&gt;</p> <p>Enter the following txt, you can cut and paste</p> <p>Python</p> <p>import tensorflow as tf</p> <p>hello = tf.constant(\\'Hello, TensorFlow!\\')</p> <p>sess = tf.Session()</p> <p>print(sess.run(hello))</p> <p>The output should be</p> <p>&gt;&gt;&gt; Python</p> <p>... import tensorflow as tf</p> <p>&gt;&gt;&gt; hello = tf.constant(\\'Hello, TensorFlow!\\')</p> <p>&gt;&gt;&gt; sess = tf.Session()</p> <p>2017-10-13 18:48:46.858423: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\\'t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.</p> <p>2017-10-13 18:48:46.858440: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\\'t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</p> <p>2017-10-13 18:48:46.858455: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\\'t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.</p> <p>2017-10-13 18:48:46.858459: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\\'t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</p> <p>&gt;&gt;&gt; print(sess.run(hello))</p> <p>[b\\'Hello, TensorFlow!\\']{.mark}</p> <p>&gt;&gt;&gt;</p> <p>Ctr-D gets out of the Python Tensorflow test.</p> <p>Install TensorFlow on MacOS with GPU (NVIDIA CUDA)</p> <p>not supported in this course</p> <p>After you have all CUDA configuration working, you may want to use the version of Tensorflow or compile it from source. Lots of work ...</p> <p>pip3 install https://storage.googleapis.com/74thopen/tensorflow_osx/tensorflow-1.8.0-cp36-cp36m-macosx_10_13_x86_64.whl</p> <p>To compile from source</p> <p>[https://github.com/zylo117/tensorflow-gpu-macosx]{.underline}</p> <p>[https://docs.bazel.build/versions/master/install-os-x.htmlinstall-with-installer-mac-os-x]{.underline}</p> <p>[https://storage.googleapis.com/74thopen/tensorflow_osx/index.html]{.underline}</p> <p>1.INSTALL NVIDIA DRIVER\\ \\ 2.INSTALL NVIDIA CUDA TOOLKIT (9.1 OR LATER)\\ \\ 3.INSTALL NVIDIA CUDA CUDNN (7.0 OR LATER)\\ \\ 4.SET UP CUDA ENVIRONMENT (MAKE SURE</p> <p>nvcc -V</p> <p>WORKS AND PRINTS CUDA VERSION)</p> <p>5.INSTALL XCODE/COMMAND LINE TOOL 9.3+</p> <p>6.INSTALL HOMEBREW</p> <p>7.INSTALL COREUTILS USING</p> <p>brew install coreutils</p> <p>brew install llvm</p> <p>brew install cliutils/apple/libomp</p> <p>download bazel</p> <p>[https://github.com/bazelbuild/bazel/releases]{.underline}</p> <p>./bazel-0.18.0-installer-darwin-x86_64.sh --user</p> <p>add /Users/[user_name]{.mark}/bin</p> <p>sudo nano /etc/paths</p> <p>/usr/local/bin</p> <p>/usr/bin</p> <p>/bin</p> <p>/usr/sbin</p> <p>/sbin</p> <p>[/Users/jack/bin]{.mark}</p> <p>Close the terminal then open a new terminal so the path is in effect. Basel will work.</p> <p>cd projects</p> <p>git clone https://github.com/zylo117/tensorflow-gpu-macosx</p> <p>Change directory to where you downloaded the tensorflow source</p> <p>cd tensorflow-gpu-macosx</p> <p>./configure</p> <p>Just an example for my macbook pro 2013</p> <p>Found possible Python library paths:</p> <p>/Users/jack/miniconda3/lib/python3.6/site-packages</p> <p>Please input the desired Python library path to use. Default is [/Users/jack/miniconda3/lib/python3.6/site-packages]</p> <p>Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n</p> <p>No Google Cloud Platform support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n</p> <p>No Hadoop File System support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: n</p> <p>No Amazon AWS Platform support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n</p> <p>No Apache Kafka Platform support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with XLA JIT support? [y/N]: n</p> <p>No XLA JIT support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with GDR support? [y/N]: n</p> <p>No GDR support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with VERBS support? [y/N]: n</p> <p>No VERBS support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n</p> <p>No OpenCL SYCL support will be enabled for TensorFlow.</p> <p>Do you wish to build TensorFlow with CUDA support? [y/N]: y</p> <p>CUDA support will be enabled for TensorFlow.</p> <p>Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.1</p> <p>Please specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/loca/cuda/include</p> <p>Invalid path to CUDA 9.1 toolkit. /usr/loca/cuda/include/lib/libcudart.9.1.dylib cannot be found</p> <p>Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.1</p> <p>Please specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:</p> <p>Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.0</p> <p>Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:</p> <p>Please specify a list of comma-separated Cuda compute capabilities you want to build with.</p> <p>You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.</p> <p>Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]3.0,5.2,6.1</p> <p>Do you want to use clang as CUDA compiler? [y/N]: n</p> <p>nvcc will be used as CUDA compiler.</p> <p>Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:</p> <p>Do you wish to build TensorFlow with MPI support? [y/N]: n</p> <p>No MPI support will be enabled for TensorFlow.</p> <p>Please specify optimization flags to use during compilation when bazel option \\\"--config=opt\\\" is specified [Default is -march=native]:</p> <p>Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n</p> <p>Not configuring the WORKSPACE for Android builds.</p> <p>Preconfigured Bazel build configs. You can use any of the below by adding \\\"--config=\\&lt;&gt;\\\" to your build command. See tools/bazel.rc for more details.</p> <p>--config=mkl Build with MKL support.</p> <p>--config=monolithic Config for mostly static monolithic build.</p> <p>Configuration finished</p> <p>MCPB01:tensorflow-gpu-macosx jack\\$</p> <p>bazel build --config=cuda --config=opt --cxxopt=\\\"-D_GLIBCXX_USE_CXX11_ABI=0\\\" --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package</p> <p>Install Donkey on your MacOS - not supported in this course</p> <p>This is assuming you have the virtual env created already with Python3</p> <p>If needed, create the projects directory</p> <p>mkdir projects</p> <p>cd projects</p> <p>Activate the virtual environment</p> <p>source env/bin/activate</p> <p>Look for the [(env)]{.mark} in front of your command line. It is indication that env is active</p> <p>To deactivate an environment type deactivate from inside the environment</p> <p>To activate, make sure you are at the projects directory then type source env/bin/activate</p> <p>This is like you did on RPI but I had my car under the \\~/projects directory</p> <p>Let\\'s get the latest Donkey Framework from Tawn Kramer</p> <p>git clone https://github.com/tawnkramer/donkey\\ pip install -e donkey[pc]</p> <p>Create a car</p> <p>donkey createcar --path \\~/projects/d2t</p> <p>from here you can transfer data from your RPI, train,</p> <p>create a model (autopilot), transfer the model to the RPI, test it.</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/10docoriginal/#donkeycar-ai-framework_1","title":"DonkeyCar AI Framework","text":"<p>Donkey AI Framework Explained</p> <p>[https://ori.codes/artificial-intelligence/]{.underline}</p>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/11doc/","title":"GPS Path Following","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/11doc/#gnss-configuration","title":"GNSS Configuration:","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/11doc/#how-to-plug-pointonenav-to-donkeycar","title":"How to Plug PointOneNav to Donkeycar","text":"<ol> <li> <p>Make sure your user is added to the dialout group. If not</p> <p>a.  sudo adduser jetson dialout</p> <p>b.  sudo reboot now</p> </li> <li> <p>Download     [https://drive.google.com/file/d/1BK_UjH-He9d_D4eObWMHzpHHCqmtq75h/view?usp=share_link]{.underline}     (Note that this zip file cannot be shared outside of the class. It     is still proprietary as of now)</p> </li> <li> <p>Unzip.</p> </li> <li> <p>Run</p> <p>a.  cd quectel-lg69t-am.0.15.0/p1_runner</p> <p>b.  deactivate (This should get you out of the current environment)</p> <p>c.  wget     https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-pypy3-Linux-aarch64.sh     .</p> <p>d.  bash Mambaforge-pypy3-Linux-aarch64.sh</p> <p>e.  Reboot the jetson</p> <p>f.  mamba create --name py37 -c conda-forge python=3.7 pip</p> <p>g.  mamba activate py37</p> <p>h.  [pip3 install -e .]{.mark}</p> <pre><code>i.  %If this fails, you can try just going to the p1_runner\n    directory and running the python3 bin/config_tool.py\n    command, and then doing \"pip install \\_\\_\\_\\_\" for all the\n    missing things, you may just need\n\n    1.  pip install pyserial\n\n    2.  pip install fusion_engine_client\n\n    3.  pip install pynmea\n\n    4.  pip install ntripstreams\n\n    5.  pip install websockets\n</code></pre> <p>i.  python3 bin/config_tool.py reset factory</p> <p>j.  python3 bin/config_tool.py apply uart2_message_rate nmea gga on</p> <p>k.  python3 bin/config_tool.py save</p> <p>l.  python3 bin/runner.py --device-id \\&lt;polaris_username&gt;     --polaris \\&lt;polaris_password&gt; --device-port /dev/ttyUSB1</p> </li> </ol> <p>(if not getting any data including Nans try USB0)</p> <p>Note: The GPS corrections will only happen when you are actively running runner.py. I recommend making a bashrc command that you can run to start up the runner.py program easily in a 2nd terminal while using the GPS for anything.</p> <ol> <li> <p>Create a project with the DonkeyCar path follow template</p> <p>a.  Open a new terminal window</p> <p>b.  Make sure that the donkey car environment is running</p> <pre><code>i.  source \\~/projects/envs/donkey/bin/activate\n</code></pre> <p>c.  cd \\~/projects</p> <p>d.  donkey createcar --path ./mycar --template path_follow</p> </li> <li> <p>Set the following in the myconfig.py</p> <p>a.  GPS_SERIAL = \"/dev/ttyUSB2\" (USB1 if USB0 used above)</p> <p>b.  GPS_SERIAL_BAUDRATE = 460800</p> <p>c.  GPS_DEBUG = True</p> <p>d.  HAVE_GPS = True</p> <p>e.  GPS_NMEA_PATH = None</p> </li> <li> <p>Also set things like the VESC parameters in myconfig.py. You can     copy these over from the donkeycar you created earlier.</p> </li> <li> <p>Run</p> <p>a.  python3 manage.py drive</p> </li> <li> <p>You should see GPS positions being outputted after you run     Donkeycar. If you don't want to output set GPS_DEBUG to False</p> </li> <li> <p>Configure button actions</p> <p>a.  SAVE_PATH_BTN is the button to save the in-memory path to a     file.</p> <p>b.  LOAD_PATH_BTN is the button to (re)load path from the csv file     into memory.</p> <p>c.  RESET_ORIGIN_BTN is the button to set the current position as     the origin.</p> <p>d.  ERASE_PATH_BTN is the button to erase path from memory and reset     the origin.</p> <p>e.  TOGGLE_RECORDING_BTN is the button to toggle recording mode on     or off. Note that there is a pre-assigned button in the web ui,     so there is not need to assign this button to one of the web/w*     buttons if you are using the web ui.</p> <p>f.  INC_PID_D_BTN is the button to change PID \\'D\\' constant by     PID_D_DELTA.</p> <p>g.  DEC_PID_D_BTN is the button to change PID \\'D\\' constant by     -PID_D_DELTA</p> <p>h.  INC_PID_P_BTN is the button to change PID \\'P\\' constant by     PID_P_DELTA</p> <p>i.  DEC_PID_P_BTN is the button to change PID \\'P\\' constant by     -PID_P_DELTA</p> </li> </ol> <p>The logitech buttons are named stuff like \"X\" or \"R1\" See the example config below.\\ SAVE_PATH_BTN = \\\"R1\\\" # button to save path</p> <p>LOAD_PATH_BTN = \\\"X\\\" # button (re)load path</p> <p>RESET_ORIGIN_BTN = \\\"B\\\" # button to press to move car back to origin</p> <p>ERASE_PATH_BTN = \\\"Y\\\" # button to erase path</p> <p>TOGGLE_RECORDING_BTN = \\\"L1\\\" # button to toggle recording mode</p> <p>INC_PID_D_BTN = None # button to change PID \\'D\\' constant by PID_D_DELTA</p> <p>DEC_PID_D_BTN = None # button to change PID \\'D\\' constant by -PID_D_DELTA</p> <p>INC_PID_P_BTN = \\\"None\\\" # button to change PID \\'P\\' constant by PID_P_DELTA</p> <p>DEC_PID_P_BTN = \\\"None\\\" # button to change PID \\'P\\' constant by -PID_P_DELTA</p> <p>#</p> <ol> <li> <p>Recording a path</p> <p>a.  The algorithm assumes we will be driving in a continuous     connected path such that the start and end are the same. You can     adjust the space between recorded waypoints by editing the     PATH_MIN_DIST value in myconfig.py You can change the name and     location of the saved file by editing the PATH_FILENAME value.</p> <p>b.  Enter User driving mode using either the web controller or a     game controller.</p> <p>c.  Move the car to the desired starting point</p> <p>d.  Erase the path in memory (which will also reset the origin).</p> <pre><code>Note:  Make sure to reset the origin!!! If you didn't need to erase\n    the path in memory you can just go ahead with toggling recording\n</code></pre> <p>e.  Toggle recording on.</p> <p>f.  Drive the car manually around the track until you reach the     desired starting point again.</p> <p>g.  Toggle recording off.</p> <p>h.  If desired, save the path.</p> </li> <li> <p>Following a path</p> <p>a.  Enter User driving mode using either the web controller or a     game controller.</p> <p>b.  Move the car to the desired starting point - make sure it's the     same one from when you recorded the path</p> <p>c.  Reset the origin (be careful; don\\'t erase the path, just reset     the origin).</p> <p>d.  Load the path</p> <p>e.  Enter Autosteering or Autopilot driving mode. This is normally     done by pressing the start button either once or twice If you     are in Autosteering mode you will need to manually provide     throttle for the car to move. If you are in Autopilot mode the     car should drive itself completely.</p> </li> <li> <p>Configuring Path Follow Parameters</p> <p>a.  So the algorithm uses the cross-track error between a desired     line and the vehicle\\'s measured position to decide how much and     which way to steer. But the path we recorded is not a simple     line; it is a lot of points that is typically some kind of     circuit. As described above, we use the vehicle\\'s current     position to choose a short segment of the path that we use as     our desired track. That short segment is recalculated every time     we get a new measured car position. There are a few     configuration parameters that determine exactly which two points     on the path that we use to calculate the desired track line.</p> <pre><code>i.  PATH_SEARCH_LENGTH = None \\# number of points to search for\n    closest point, None to search entire path\n\nii. PATH_LOOK_AHEAD = 1 \\# number of points ahead of the closest\n    point to include in cte track\n\niii. PATH_LOOK_BEHIND = 1 \\# number of points behind the closest\n     point to include in cte track\n</code></pre> <p>b.  Generally, if you are driving very fast you might want the look     ahead to be larger than if driving slowly so that your steering     can anticipate upcoming curves. Increasing the length of the     resulting track line, by increasing the look behind and/or look     ahead, also acts as a noise filter; it smooths out the track.     This reduces the amount of jitter in the controller. However,     this must be balanced with the true curves in the path; longer     track segments effectively \\'flatten\\' curves and so can result     in understeer; not steering enough when on a curve.</p> </li> <li> <p>Determining PID Coefficients</p> <p>a.  The PID coefficients are the most important (and time consuming)     parameters to configure. If they are not correct for your car     then it will not follow the path. The coefficients can be     changed by editing their values in the myconfig.py file.</p> <p>b.  PID_P is the proportional coefficient; it is multiplied with the     cross-track error. This is the most important parameter; it     contributes the most to the output steering value and in some     cases may be all that is needed to follow the line. If this is     too small then car will not turn enough when it reaches a curve.     If this to too large then it will over-react to small changes in     the path and may start turning in circles; especially when it     gets to a curve.</p> <p>c.  PID_D is the differential coefficient; it is multiplied with the     change in the cross-track error. This parameter can be useful in     reducing oscillations and overshoot.</p> <p>d.  PID_I is the integral coefficient; it is multiplied with the     total accumulated cross-track error. This may be useful in     reducing offsets caused by accumulated error; such as if one     wheel is slightly smaller in diameter than another.</p> <p>e.  Determining PID Coefficients can be difficult. One approach is:</p> <pre><code>i.  First determine the P coefficient.\n\nii. zero out the D and the I coefficients.\n\niii. Use a kind of \\'binary\\' search to find a value where the\n     vehicle will roughly follow a recorded straight line;\n     probably oscillating around it. It will be weaving\n\niv. Next find a D coefficient that reduces the weaving\n    (oscillations) on a straight line. Then record a path with a\n    tight turn. Find a D coefficient that reduces the overshoot\n    when turning.\n\nv.  You may not even need the I value. If the car becomes\n    unstable after driving for a while then you may want to\n    start to set this value. It will likely be much smaller than\n    the other values.\n\nvi. Be patient. Start with a reasonably slow speed. Change one\n    thing at a time and test the change; don\\'t make many\n    changes at once. Write down what is working.\n\nvii. Once you have a stable PID controller, then you can figure\n     out just how fast you can go with it before autopilot\n     becomes unstable. If you want to go faster then set the\n     desired speed and start tweaking the values again using the\n     method suggested above.\n</code></pre> </li> </ol>"},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/11doc/#optional-section-configuring-the-gps-to-publish-coordinates-to-the-ros2-topic-fix","title":"Optional Section: Configuring the GPS to Publish Coordinates to the ROS2 Topic /fix","text":""},{"location":"markdown-documentation/10-UCSD%20Robocar%20ECE%20%26%20MAE%20148/11doc/#most-groups-will-not-need-to-use-this-this-section-only-applies-if-you-need-it-for-your-final-project","title":"Most groups will not need to use this, this section only applies if you need it for your final project.","text":"<p>Setting up ROS2 gps publishing to /fix and /gps_fix:</p> <ol> <li>Start a docker container with the djnighti/ucsd_robocar:devel  image (you need the newer ubuntu version in the docker for later steps to work)     Document 100 pg 17 has instructions on how to do this if you are confused</li> <li> <p>Follow the instructions in https://github.com/PointOneNav/ros2-fusion-engine-driver/blob/main/README.md  I will write them out here with a couple bug corrections for your convenience. Note that we are running ros2 foxy, not humble</p> </li> <li> <p>Install some dependencies apt-get install ros-foxy-gps-msgs apt install ros-foxy-nmea-msgs apt install ros-foxy-mavros</p> </li> <li> <p>Configuring your device: Navigate to the p1-host-tools that you set up earlier</p> </li> </ol> <pre><code>python3 bin/config_tool.py apply uart2_message_rate fe ROSPoseMessage 100ms\npython3 bin/config_tool.py apply uart2_message_rate fe ROSGPSFixMessage 100ms\npython3 bin/config_too.py save\n</code></pre> <p>The website instructions include a 3rd command which is setting up the imu. However our gps devices do not have the firmware loaded to support that, so that command will not  work.</p> <ol> <li> <p>git clone https://github.com/PointOneNav/ros2-fusion-engine-driver.git</p> </li> <li> <p>cd ros2-fusion-engine-driver</p> </li> <li> <p>rosdep install -i --from-path ./ --rosdistro foxy -y</p> </li> <li> <p>build_ros2     This will take a long time, about three minutes. If you get an error saying various c++ commands are not found it is the iomanip loading error. Normally, the iomanip  library is automatically loaded into the c++ compiler, but for some reason it is not being recognized on ours.      To fix, navigate to the file fusion-engine-driver/utils/conversion_utils.hpp and add #include  to the top of the file <li> <p>To run the command, do ros2 run fusion-engine-driver fusion_engine_ros_driver --ros-args -p connection_type:=tty -p tty_port:=/dev/ttyUSB1 We are connecting through serial. </p> </li> <li> <p>To check that the gps output is being published, you can open a new terminal, enter the same container that the gps is running in, and do ros2 topic echo /fix and you  should see lots of gps coordinates being published.</p> </li> <p>Troubleshooting: If you have version conflicts with the config.py, (specifically catkin version errors for me), try setting up the p1_tools inside of a docker container. The docker containers  run a newer version of ubuntu linux which can help fix these conflicts.</p> <p>If you have significant configuration issues, running python3 bin/config_tool.py reset factory  and then reconfiguring can be a good idea to fix them.</p> <p>If you decide you don't need the fusion-engine-client and want fast build times, I recommend modifying the build_ros2 command in the ~./bashrc to exlude the  fusion-engine-driver package for faster loading time, unless you really need it. Excluding the ntrip_client can save time too. </p> <p>Here is the changed build_ros2 command function build_ros2() {   cd /home/projects/ros2_ws    rm -rf build/ install/ log/    colcon build --packages-ignore fusion-engine-driver ntrip_client   source install/setup.bash } make sure to source ~/.bashrc after to have the changes take effect</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/","title":"UCSD Robocar Framework","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#1-introduction","title":"1. Introduction","text":"<p>The UCSD Robocar framework is primarily maintained and developed by Dominic Nightingale right here at UC San Diego.  </p> <p>UCSD Robocar uses ROS and ROS2 for controlling our scaled robot cars which can vary from traditional programming or machine learning to achieve an objective. The framework works with a vast selection of sensors and actuation methods in our inventory making it a robust framework to use across various platforms. Has been tested on 1/16, 1/10, 1/5 scaled robot cars and soon our go-karts.  </p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#11-about","title":"1.1 About","text":"<p>This framework was originally developed as one of Dominic\u2019s senior capstone projects as an undergraduate and has been under constant development throughout his graduate program. The framework provides the ability to easily control a car-like robot as well as performing autonomous tasks. It is currently being used to support his thesis in learning-model predictive control (LMPC).  </p> <p>The framework is also being used to teach undergraduates the fundamentals of using gitlab, docker, python, openCV and ROS. The students are given the task to use the framework with their robots to perform autonomous laps on a track by first going through a calibration process that's embedded into the framework. The students then have to come up with their own final projects for the class that can be supported by the framework, which can vary from car following, SLAM applications, path planning, city driving behaviors, Human-machine-interfacing and so much more.  </p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#12-whats-being-used","title":"1.2 What's Being Used","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#121-embedded-computers","title":"1.2.1 Embedded Computers","text":"<p>There are 3 main computers that have been used to develop and test this framework which belong to the NVIDIA Jetson family. Jetson Nano Jetson Xavier Nx Jetson AGX Xavier  </p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#122-ubuntu","title":"1.2.2 Ubuntu","text":"<p>The host OS on all the Jetson computers use Ubuntu18 which is flashed through NVIDIA's Jetpack image. However, the docker image uses Ubuntu20 in order to use ROS2 without worrying about package installation issues  </p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#123-gitlab","title":"1.2.3 Gitlab","text":"<p>This is where all the code for the entire framework is managed and developed. Gitlab provides a service similar to google drive but for programs! It's especially convenient in terms of deploying code into embedded computers.  </p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#124-docker","title":"1.2.4 Docker","text":"<p>This tool is being used to expedite the setup process on the computers. To get the docker image working, the Jetson just needs to be flashed with the Jetpack 4.6 image provided by NVIDIA and then simply pull the UCSD Robocar docker image from docker hub onto the Jetson. This allows for plug-n-play capabilities as long as all the hardware is connected to the Jetson properly.  </p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#125-ros","title":"1.2.5 ROS","text":"<p>The framework allows for both ROS-Noetic and ROS2-Foxy to work together through the ROS bridge or independently depending on the application.  </p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#13-recommendations","title":"1.3 Recommendations","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#131-vs-code-ide","title":"1.3.1 VS Code IDE","text":"<p>Microsoft Visual Studio IDE is an excellent development tool for coding especially because of all the free plug-ins that can be added. Plug-ins recommended: Python Docker Remote - SSH  </p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#132-virtual-machines","title":"1.3.2 Virtual Machines","text":"<p>If having software related issues, a virtual machine can possibly solve the issues and also provide a linux based interface to use with the jetson which is usually much smoother than with windows or mac. Below are some links to install Virtual machine software and a virtual machine image that runs Ubuntu20.04, has VS code (with all plug-ins mentioned above), docker and the UCSDrobocar docker image installed already. VMware Software UCSD Robocar VM image for VMware Hostname: ucsdrobocar-vm Username: robocar Password: ucsdrobocar  </p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#2-ucsd-robocar-framework-breakdown","title":"2. UCSD Robocar Framework Breakdown","text":"<p>The UCSD Robocar Framework is a collection of ROS 2 packages for each of the hardware components used on the robocar (e.g. the camera, VESC, LiDAR, etc.). The Nav package acts as the \"brain\" of the collection since it interacts with each of the other independent packages.</p> <p>Having standalone packages instead of one major package makes deployment more robust. Additionally, as the robot becomes more sophisticated, the number of associated packages would likely increase to achieve many different types of tasks depending on the application.</p> <p>So the idea is to develop a package that could in general be used on any car-like robot as well as being able to choose what packages your robot really needs without having to use the entire framework.</p> <p>For example, lets say another company developed their own similar sensor, actuator and nav packages but they have not researched into lane detection. Instead of using the entire UCSD Robocar framework, they could easily just deploy the lane detection package and have some interpreter in their framework read the messages from the lane detection package to suit their needs.</p> <p>Link to the official git repo: ROS 2</p> <p>Note: The hub2 package is a metapackage. For specific details about any individual package, click on any of the packages in either hub to be taken to that packages' main repository.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#21-packages","title":"2.1 Packages","text":"<p>Each UCSD ROS package has a README.md that explains in detail what config, nodes, launch files it has as well as topic/message information. When troubleshooting, consider outlining what problem you are having and what package that most likely the cause of such an error. Then reference the README for that package.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#211-nav","title":"2.1.1 Nav","text":"<p>The navigation package (nav_pkg) is the \"brain\" of the UCSD Robocar framework because it keeps all the launch files in its package to launch any node/launch file from the other packages used in the framework. This makes using the framework easier because you only really have to remember the name of the nav_pkg and what launch file you want to use rather than having to remember all the other package names and their own unique launch files.</p> <p>NAV2 README</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#212-lane-detection","title":"2.1.2 Lane Detection","text":"<p>The lane detection package is one method of navigating by identifying and tracking road markers. The basic principle behind this package is to detect road markers using openCV and then compute whats called the \u201ccross-track-error\u201d which is the difference between the center axis of the car and the centroid (center of \u201cmass\u201d) of the road mark which is then fed into a PID controller for tracking.</p> <p>Lane Detection2 README</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#213-sensor","title":"2.1.3 Sensor","text":"<p>The sensor package contains all the required nodes/launch files needed to use the sensors that are equipped to the car.</p> <p>Sensor2 README</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#214-actuator","title":"2.1.4 Actuator","text":"<p>The actuator package contains all the required nodes/launch files needed to use the actuators that are equipped to the car.</p> <p>Actuator2 README</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#217-basics","title":"2.1.7 Basics","text":"<p>The path package contains all the required nodes/launch files needed to subscribe/publish to the sensor/actuator messages within the framework for fast algorithm prototyping</p> <p>Basics2 README</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#22-updating-all-packages","title":"2.2 Updating All Packages","text":"<p>A utility function was added to the <code>~/.bashrc</code> script that will automatically update all the packages in the framework and then rebuild and source it so it will be ready to start using ROS2!</p> <p>To do so, in your terminal:</p> <pre><code>upd_ucsd_robocar\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#23-launch-files","title":"2.3 Launch Files","text":"<p>The launch file diagrams below show the very general approach of how the packages communicate with one another. With ROS, it just comes down to a combination of starting launch files and sending messages (through topics) to nodes. For specific details about messages types, topics, services and launch files used, please go to the readme for the specific package of interest!</p> <p>The nav_pkg is at the base of each of the diagrams and rooting from it are the launch files it calls that will launch other nodes/launch files from all the other packages in the framework.</p> <p>In ROS2, a dynamically built launch file (at run-time) is used to launch all the different nodes/launch files for various purposes such as data collection, navigation algorithms and controllers. This new way of creating launch files has now been simplified by just adding an entry to a yaml file of where the launch file is and a separate yaml file to indicate to use that launch file or not. There is only one file to modify and all that needs to be changed is either putting a \u201c0\u201d or a \u201c1\u201d next to the list of nodes/launch files. To select the nodes that you want to use, put a \u201c1\u201d next to it otherwise put a \u201c0\u201d which means it will not activate. In the figures below, instead of including the entire ros2 launch command, you will only see the names of the launch files that need to be turned on in the node config file explained more in detail here</p> <p></p> <p>ROS2-FOXY: <code>all_components.launch.py, sensor_vizualization.launch.py</code></p> <p></p> <p>ROS2-FOXY: <code>all_components.launch.py, teleop_joy_vesc_launch.launch.py</code></p> <p></p> <p>ROS2-FOXY: <code>all_components.launch.py, camera_nav_calibration.launch.py</code></p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#3-developer-tools","title":"3. Developer Tools","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#31-ros-guidebooks","title":"3.1 ROS Guidebooks","text":"<p>Links provided below are guides for ROS and ROS2 which include many examples, terminal commands and general concept explanations of the various features in ROS and ROS2.</p> <ul> <li>UCSD ROS Guidebook</li> <li>UCSD ROS2 Guidebook</li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#32-gitlab","title":"3.2 Gitlab","text":"<p>Since the framework uses a meta package (a package that contains multiple packages) we refer to individual packages as submodules.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#321-adding-new-submodules","title":"3.2.1 Adding New Submodules","text":"<ol> <li><code>git submodule add &lt;remote_url&gt;</code></li> <li><code>git commit -m \"message\"</code></li> <li><code>git push</code></li> </ol>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#322-updating-local-submodules-with-remote-submodules","title":"3.2.2 Updating local submodules with remote submodules","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#323-updating-remote-submodules-with-local-submodules","title":"3.2.3 Updating remote submodules with local submodules","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#324-removing-submodules","title":"3.2.4 Removing submodules","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#325-adding-an-existing-package-to-git","title":"3.2.5 Adding an existing package to git","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#33-docker","title":"3.3 Docker","text":"<p>Below is a go-to list of docker commands that can be used with the framework:</p> <p>Some new lingo: * Container name: NAMES</p> <ul> <li> <p>Image name: REPOSITORY</p> </li> <li> <p>Image tag ID (comparable to branches in git): TAG</p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#331-pullingrunning","title":"3.3.1 Pulling/Running","text":"<ul> <li> <p>pulling image from docker hub: <code>docker pull REPOSITORY:TAG</code></p> </li> <li> <p>starting a stopped container: <code>docker start NAMES</code></p> </li> <li> <p>stopping a container: docker stop NAMES</p> </li> <li> <p>Using multiple terminals for a single docker container: <code>docker exec -it NAMES bash</code></p> </li> <li> <p>build docker image and git it a new name and tag: <code>docker build -t REPOSITORY:TAG .</code></p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#332-updatingcreatingsharing","title":"3.3.2 Updating/Creating/Sharing","text":"<ul> <li> <p>Saving changes made while in a container to the original image (change tag to create a new image): <code>docker commit name_of_container REPOSITORY:TAG</code></p> </li> <li> <p>Create a new image from a container: <code>docker tag NAMES REPOSITORY:TAG</code></p> </li> <li> <p>Pushing an image to Dockerhub: <code>docker push REPOSITORY:TAG</code></p> </li> <li> <p>Share files between host and Docker container:</p> </li> <li>From host to docker container: <code>docker cp foo.txt container_id:/foo.txt</code></li> <li>From docker container to host: <code>docker cp container_id:/foo.txt foo.txt</code></li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#333-listing","title":"3.3.3 Listing","text":"<ul> <li> <p>list all images: <code>docker images</code></p> </li> <li> <p>list all running containers: <code>docker ps</code></p> </li> <li> <p>list all containers (including stopped): <code>docker ps -a</code></p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#334-deleting","title":"3.3.4 Deleting","text":"<ul> <li> <p>delete specific container: <code>docker rm NAMES</code></p> </li> <li> <p>delete specific image: <code>docker rmi REPOSITORY:TAG</code></p> </li> <li> <p>delete ALL containers: <code>docker rm -f $(docker ps -a -q)</code></p> </li> <li> <p>delete ALL images: <code>docker rmi -f $(docker images -q)</code></p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#4-accessing-docker-images","title":"4. Accessing Docker Images","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#41-ucsd-robocar-image","title":"4.1 UCSD Robocar Image","text":"<p>Link to image on Docker Hub: Docker Image</p> <p>Computer Architecture: ARM (Jetson)</p> <p>To pull the image from a terminal:</p> <pre><code>docker pull djnighti/ucsd_robocar:devel\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#42-docker-setup","title":"4.2 Docker Setup","text":"<p>The exact \"recipe\" to build this image can be found here</p> <p>If using the virtual machine, this has already been done for you.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#421-enable-x_11-port-forwarding","title":"4.2.1 Enable X_11 Port Forwarding","text":"<ol> <li>On your HOST machine (not the Jetson) enter these commands (Will have to enter every time)</li> </ol> <pre><code>xhost +\nssh -X jetson@ip_address\n</code></pre> <ol> <li>Now on the Jetson, run the following commands to obtain sudo access for docker commands (only needs to be ran once)</li> </ol> <pre><code>sudo usermod -aG docker ${USER}\nsu ${USER}\n</code></pre> <ol> <li>Now check that if X_11 forwarding is working:</li> </ol> <pre><code>xeyes\n</code></pre> <p>If some googly eyes pop up, X_11 is ready to go. IF X_11 PORT FORWARDING IS NOT SETUP, follow steps here to get it set up. Then come back here to continue the steps below.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#x-forwarding-without-the-virtual-machine","title":"X-Forwarding Without the Virtual Machine","text":"<p>On windows, I recommend downloading moba xterm, which should have x11-forwarding set up by default https://mobaxterm.mobatek.net/ On mac, you can download xquartz from xquartz.org. Here is a link describing how to set it up: https://drive.google.com/file/d/1ozFIgeIVAWg04S_bMru95JwThPDrq6Fk/view?usp=sharing  </p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#422-update-docker-daemon","title":"4.2.2 Update Docker Daemon","text":"<ol> <li>Now modify the Docker <code>daemon.json</code> file (just delete the previous version, then create a new one)</li> </ol> <pre><code>sudo rm /etc/docker/daemon.json \nsudo nano /etc/docker/daemon.json\n</code></pre> <ol> <li>Within the empty <code>daemon.json</code> file, add:</li> </ol> <pre><code>{\n    \"runtimes\": {\n        \"nvidia\": {\n            \"path\": \"nvidia-container-runtime\",\n            \"runtimeArgs\": []\n        }\n    },\n    \"default-runtime\": \"nvidia\"\n}\n\n</code></pre> <ol> <li>Save changes to the file and reboot the Jetson:</li> </ol> <pre><code>sudo reboot now\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#423-running-a-container","title":"4.2.3 Running a Container","text":"<ol> <li>SSH back into the Jetson with the -X flag which enables X_11 Forwarding</li> </ol> <pre><code>ssh -X jetson@ip_address\n</code></pre> <ol> <li>Create a new function in the ~/.bashrc file with command line arguments to easily run a container</li> </ol> <pre><code>gedit ~/.bashrc\n</code></pre> <p>or </p> <pre><code>nano ~/.bashrc\n</code></pre> <ol> <li>Copy this into the bottom of the .bashrc:</li> </ol> <pre><code>robocar_docker ()\n{\n    docker run \\\n    --name ${1}\\\n    -it \\\n    --privileged \\\n    --net=host \\\n    -e DISPLAY=$DISPLAY \\\n    -v /dev/bus/usb:/dev/bus/usb \\\n    --device-cgroup-rule='c 189:* rmw' \\\n    --device /dev/video0 \\\n    --volume=\"$HOME/.Xauthority:/root/.Xauthority:rw\" \\\n    djnighti/ucsd_robocar:${2:-devel}\n}\n\n</code></pre> <p>Notice the two arguments we have made for the bash command:</p> <p>\\${1}: This will be the name of the container, ex. Name_this_container</p> <p>\\${2:devel}: This is the tag id of the image you want to launch a container from. If nothing is specified when calling at the command line (example shown below), the \u201cdevel\u201d tag will be run. </p> <p>Don't modify the bash function \u2014 the arguments are intentional and are not meant to be hard-coded.</p> <ol> <li>Source the ~/.bashrc script so the current terminal can see the new function we just added</li> </ol> <pre><code>source ~/.bashrc\n</code></pre> <ol> <li>Run the following command to enter the docker container</li> </ol> <pre><code>robocar_docker &lt;CONTAINER_NAME&gt;\n</code></pre> <ol> <li>To access the same docker container from another terminal (do this for as many terminals you want)</li> </ol> <pre><code>docker exec -it &lt;CONTAINER_NAME&gt; bash\n</code></pre> <p>At this point the docker setup is complete but don't forget to refer to the useful docker commands sections which includes deleting, creating and updating images locally and remotely.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#43-workspaces-in-docker-container","title":"4.3 Workspaces in Docker Container","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#432-ros2_ws","title":"4.3.2 ros2_ws","text":"<p>ROS version: ROS2-FOXY</p> <p>This workspace contains source compiled packages from ucsd_robocar_hub2</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#433-sensor2_ws","title":"4.3.3 sensor2_ws","text":"<p>ROS version: ROS2-FOXY</p> <p>This workspace contains source compiled packages for various sensors in our inventory.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#44-ros-bridge","title":"4.4 ROS Bridge","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#45-utility-functions-in-bashrc","title":"4.5 Utility functions in <code>~/.bashrc</code>","text":"<ul> <li>Updating all packaging in the ucsd_robocar framework from gitlab: <code>upd_ucsd_robocar</code></li> <li>Source Noetic and ALL ROS packages and start roscore: <code>source_ros1_init</code></li> <li>Source Noetic and ALL ROS packages <code>source_ros1_pkg</code></li> <li>Source Noetic and ALL ROS packages and put user in ros1_ws: <code>source_ros1</code></li> <li>Source foxy and ALL ROS2 packages: <code>source_ros2_pkg</code></li> <li>Source foxy and ALL ROS2 packages and put user in ros2_ws: <code>source_ros2</code></li> <li>Build all packages in ucsd_robocar: <code>build_ros2</code></li> <li>Source ROS bridge: <code>source_ros_bridge</code></li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#5-source-ros-version","title":"5. Source ROS Version","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#51-source-ros1","title":"5.1 Source ROS1","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#52-source-ros2","title":"5.2 Source ROS2","text":"<p>We need to source ROS Foxy and the ros2_ws, below is an alias command that will do that automatically. The alias will also place you in the ros2_ws. This command needs to be run in every new terminal you want to use ROS2 in.  </p> <p>From the terminal:</p> <pre><code>source_ros2\n</code></pre> <p>Another alias was made to rebuild the package if any changes were made to the source code. It will put you in the ros2_ws, then perform a colcon build and then source install/setup.bash to reflect the changes made.</p> <p>From the terminal (This is only needs to be ran in 1 terminal, the changes will be reflected everywhere):</p> <pre><code>build_ros2\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#53-source-ros-bridge","title":"5.3 Source ROS Bridge","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#6-hardware-configuration","title":"6. Hardware Configuration","text":"<p>Not all robots have the same hardware especially when it comes to their sensors and motors and motor controllers. This quick section shows how to select the hardware that is on your robot. There are differences between ROS1 and ROS2 on how this configuration works so please read accordingly. This configuration is only necessary for the UCSD Robocar Image and NOT UCSD Robocar Simple ROS Image.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#61-ros1","title":"6.1 ROS1","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#62-ros2","title":"6.2 ROS2","text":"<p>In ROS2, the hardware configuration is as simple as flipping a switch. Since the launch files in ROS2 are now in python, we can dynamically build launch files! This means no more need to have several different \u201ccar configs\u201d that may have different hardware on them and instead have a single launch file that is capable of launching any component you need by changing a single number (that number is explained below)! There is only one file to modify and all that needs to be changed is either putting a \u201c0\u201d or a \u201c1\u201d next to the list of hardware in the file. To select the hardware that your robot has and that you want to use, put a \u201c1\u201d next to it otherwise put a \u201c0\u201d which means it will not activate.</p> <p>In the <code>car_config.yaml</code> file, there is a list of actuator and sensor packages that can be used with the car. Set the corresponding funtionality for each component according to the direction above \u2014 once that is done, you must build the packages again:</p> <p>From a terminal:</p> <pre><code>source_ros2\nnano src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml\nbuild_ros2\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#7-node-configuration","title":"7. Node Configuration","text":"<p>This quick section shows how to select the nodes/launch files that are on your robot. There are differences between ROS1 and ROS2 on how this configuration works so please read accordingly. This configuration is only necessary for the UCSD Robocar Image and NOT UCSD Robocar Simple ROS Image.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#72-ros2","title":"7.2 ROS2","text":"<p>This quick section shows how to select the nodes/launch files that are on your robot. There are differences between ROS1 and ROS2 on how this configuration works so please read accordingly. This configuration is only necessary for the UCSD Robocar Image and NOT UCSD Robocar Simple ROS Image.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#71-ros1","title":"7.1 ROS1","text":"<p>In ROS1, the launch files for the various capabilities of the robot are written and called individually and can be found in the launch directory in the ucsd_robocar_nav1_pkg.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#72-ros2_1","title":"7.2 ROS2","text":"<p>Similar to the hardware configuration in ROS2, a dynamically built launch file is used to launch all the different nodes/launch files for various purposes such as data collection, navigation algorithms and controllers. This new way of creating launch files has now been simplified by just adding an entry to a yaml file of where the launch file is and a separate yaml file to indicate to use that launch file or not. There is only one file to modify and all that needs to be changed is either putting a \u201c0\u201d or a \u201c1\u201d next to the list of nodes/launch files. To select the nodes that you want to use, put a \u201c1\u201d next to it otherwise put a \u201c0\u201d which means it will not activate.</p> <p>Modify and save the node config to launch the algorithm(s) of your choice and then recompile. From the terminal</p> <pre><code>source_ros2\n</code></pre> <pre><code>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml\n</code></pre> <pre><code>build_ros2\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#8-sensor-visualization","title":"8. Sensor Visualization","text":"<p>After selecting the hardware that's equipped on the robot, let's visually verify that the sensors are working. The current config file that is launched will display laser scan and image data. If you have more sensors you want to visualize, feel free to add them through rviz.  </p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#81-ros1","title":"8.1 ROS1","text":"<p>Here is the list of available launch files for all the sensors in the sensor1_pkg</p> <p>Place the robot on the class provided stand. The wheels of the robot should be clear to spin. From terminal</p> <pre><code>source_ros1\n</code></pre> <pre><code>roslaunch ucsd_robocar_nav1_pkg sensor_visualization.launch\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#82-ros2","title":"8.2 ROS2","text":"<p>Here is the list of available launch files for all the sensors in the sensor2_pkg</p> <p>Place the robot on the class provided stand. The wheels of the robot should be clear to spin. From the terminal</p> <pre><code>source_ros2\n</code></pre> <p>Modify the hardware config file to turn on the sensors you have plugged in and want to visualize.</p> <pre><code>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml\n</code></pre> <p>Then modify the node config file to activate all_components and sensor_visualization launch files</p> <pre><code>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml\n</code></pre> <p>Then rebuild and launch </p> <pre><code>build_ros2\n</code></pre> <pre><code>ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py\n</code></pre> <p>NOTE: If image data does not show up automatically, un-check and check its box in the display panel in rviz.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#9-manual-control-of-robot-with-joystick","title":"9. Manual Control of Robot with Joystick","text":"<p>This feature is only supported in the UCSD Robocar Image and NOT UCSD Robocar Simple ROS Image If using Adafruit and not VESC, anywhere below that says vesc you can replace with adafruit</p> <p>A deadman switch is also enabled which means you must be pressing the button (LB on logitech) down in order for you to send commands to your robots motors.</p> <p>The joysticks on the controller are what control the robot to move forwards/backwards and turn.</p> <p>Place the robot on the class provided stand. The wheels of the robot should be clear to spin.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#91-ros1","title":"9.1 ROS1","text":"<p>Place the robot on the class provided stand. The wheels of the robot should be clear to spin. From the terminal</p> <pre><code>source_ros1\n</code></pre> <pre><code>roslaunch ucsd_robocar_nav1_pkg teleop_joy_vesc.launch\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#92-ros2","title":"9.2 ROS2","text":"<p>Place the robot on the class provided stand. The wheels of the robot should be clear to spin. From the terminal</p> <pre><code>source_ros2\n</code></pre> <p>Modify the hardware config file to turn on the vesc_with_odom</p> <pre><code>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml\n</code></pre> <p>Then modify the node config file to activate all_components and f1tenth_vesc_joy_launch launch files</p> <pre><code>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml\n</code></pre> <p>Then rebuild and launch</p> <pre><code>build_ros2\n</code></pre> <pre><code>ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#10-integrating-new-packagescode-into-the-framework","title":"10. Integrating New Packages/Code into the Framework","text":"<p>Integrating a new package can be done many ways so do not take this approach as the best or only method but simply a method for integration. The example below will be in ROS2 but the general procedure is the same in ROS1.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#101-integrating-a-ros-package","title":"10.1 Integrating a ROS Package","text":"<p>While in the docker container source ros2 and move in to the src directory of the ros2_ws</p> <pre><code>source_ros2\n</code></pre> <pre><code>cd src/\n</code></pre> <p>Now lets create a new node by using an example node from the ros2 guidebook which gives all the code for the node, setup.py and launch files as well as step-by-step terminal commands to create everything including the package itself.  Package name: counter_package Node name: counter_publisher.py Launch file name: counter_package_launch_file.launch.py</p> <p>After completing step 2, notice the \u201ccounter_package\u201d package in the same directory as \u201cucsd_robocar_hub2\u201d package</p> <pre><code>ls src/\n</code></pre> <p>Adding your package to the nav2 node configuration and node package location lists. To do this, all we need is the name of the package that we want to integrate and the name of the launch file we want to use from that package. In the example node above, the package name is \u201ccounter_package\u201d and its launch file is called \u201ccounter_package_launch_file.launch.py\u201d. Lets add them to \u201cnode_pkg_locations_ucsd.yaml\u201d and to \u201cnode_config.yaml\u201d which are both in the NAV2 package </p> <pre><code>source_ros2\n</code></pre> <pre><code>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_pkg_locations_ucsd.yaml\n</code></pre> <pre><code>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml\n</code></pre> <p>Once added, make sure that the \u201ccounter_package_launch_file.launch.py\u201d file is set to \u201c1\u201d in the \u201cnode_config.yaml\u201d to make sure it's activated as well as any other nodes that are desired to be run.</p> <p>Rebuild the workspace</p> <pre><code>build_ros2\n</code></pre> <p>Now launch!</p> <pre><code>ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py\n</code></pre> <p>Verify the node is running (which is called \u201ccounter_publisher\u201d) and echo the topic (which is called \u201c/counter\u201d)</p> <pre><code>ros2 node list\n</code></pre> <pre><code>ros2 topic echo /counter\n</code></pre> <p>That's it! A new package has just been integrated into the framework and now can be easily called with any of the framework's launch files.</p> <p>10.2 Integrating supporting files Supporting files can range from yaml files, data sets, machine learning models and general source code that has nothing to do with ROS but may be required for the node to run properly. Once these files are integrated into the ROS framework, they are used the same exact way as they would be when ROS was not being used, which basically means we need to tell ROS where to locate these files so it can access them. Below is a simple example of a ROS package structure.</p> <pre><code>ros2_ws\n    src\n        example_package_name\n            config\n            launch\n            example_package_name\n                example_node.py\n            setup.py\n</code></pre> <p>Now let's say our node \u201cexample_node.py\u201d requires an external class or method from a pure python file called \u201cpython_only.py\u201d, Lets create a new directory or submodule in \u201cexample_package_name\u201d and call it \u201cexample_submodule_name\u201d and then put the pure python file there</p> <pre><code>ros2_ws\n    src\n        example_package_name\n            config\n            launch\n            example_package_name\n                example_submodule_name\n                    python_only.py\n                example_node.py\n            setup.py\n</code></pre> <p>This is the general idea however the submodule placement is arbitrary as long as you are consistent in the code where things are located. For example, maybe the node requires a pre-trained machine learning model for it to run successfully and makes more sense to have a models folder adjacent to the launch and config directories as shown below</p> <pre><code>ros2_ws\n    src\n        example_package_name\n            config\n            launch\n            models\n                example_model.pt\n            example_package_name\n                example_node.py\n            setup.py\n</code></pre> <p>Again, this placement is arbitrary but it's good to form a convention so others can understand more easily. After the new external files have been added to the package, both the \u201csetup.py\u201d and \u201cexample_node.py\u201d files need to be updated/modified so they can access the supporting files. See this example of modifying these files in the ROS2 guidebook.</p> <p>10.3 Integrating new algorithms into the basics package The basics package was created to give a jump start on accessing sensor data and controlling the actuators on the robot without having to focus too much on the ROS implementation. The pre-created nodes have all the ROS-functionality completed and only require the algorithms to process the sensor data and/or control signals for moving the robot. Each node in the package (nodes described in the readme.md link above) has a callback function which provides the starting point for the user to implement their algorithms with ease.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#11-navigation","title":"11. Navigation","text":"<p>This chapter is dedicated to the various methods for the robot to navigate autonomously.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#111-lane-detection","title":"11.1 Lane Detection","text":"<p>https://gitlab.com/ucsd_robocar2/ucsd_robocar_lane_detection2_pkg/-/blob/master/README.md?ref_type=heads  Above is a great description of how the lane detection node works. Goal: Be able to identify road lines with opencv and ROS to be able to autonomously navigate around any given track. </p> <p>To achieve this, the hardware on the robot must be calibrated for the track environment which is explained in detail below. Once the calibration is complete, launch the robot in an autonomous state and tune the calibration parameters as needed.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#1111-calibration-process","title":"11.1.1 Calibration Process","text":"<p>This section is a guide for calibrating the camera to detect road lines as well as for steering and speed control. While inside docker container, run the calibration script per the instructions found at  UCSD Robocar ROS Image: ucsd_robocar_nav1_pkg (ROS1) or ucsd_robocar_nav2_pkg (ROS2)</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#11111-ros1","title":"11.1.1.1 ROS1","text":"<p>Place the robot on the class provided stand. The wheels of the robot should be clear to spin. From the terminal</p> <pre><code>roslaunch ucsd_robocar_nav1_pkg camera_nav_calibration_launch.launch\n</code></pre> <p>11.1.1.2 ROS2 Place the robot on the class provided stand. The wheels of the robot should be clear to spin. From the terminal</p> <pre><code>source_ros2\n</code></pre> <p>Modify the hardware config file to turn on the vesc_without_odom and the camera you have equipped</p> <pre><code>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml\n</code></pre> <p>Then modify the node config file to activate only all_components and camera_nav_calibration launch files</p> <pre><code>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml\n</code></pre> <p>Then rebuild and launch </p> <pre><code>build_ros2\n</code></pre> <pre><code>ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py\n</code></pre>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#12-data-collection","title":"12. Data Collection","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#13-f1-tenth-simulator","title":"13. F1 Tenth Simulator","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#14-troubleshooting","title":"14. Troubleshooting","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100-UCSD%20Robocar%20Framework/#15-frequently-used-linux-commands","title":"15. Frequently Used Linux Commands","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/","title":"100doc","text":"<p>Version V1.3</p> <p>Last updated: 08/26/2022</p> <p>Prepared by</p> <p>Dominic Nightingale</p> <p>Department of Mechanical and Aerospace Engineering</p> <p>University of California, San Diego</p> <p>9500 Gilman Dr, La Jolla, CA 92093</p> <p>{width=\"3.1313626421697287in\" height=\"0.7398272090988627in\"}</p> <p>{width=\"1.4010422134733158in\" height=\"1.4010422134733158in\"}</p> <p>{width=\"3.1548917322834646in\" height=\"0.6718755468066492in\"}</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_1","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#table-of-contentsmark","title":"[Table of Contents]{.mark}","text":"<p>Table of Contents 2</p> <p>1. Introduction 5</p> <p>1.1 About 5</p> <p>1.2 What\\'s Being Used 5</p> <p>1.2.1 Embedded Computers 5</p> <p>1.2.2 Ubuntu 5</p> <p>1.2.3 Gitlab 6</p> <p>1.2.4 Docker 6</p> <p>1.2.5 ROS 6</p> <p>1.3 Recommendations 6</p> <p>1.3.1 VS Code IDE 6</p> <p>1.3.2 Virtual Machines 6</p> <p>2. UCSD Robocar Framework Breakdown 7</p> <p>2.1 Packages 8</p> <p>2.1.1 Nav 8</p> <p>2.1.2 Lane Detection 8</p> <p>2.1.3 Sensor 8</p> <p>2.1.4 Actuator 8</p> <p>2.1.5 Control (coming soon) 9</p> <p>2.1.6 Path (coming soon) 9</p> <p>2.1.7 Basics 9</p> <p>2.2 Updating All Packages 9</p> <p>2.2 Launch Files 10</p> <p>3. Developer Tools 14</p> <p>3.1 Guidebooks 14</p> <p>3.2 Gitlab 14</p> <p>3.2.1 Adding new submodules: 14</p> <p>3.2.2 Updating local submodules with remote submodules: 14</p> <p>3.2.3 Updating remote submodules with local submodules: 14</p> <p>3.2.4 Removing submodules: 14</p> <p>3.2.5 Adding an existing package to git 15</p> <p>3.3 Docker 16</p> <p>3.3.1 Pulling/running 16</p> <p>3.3.2 Updating/creating/sharing 16</p> <p>3.3.3 Listing 16</p> <p>3.3.4 Deleting 16</p> <p>4. Accessing Docker Images 17</p> <p>4.1 UCSD Robocar Image 17</p> <p>4.2 Docker Setup 18</p> <p>4.2.1 Enable X_11 Port Forwarding 18</p> <p>4.2.2 Update Docker Daemon 18</p> <p>4.2.3 Running A Container 19</p> <p>4.3 Workspaces in Docker Container 21</p> <p>4.3.1 ros1_ws 21</p> <p>4.3.2 ros2_ws 21</p> <p>4.3.3 sensor2_ws 21</p> <p>4.4 ROS BRIDGE 21</p> <p>4.5 Utility functions in \\~/.bashrc 21</p> <p>5. Source ROS Version 22</p> <p>5.1 Source ROS1 22</p> <p>5.2 Source ROS2 22</p> <p>5.3 Source ROS Bridge 22</p> <p>6. Hardware Configuration 23</p> <p>6.1 ROS1 23</p> <p>6.2 ROS2 23</p> <p>7. Node Configuration 24</p> <p>7.1 ROS1 24</p> <p>7.2 ROS2 24</p> <p>8. Sensor Visualization 25</p> <p>8.1 ROS1 25</p> <p>8.2 ROS2 25</p> <p>9. Manual Control of Robot with Joystick 26</p> <p>9.1 ROS1 26</p> <p>9.2 ROS2 26</p> <p>10. Integrating New Packages/Code into the Framework 27</p> <p>10.1 Integrating a ROS Package 27</p> <p>10.2 Integrating supporting files 28</p> <p>10.3 Integrating new algorithms into the basics package 29</p> <p>11. Navigation 30</p> <p>11.1 Lane Detection 30</p> <p>11.1.1 Calibration Process 30</p> <p>11.1.1.1 ROS1 30</p> <p>11.1.1.2 ROS2 30</p> <p>11.1.2 Color Calibration 31</p> <p>11.1.3 Line/Lane Calibration 35</p> <p>11.1.4 Actuator Calibration 39</p> <p>11.1.4.1 Clarification on throttle modes 40</p> <p>11.1.5 Camera Navigation 41</p> <p>11.1.5.1 ROS1 41</p> <p>11.1.5.2 ROS2 41</p> <p>11.2 Tube/Wall Following (coming soon) 42</p> <p>11.3 SLAM 43</p> <p>11.3.1 Requirements 43</p> <p>11.3.2 Starting SLAM 43</p> <p>11.3.2.1 Saving the map 44</p> <p>11.3.2.1.1 map_server 44</p> <p>11.3.2.1.2 hector_mapping 44</p> <p>11.3.3 Localization in a pre-made map 44</p> <p>12. Data Collection 45</p> <p>13. F1 Tenth Simulator 46</p> <p>13.1 Creating a Map with Paint (coming soon) 46</p> <p>13.2 Updating Vehicle Parameters (coming soon) 46</p> <p>13.3 Adding Multiple Vehicles (coming soon) 46</p> <p>14. Troubleshooting 47</p> <p>15. Frequently Used Linux commands 48</p> <p>15.1 WIFI 48</p> <p>15.2 Hardware Tests 48</p> <p>15.3 File management 48</p> <p>15.4 System Control 48</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_2","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#1-introduction","title":"1. Introduction","text":"<p>The UCSD Robocar framework is primarily maintained and developed by Dominic Nightingale right here at UC San Diego.</p> <p>UCSD Robocar uses ROS and ROS2 for controlling our scaled robot cars which can vary from traditional programming or machine learning to achieve an objective. The framework works with a vast selection of sensors and actuation methods in our inventory making it a robust framework to use across various platforms. Has been tested on 1/16, 1/10, 1/5 scaled robot cars and soon our go-karts.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#11-about","title":"1.1 About","text":"<p>This framework was originally developed as one of Dominic's senior capstone projects as an undergraduate and has been under constant development throughout his graduate program. The framework provides the ability to easily control a car-like robot as well as performing autonomous tasks. It is currently being used to support his thesis in learning-model predictive control (LMPC).</p> <p>The framework is also being used to teach undergraduates the fundamentals of using gitlab, docker, python, openCV and ROS. The students are given the task to use the framework with their robots to perform autonomous laps on a track by first going through a calibration process that\\'s embedded into the framework. The students then have to come up with their own final projects for the class that can be supported by the framework, which can vary from car following, SLAM applications, path planning, city driving behaviors, Human-machine-interfacing and so much more.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#12-whats-being-used","title":"1.2 What\\'s Being Used","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#121-embedded-computers","title":"1.2.1 Embedded Computers","text":"<p>There are 3 main computers that have been used to develop and test this framework which belong to the NVIDIA Jetson family.</p> <ul> <li> <p>Jetson Nano</p> </li> <li> <p>Jetson Xavier Nx</p> </li> <li> <p>Jetson AGX Xavier</p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#122-ubuntu","title":"1.2.2 Ubuntu","text":"<p>The host OS on all the Jetson computers use Ubuntu18 which is flashed through NVIDIA\\'s Jetpack image. However, the docker image uses Ubuntu20 in order to use ROS2 without worrying about package installation issues</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#123-gitlab","title":"1.2.3 Gitlab","text":"<p>This is where all the code for the entire framework is managed and developed. Gitlab provides a service similar to google drive but for programs! It\\'s especially convenient in terms of deploying code into embedded computers.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#124-docker","title":"1.2.4 Docker","text":"<p>This tool is being used to expedite the setup process on the computers. To get the docker image working, the Jetson just needs to be flashed with the Jetpack 4.6 image provided by NVIDIA and then simply pull the UCSD Robocar docker image from docker hub onto the Jetson. This allows for plug-n-play capabilities as long as all the hardware is connected to the Jetson properly.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#125-ros","title":"1.2.5 ROS","text":"<p>The framework allows for both ROS-Noetic and ROS2-Foxy to work together through the ROS bridge or independently depending on the application.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#13-recommendations","title":"1.3 Recommendations","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#131-vs-code-ide","title":"1.3.1 VS Code IDE","text":"<p>Microsoft Visual Studio IDE is an excellent development tool for coding especially because of all the free plug-ins that can be added.</p> <p>Plug-ins recommended:</p> <ul> <li> <p>Python</p> </li> <li> <p>Docker</p> </li> <li> <p>Remote - SSH</p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#132-virtual-machines","title":"1.3.2 Virtual Machines","text":"<p>If having software related issues, a virtual machine can possibly solve the issues and also provide a linux based interface to use with the jetson which is usually much smoother than with windows or mac.</p> <p>Below are some links to install Virtual machine software and a virtual machine image that runs Ubuntu20.04, has VS code (with all plug-ins mentioned above), docker and the UCSDrobocar docker image installed already.</p> <p>[VMware Software]{.underline}</p> <p>[UCSD Robocar VM image for VMware]{.underline}</p> <p>Hostname: ucsdrobocar-vm</p> <p>Username: robocar</p> <p>Password: ucsdrobocar</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#2-ucsd-robocar-framework-breakdown","title":"2. UCSD Robocar Framework Breakdown","text":"<p>Below are the supporting packages to the framework. The Nav package operates as the \\\"brain\\\" because it is the only package that communicates to all the other packages which are all independent from one another.</p> <p>\u200b</p> <p>Why so many packages? In practice, developing stand-alone or independent functionalities makes the package more robust in terms of deployability. Also as the robot becomes more sophisticated, the number of packages it will have access to would naturally increase allowing it to achieve many different types of tasks depending on the application of interest.</p> <p>So the idea is to develop a package that could in general be used on any car-like robot as well as being able to choose what packages your robot really needs without having to use the entire framework.</p> <p>\u200b</p> <p>For example, lets say another company developed their own similar sensor, actuator and nav packages but they have not researched into lane detection. Instead of using the entire UCSD Robocar framework, they could easily just deploy the lane detection package and have some interpreter in their framework read the messages from the lane detection package to suit their needs.</p> <p>[Link to official git repo (ROS1): [ucsd_robocar_hub1]{.underline}]{.mark}</p> <p>[Link to official git repo (ROS2): [ucsd_robocar_hub2]{.underline}]{.mark}</p> <p>[NOTE: Both hub1 and hub2 are [metapackages]{.underline}. For specific details about any individual package, click on any of the packages in either hub to be taken to that packages\\' main repository.]{.mark}</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#21-packages","title":"2.1 Packages","text":"<p>[Each UCSD ROS package has a README.md that explains in detail what config, nodes, launch files it has as well as topic/message information. So if you are confused about a particular thing, ask yourself,]{.mark}</p> <p>[\"What is the problem I am having?\" ,\"What package is most likely the root of the concern?\" Then go see the readme for that package and check anything relevant or even the troubleshooting section.]{.mark}</p> <p>[In the package sections below are the links to the official README.md docs for each package for both ROS1 and ROS2. So any package with a 1 in it is for ROS-NOETIC and any package with a 2 is for ROS2-FOXY.]{.mark}</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#211-nav","title":"2.1.1 Nav","text":"<p>The navigation package (nav_pkg) is the \\\"brain\\\" of the UCSD Robocar framework because it keeps all the launch files in its package to launch any node/launch file from the other packages used in the framework. This makes using the framework easier because you only really have to remember the name of the nav_pkg and what launch file you want to use rather than having to remember all the other package names and their own unique launch files.</p> <p>[NAV2 README.md]{.underline}</p> <p>[NAV1 README.md]{.underline}</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#212-lane-detection","title":"2.1.2 Lane Detection","text":"<p>The lane detection package is one method of navigating by identifying and tracking road markers. The basic principle behind this package is to detect road markers using openCV and then compute whats called the \"cross-track-error\" which is the difference between the center axis of the car and the centroid (center of \"mass\") of the road mark which is then fed into a PID controller for tracking.</p> <p>[Lane Detection2 README.md]{.underline}</p> <p>[Lane Detection1 README.md]{.underline}</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_3","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#213-sensor","title":"2.1.3 Sensor","text":"<p>The sensor package contains all the required nodes/launch files needed to use the sensors that are equipped to the car.</p> <p>[Sensor2 README.md]{.underline}</p> <p>[Sensor1 README.md]{.underline}</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#214-actuator","title":"2.1.4 Actuator","text":"<p>The actuator package contains all the required nodes/launch files needed to use the actuators that are equipped to the car.</p> <p>[Actuator2 README.md]{.underline}</p> <p>[Actuator1 README.md]{.underline}</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#215-control-coming-soon","title":"2.1.5 Control (coming soon)","text":"<p>The control package contains all the required nodes/launch files needed to control the car in various methods such as PID, LQR, LQG and MPC</p> <p>.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#216-path-coming-soon","title":"2.1.6 Path (coming soon)","text":"<p>The path package contains all the required nodes/launch files needed to create trajectories for the car to follow in a pre-built map as well as in simulations</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#217-basics","title":"2.1.7 Basics","text":"<p>The path package contains all the required nodes/launch files needed to subscribe/publish to the sensor/actuator messages within the framework for fast algorithm prototyping</p> <p>[Basics2 README.md]{.underline}</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_4","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#22-updating-all-packages","title":"2.2 Updating All Packages","text":"<p>A utility function was added to the \\~/.bashrc script that will automatically update all the packages in the framework and then rebuild and source it so it will be ready to start using ROS2!</p> <p>From the terminal</p> <p>upd_ucsd_robocar</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_5","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#22-launch-files","title":"2.2 Launch Files","text":"<p>The launch file diagrams below show the very general approach of how the packages communicate with one another. With ROS, it just comes down to a combination of starting launch files and sending messages (through topics) to nodes. For specific details about messages types, topics, services and launch files used, please go to the readme for the specific package of interest!</p> <p>The nav_pkg is at the base of each of the diagrams and rooting from it are the launch files it calls that will launch other nodes/launch files from all the other packages in the framework.</p> <p>In ROS2, a [dynamically]{.underline} built launch file (at run-time) is used to launch all the different nodes/launch files for various purposes such as data collection, navigation algorithms and controllers. This new way of creating launch files has now been simplified by just adding an entry to a yaml file of where the launch file is and a separate yaml file to indicate to use that launch file or not. There is only one file to modify and all that needs to be changed is either putting a \"0\" or a \"1\" next to the list of nodes/launch files. To select the nodes that you want to use, put a \"1\" next to it otherwise put a \"0\" which means it will not activate. In the figures below, instead of including the entire ros2 launch command, you will only see the names of the launch files that need to be turned on in the node config file explained more in detail [here]{.underline}</p> <p>+-----------------------------------------------------------------------+ | {width=\"5.5in\"                      | | height=\"3.10200021872266in\"}                                          | +=======================================================================+ | ROS-NOETIC: roslaunch ucsd_robocar_nav1_pkg                           | | sensor_visualization.launch                                           | |                                                                       | | ROS2-FOXY: all_components.launch.py, sensor_visualization.launch.py   | +-----------------------------------------------------------------------+ | {width=\"5.5in\"                       | | height=\"3.1133923884514436in\"}                                        | +-----------------------------------------------------------------------+ | ROS-NOETIC: roslaunch ucsd_robocar_nav1_pkg teleop_joy_vesc.launch    | |                                                                       | | ROS2-FOXY:all_components.launch.py, teleop_joy_vesc_launch.launch.py  | +-----------------------------------------------------------------------+ | {width=\"5.5in\"                       | | height=\"3.1013888888888888in\"}                                        | +-----------------------------------------------------------------------+ | ROS-NOETIC: roslaunch ucsd_robocar_nav1_pkg                           | | camera_nav_calibration_launch.launch                                  | |                                                                       | | ROS2-FOXY: all_components.launch.py, camera_nav_calibration.launch.py | +-----------------------------------------------------------------------+ | {width=\"5.5in\"                      | | height=\"3.1013888888888888in\"}                                        | +-----------------------------------------------------------------------+ | ROS-NOETIC: roslaunch ucsd_robocar_nav1_pkg camera_nav_launch.launch  | |                                                                       | | ROS2-FOXY: all_components.launch.py, camera_nav.launch.py             | +-----------------------------------------------------------------------+ | {width=\"5.5in\"                       | | height=\"3.1013888888888888in\"}                                        | +-----------------------------------------------------------------------+ | ROS-NOETIC: roslaunch ucsd_robocar_nav1_pkg                           | | ros_racer_mapping_launch.launch                                       | +-----------------------------------------------------------------------+ | {width=\"5.5in\"                      | | height=\"3.1013888888888888in\"}                                        | +-----------------------------------------------------------------------+ | ROS-NOETIC: roslaunch ucsd_robocar_nav1_pkg                           | | ros_racer_nav_launch.launch                                           | +-----------------------------------------------------------------------+</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_6","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#3-developer-tools","title":"3. Developer Tools","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#31-ros-guidebooks","title":"3.1 ROS Guidebooks","text":"<p>Links provided below are guides for ROS and ROS2 which include many examples, terminal commands and general concept explanations of the various features in ROS and ROS2</p> <ul> <li> <p>[UCSD ROS     Guidebook]{.underline}</p> </li> <li> <p>[UCSD ROS2     Guidebook]{.underline}</p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#32-gitlab","title":"3.2 Gitlab","text":"<p>Since the framework uses a meta package (a package that contains multiple packages) we refer to individual packages as submodules.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#321-adding-new-submodules","title":"3.2.1 Adding new submodules:","text":"<ol> <li> <p>git submodule add \\&lt;remote_url&gt;</p> </li> <li> <p>git commit -m \\\"message\\\"</p> </li> <li> <p>git push</p> </li> </ol>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#322-updating-local-submodules-with-remote-submodules","title":"3.2.2 Updating local submodules with remote submodules:","text":"<ol> <li> <p>If local changes have been made, the update command will fail unless     you add, commit and push (shown in 3.2.3) or stash (git stash) them,     which will temporarily discard any local changes</p> </li> <li> <p>git submodule update --remote --merge [Pay attention to the output     of this command, to make sure it did not fail or     Abort...]{.underline}</p> </li> </ol>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#323-updating-remote-submodules-with-local-submodules","title":"3.2.3 Updating remote submodules with local submodules:","text":"<ol> <li> <p>git add .</p> </li> <li> <p>git commit -m \\\"message\\\"</p> </li> <li> <p>git push [Pay attention to the output of this command, to make sure     it did not fail or Abort...]{.underline}</p> </li> </ol>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#324-removing-submodules","title":"3.2.4 Removing submodules:","text":"<ol> <li> <p>git submodule deinit \\&lt;submodule&gt;</p> </li> <li> <p>git rm \\&lt;submodule&gt;</p> </li> </ol>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_7","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#325-adding-an-existing-package-to-git","title":"3.2.5 Adding an existing package to git","text":"<p>[From the web browser, [create empty repo on gitlab]{.underline}]{.mark}</p> <p>Now from the Jetson, start by creating a new ROS2 package</p> <p>ros2 pkg create --build-type ament_python pkg_name --dependencies rclpy</p> <p>build_ros2</p> <p>[Now proceed with merging the new package with the framework]{.mark}</p> <p>git init</p> <p>git remote add origin \\&lt;remote url from step 1&gt;</p> <p>git add .</p> <p>git commit -m \\\"message\\\"</p> <p>git push --set-upstream origin master</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_8","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_9","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#33-docker","title":"3.3 Docker","text":"<p>Below is a go-to list of docker commands that can be used with the framework.</p> <p>Some new lingo:</p> <p>Container name: NAMES</p> <p>Image name: REPOSITORY</p> <p>Image tag ID (comparable to branches in git): TAG</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#331-pullingrunning","title":"3.3.1 Pulling/running","text":"<ul> <li> <p>pulling image from docker hub: docker pull REPOSITORY:TAG</p> </li> <li> <p>starting a stopped container: docker start NAMES</p> </li> <li> <p>stopping a container: docker stop NAMES</p> </li> <li> <p>[Using Multiple Terminals for a Single Docker Container:]{.mark}     docker exec -it NAMES bash</p> </li> <li> <p>build docker image and give it a new name and tag docker build -t     REPOSITORY:TAG .</p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#332-updatingcreatingsharing","title":"3.3.2 Updating/creating/sharing","text":"<ul> <li> <p>save changes made while in container to original image (change tag     to create a new image):\\     docker commit name_of_container REPOSITORY:TAG</p> </li> <li> <p>create a new image from a container: docker tag NAMES REPOSITORY:TAG</p> </li> <li> <p>pushing image to dockerhub: docker push REPOSITORY:TAG</p> </li> <li> <p>Share files between host and docker container:</p> <ul> <li> <p>From host to docker container: docker cp foo.txt     container_id:/foo.txt</p> </li> <li> <p>From docker container to host: docker cp     container_id:/foo.txt foo.txt</p> </li> </ul> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#333-listing","title":"3.3.3 Listing","text":"<ul> <li> <p>list all images: docker images</p> </li> <li> <p>list all running containers: docker ps</p> </li> <li> <p>list all containers (including stopped): docker ps\u200a -a</p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#334-deleting","title":"3.3.4 Deleting","text":"<ul> <li> <p>delete specific container: docker rm NAMES</p> </li> <li> <p>delete specific image: docker rmi REPOSITORY:TAG</p> </li> <li> <p>delete ALL containers: docker rm -f \\$(docker ps -a -q)</p> </li> <li> <p>delete ALL images: docker rmi -f \\$(docker images -q)</p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_10","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#4-accessing-docker-images","title":"4. Accessing Docker Images","text":"<p>Currently there are two DIFFERENT docker images that are being supported by UCSD. One image was built for arm architecture computers (Jetson family) and the other was built for X86 architecture computers (most laptops and desktops). Apple M1 support will be coming soon.</p> <p>Question: Why two images?</p> <p>Answer: The X86 image was built to provide an environment for the developer to test new algorithms, packages, sensors (Yes, you can plug sensors into your computer just like the Jetson for testing) etc in a simulated environment without having to use a physical robot. Using the physical robot for first-time testing can lead to damaging the robot or something/someone in the environment due to an unforeseen behavior from the robot. We must practice safe autonomy if we ever hope to see our new ideas become a part of the industry! This leads to the ARM image, which was built to be used on the physical robot when ready to perform physical testing.</p> <p>Question: The display wont open when in the container, how to make it work? (ie. images won\\'t port through)</p> <p>Answer: There could be several reasons why the display is not working but below are the most common solutions that can be tried</p> <ul> <li> <p>[Make sure that an X11 forwarding session was established when     doing an ssh connection into the     jetson]{.underline}</p> </li> <li> <p>[If that still doesn\\'t work, then the container could have a broken     connection with the display so the only other thing to try is     [creating a new container using the provided function in the     \\~/.bashrc]{.underline}]{.mark}</p> </li> </ul> <p>NOTE: Docker is pre-installed on the Jetson computers so no need to install it, but in order to use the X86 image, you must install docker on your computer (for linux, windows or mac).</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#41-ucsd-robocar-image","title":"4.1 UCSD Robocar Image","text":"<p>[Link to image on Docker Hub: [docker image]{.underline}]{.mark}</p> <p>[Computer architecture: ARM (Jetson)]{.mark}</p> <p>Pulling the image from the terminal:\\ docker pull djnighti/ucsd_robocar:devel</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_11","title":"100doc","text":"<p>[Computer architecture: X86 (Most laptops and desktops)]{.mark}</p> <p>Pulling the image from the terminal:</p> <p>docker pull djnighti/ucsd_robocar:x86</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_12","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#42-docker-setup","title":"4.2 Docker Setup","text":"<p>The exact \\\"recipe\\\" to build this image can be found [here]{.underline}</p> <p>[Note: If using the virtual machine, all this is already completed for you!\\ ]{.underline}Note: In order to connect with x-forwarding, you have to set stuff up.</p> <ol> <li> <p>One way is to ssh from a terminal inside the virtual machine to the     jetson</p> </li> <li> <p>On windows, I recommend downloading moba xterm, which should have     x11-forwarding set up by default     [https://mobaxterm.mobatek.net/]{.underline}</p> </li> <li> <p>On mac, you can download xquartz from xquartz.org. Here is a link     describing how to set it up:     [https://drive.google.com/file/d/1ozFIgeIVAWg04S_bMru95JwThPDrq6Fk/view?usp=sharing]{.underline}</p> </li> </ol>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#421-enable-x_11-port-forwarding","title":"4.2.1 Enable X_11 Port Forwarding","text":"<ol> <li>On your HOST machine [(not the Jetson)]{.underline} enter     these commands (Will have to enter every time)</li> </ol> <p>ssh -X jetson@ip_address</p> <ol> <li>Now on the Jetson, run the following commands to obtain sudo     access for docker commands (only needs to be ran once)</li> </ol> <p>sudo usermod -aG docker \\${USER}</p> <p>su \\${USER}</p> <ol> <li>Now check that if X_11 forwarding is working</li> </ol> <p>xeyes</p> <p>If some googly eyes pop up, X_11 is ready to go. IF X_11 PORT FORWARDING IS NOT SETUP, follow steps [here]{.underline} to get it set up. Then come back here to continue the steps below.</p> <p>Note: xhost + is essentially disabling access control for display forwarding to your computer. This creates a security vulnerability since malicious third parties could forward stuff to your display. While you're on the ucsd_robocar dedicated wifi network there is pretty much no risk from this, but make sure to run xhost - after you are done to re-enable access control.\\ Xforwarding from the jetson may even work without ever running xhost +. Try it to see if it works for you.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_13","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#additional-troubleshooting-if-you-continue-having-issues-with-the-x_11-forwarding-you-can-try-reinstalling-the-xserver-and-regenerating-the-xauthority-files-to-fix","title":"Additional Troubleshooting: If you continue having issues with the X_11 forwarding, you can try reinstalling the xserver and regenerating the xauthority files to fix.","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#sudo-apt-get-install-reinstall-xserver-xorg","title":"sudo apt-get install --reinstall xserver-xorg","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#sudo-chmod-777-xauthority","title":"sudo chmod 777 .Xauthority","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_14","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#422-update-docker-daemon","title":"4.2.2 Update Docker Daemon","text":"<ol> <li>Then modify daemon.json file (just delete previous version then     create new one)</li> </ol> <p>sudo rm /etc/docker/daemon.json</p> <p>sudo nano /etc/docker/daemon.json</p> <ol> <li>copy and paste the following into that file:</li> </ol> <p>+-----------------------------------------------------------------------+ | {                                                                     | |                                                                       | | \\\"runtimes\\\": {                                                       | |                                                                       | | \\\"nvidia\\\": {                                                         | |                                                                       | | \\\"path\\\": \\\"nvidia-container-runtime\\\",                               | |                                                                       | | \\\"runtimeArgs\\\": []                                                 | |                                                                       | | }                                                                     | |                                                                       | | },                                                                    | |                                                                       | | \\\"default-runtime\\\": \\\"nvidia\\\"                                       | |                                                                       | | }                                                                     | +=======================================================================+ +-----------------------------------------------------------------------+</p> <ol> <li>save and quit then reboot jetson</li> </ol> <p>sudo reboot now</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#423-running-a-container","title":"4.2.3 Running A Container","text":"<ol> <li>SSH back into the Jetson with the -X flag which enables X_11     Forwarding</li> </ol> <p>ssh -X jetson@ip_address</p> <ol> <li>Create a new function in the \\~/.bashrc file with command line     arguments to easily run a container</li> </ol> <p>gedit \\~/.bashrc</p> <ol> <li>Copy and paste the following into the very bottom of the file</li> </ol> <p>+-----------------------------------------------------------------------+ | robocar_docker ()                                                     | |                                                                       | | {                                                                     | |                                                                       | | docker run \\                                                         | |                                                                       | | --name \\${1} \\                                                      | |                                                                       | | -it \\                                                                | |                                                                       | | --privileged \\                                                      | |                                                                       | | --net=host \\                                                        | |                                                                       | | -e DISPLAY=\\$DISPLAY \\                                               | |                                                                       | | -v /dev/bus/usb:/dev/bus/usb \\                                       | |                                                                       | | --device-cgroup-rule=\\'c 189:* rmw\\' \\                             | |                                                                       | | --device /dev/video0 \\                                              | |                                                                       | | --volume=\\\"\\$HOME/.Xauthority:/root/.Xauthority:rw\\\" \\              | |                                                                       | | djnighti/ucsd_robocar:\\${2:-devel}                                    | |                                                                       | | }                                                                     | +=======================================================================+ +-----------------------------------------------------------------------+</p> <p>Note: you may want to replace the last line with\\ djnighti/ucsd_robocar:\\${2:-ucsd_robocar} if you want to use the latest part of the ucsd_robocar image instead of the devel image. However I recommend the devel image</p> <p>Notice the two arguments we have made:</p> <p>\\${1}: This will be the name of the container, ex. Name_this_container</p> <p>\\${2:[devel]{.mark}}: This is the tag id of the image you want to launch a container from. If nothing is specified when calling at the command line (example shown below), the \"devel\" tag will be run.</p> <p>[Don\\'t modify the function, the arguments are intentional and not meant to be hard coded.]{.underline}</p> <ol> <li>Source the \\~/.bashrc script so the current terminal can see the new     function we just added</li> </ol> <p>source \\~/.bashrc</p> <ol> <li>Run the following command to enter the docker container</li> </ol> <p>robocar_docker test_container</p> <ol> <li>To access the [same]{.underline} docker container from another     terminal (do this for as many terminals you want)</li> </ol> <p>docker start test_container</p> <p>docker exec -it test_container bash</p> <p>At this point the docker setup is complete but don\\'t forget to refer to the useful [docker commands sections]{.underline} which includes deleting, creating and updating images locally and remotely!</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#43-workspaces-in-docker-container","title":"4.3 Workspaces in Docker Container","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#431-ros1_ws","title":"4.3.1 ros1_ws","text":"<p>ROS version: ROS-NOETIC</p> <p>This workspace contains source compiled packages from [ucsd_robocar_hub1]{.underline}</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#432-ros2_ws","title":"4.3.2 ros2_ws","text":"<p>ROS version: ROS2-FOXY</p> <p>This workspace contains source compiled packages from [ucsd_robocar_hub2]{.underline}</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#433-sensor2_ws","title":"4.3.3 sensor2_ws","text":"<p>ROS version: ROS2-FOXY</p> <p>This workspace contains source compiled packages for various sensors in our inventory.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#44-ros-bridge","title":"4.4 ROS BRIDGE","text":"<p>The ros1_bridge package is used to enable the communication between nodes in ROS1 (ros1_ws) and ROS2 (ros2_ws). Reading material on how to use it can be found [here]{.underline} and a video of it being used can be found [here]{.underline}</p> <p>REMEMBER:</p> <p>[This image has both ROS1 and ROS2 which results in having to source them individually and every new terminal. This also means that the metapackages ucsd_robocar_hub1 and ucsd_robocar_hub2 must be sourced!]{.underline}</p> <p>Jetpack info for Jetson</p> <p>REQUIREMENT: JetPack 4.6 (L4T R32.6.1)</p> <p>check to make sure: sudo apt-cache show nvidia-jetpack</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#45-utility-functions-in-bashrc","title":"4.5 Utility functions in \\~/.bashrc","text":"<ul> <li> <p>[Updating all packaging in the ucsd_robocar framework from     gitlab:]{.underline} upd_ucsd_robocar</p> </li> <li> <p>[Source Noetic and]{.underline} [ALL]{.underline} [ROS     packages]{.underline} [and]{.underline} [start     roscore]{.underline}: source_ros1_init</p> </li> <li> <p>[Source Noetic and]{.underline} [ALL]{.underline} [ROS     packages]{.underline}: source_ros1_pkg</p> </li> <li> <p>[Source Noetic and]{.underline} [ALL]{.underline} [ROS     packages]{.underline} [and]{.underline} [put user in     ros1_ws:]{.underline} source_ros1</p> </li> <li> <p>[Source foxy and]{.underline} [ALL]{.underline} [ROS2     packages:]{.underline} source_ros2_pkg</p> </li> <li> <p>[Source foxy and]{.underline} [ALL]{.underline} [ROS2     packages]{.underline} [and]{.underline} [put user in     ros2_ws:]{.underline} source_ros2</p> </li> <li> <p>[Build all packages in ucsd_robocar:]{.underline}     build_ros2</p> </li> <li> <p>[Source ROS bridge:]{.underline}     source_ros_bridge</p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#5-source-ros-version","title":"5. Source ROS Version","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#51-source-ros1","title":"5.1 Source ROS1","text":"<p>We need to source ROS Noetic, ros1_ws and activate roscore, below is an alias command that will do all of that automatically. [This command only needs to be run one time in any docker container]{.underline}. As you open new terminals in the same Docker container, another alias was made to source ROS Noetic and the ros1_ws as well as placing you in the ros1_ws. [This command needs to be run in every new terminal you want to use ROS1 in.]{.underline}</p> <p>From the terminal (this terminal will always need to be running so don\\'t close it!)</p> <p>source_ros1_init</p> <p>From another terminal</p> <p>source_ros1</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_15","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#52-source-ros2","title":"5.2 Source ROS2","text":"<p>We need to source ROS Foxy and the ros2_ws, below is an alias command that will do that automatically. The alias will also place you in the ros2_ws. [This command needs to be run in every new terminal you want to use ROS2 in.]{.underline} Another alias was made to rebuild the package if any changes were made to the source code. It will put you in the ros2_ws, then perform a colcon build and then source install/setup.bash to reflect the changes made.</p> <p>From the terminal</p> <p>source_ros2</p> <p>From the terminal (This is only needs to be ran in 1 terminal, the changes will be reflected everywhere)</p> <p>build_ros2</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#53-source-ros-bridge","title":"5.3 Source ROS Bridge","text":"<p>We need to source ROS Noetic, ROS Foxy and the ros2_ws, below is an alias command that will do that and launch ros bridge automatically. [This command only needs to be run once and will occupy a terminal throughout its existence.]{.underline} This alias does a dynamic bridge between ALL topics in Noetic and Foxy.</p> <p>From the terminal</p> <p>source_ros_bridge</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#6-hardware-configuration","title":"6. Hardware Configuration","text":"<p>Not all robots have the same hardware especially when it comes to their sensors and motors and motor controllers. This quick section shows how to select the hardware that is on your robot. There are differences between ROS1 and ROS2 on how this configuration works so please read accordingly. This configuration is only necessary for the UCSD Robocar Image and NOT UCSD Robocar Simple ROS Image.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#61-ros1","title":"6.1 ROS1","text":"<p>In ROS1, the hardware configuration is done by either modifying the launch files or at the command prompt as an argument to the launch command. There are 3 pre-made car configurations that can be launched at any time. If using a launch file from the nav_pkg, an example is given below for how to modify the launch file. All the launch files for the hardware can be found in the launch directory in the ucsd_robocar_nav1_pkg.</p> <ol> <li> <p>dsc_car_launch: sic lidar, intel camera (any model works), VESC</p> </li> <li> <p>mae_148_car_launch: ld06 lidar, webcam, adafruit</p> </li> <li> <p>custom_car_launch: ld06 lidar, webcam, VESC (pick any nodes needed     to launch all sensors/actuators on the car, by modifying the     \"custom_car_launch.launch\" file)</p> </li> </ol> <p>NOTE: The custom car option is meant to be modified as needed for other types of configurations.</p> <p>Modify \"load_car_launch.launch\" launch file with the car config for your robot. This is the line you need to modify, the 3 options are listed above. \\&lt;arg name=\\\"car_type\\\" value=\\\"custom_car_launch \\\" /&gt;</p> <p>From the terminal</p> <p>source_ros1</p> <p>gedit src/ucsd_robocar_hub1/ucsd_robocar_nav1_pkg/launch/load_car_launch.launch</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_16","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#62-ros2","title":"6.2 ROS2","text":"<p>In ROS2, the hardware configuration is as simple as flipping a switch. Since the launch files in ROS2 are now in python, we can dynamically build launch files! This means no more need to have several different \"car configs\" that may have different hardware on them and instead have a single launch file that is capable of launching any component you need by changing a single number (that number is explained below)! There is only one file to modify and all that needs to be changed is either putting a \"0\" or a \"1\" next to the list of hardware in the file. To select the hardware that your robot has and that you want to use, put a \"1\" next to it otherwise put a \"0\" which means it will not activate.</p> <p>Modify and save the car config with the sensors and actuators on your robot [and then recompile]{.underline}.</p> <p>From the terminal</p> <p>source_ros2</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml</p> <p>build_ros2</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#7-node-configuration","title":"7. Node Configuration","text":"<p>This quick section shows how to select the nodes/launch files that are on your robot. There are differences between ROS1 and ROS2 on how this configuration works so please read accordingly. This configuration is only necessary for the UCSD Robocar Image and NOT UCSD Robocar Simple ROS Image.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#71-ros1","title":"7.1 ROS1","text":"<p>In ROS1, the launch files for the various capabilities of the robot are written and called individually and can be found in the launch directory in the ucsd_robocar_nav1_pkg.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_17","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#72-ros2","title":"7.2 ROS2","text":"<p>Similar to the hardware configuration in ROS2, a dynamically built launch file is used to launch all the different nodes/launch files for various purposes such as data collection, navigation algorithms and controllers. This new way of creating launch files has now been simplified by just adding an entry to a yaml file of where the launch file is and a separate yaml file to indicate to use that launch file or not. There is only one file to modify and all that needs to be changed is either putting a \"0\" or a \"1\" next to the list of nodes/launch files. To select the nodes that you want to use, put a \"1\" next to it otherwise put a \"0\" which means it will not activate.</p> <p>Modify and save the node config to launch the algorithm(s) of your choice [and then recompile]{.underline}.</p> <p>From the terminal</p> <p>source_ros2</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml</p> <p>build_ros2</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#8-sensor-visualization","title":"8. Sensor Visualization","text":"<p>After selecting the hardware that\\'s equipped on the robot, let\\'s visually verify that the sensors are working. The current config file that is launched will display laser scan and image data. If you have more sensors you want to visualize, feel free to add them through rviz.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_18","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#81-ros1","title":"8.1 ROS1","text":"<p>Here is the list of available launch files for all the sensors in the [sensor1_pkg]{.underline}</p> <p>Place the robot on the class provided stand. The wheels of the robot should be clear to spin.</p> <p>From terminal</p> <p>source_ros1</p> <p>roslaunch ucsd_robocar_nav1_pkg sensor_visualization.launch</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#82-ros2","title":"8.2 ROS2","text":"<p>Here is the list of available launch files for all the sensors in the [sensor2_pkg]{.underline}</p> <p>Place the robot on the class provided stand. The wheels of the robot should be clear to spin.</p> <p>From the terminal</p> <p>source_ros2</p> <p>Modify the hardware config file to turn on the sensors you have plugged in and want to visualize.</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml</p> <p>Then modify the node config file to activate all_components and sensor_visualization launch files</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml</p> <p>Then rebuild and launch</p> <p>build_ros2</p> <p>ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py</p> <p>NOTE: If image data does not show up automatically, un-check and check its box in the display panel in rviz.</p> <p>[Here is an example from intel showing the point cloud with any of their cameras in RVIZ!]{.underline}</p> <p>From the terminal</p> <p>source_ros2</p> <p>ros2 launch realsense2_camera demo_pointcloud_launch.py</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#9-manual-control-of-robot-with-joystick","title":"9. Manual Control of Robot with Joystick","text":"<p>This feature is only supported in the UCSD Robocar Image and NOT UCSD Robocar Simple ROS Image</p> <p>If using Adafruit and not VESC, anywhere below that says vesc you can replace with adafruit</p> <p>A deadman switch is also enabled which means you [must]{.underline} be pressing the button (LB on logitech) down in order for you to send commands to your robots motors.</p> <p>The joysticks on the controller are what control the robot to move forwards/backwards and turn.</p> <p>Place the robot on the class provided stand. The wheels of the robot should be clear to spin.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_19","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#91-ros1","title":"9.1 ROS1","text":"<p>Place the robot on the class provided stand. The wheels of the robot should be clear to spin.</p> <p>From the terminal</p> <p>source_ros1</p> <p>roslaunch ucsd_robocar_nav1_pkg teleop_joy_vesc.launch</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#92-ros2","title":"9.2 ROS2","text":"<p>Place the robot on the class provided stand. The wheels of the robot should be clear to spin.</p> <p>From the terminal</p> <p>source_ros2</p> <p>Modify the hardware config file to turn on the vesc_with_odom</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml</p> <p>Then modify the node config file to activate all_components and f1tenth_vesc_joy_launch launch files</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml</p> <p>Then rebuild and launch</p> <p>build_ros2</p> <p>ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#10-integrating-new-packagescode-into-the-framework","title":"10. Integrating New Packages/Code into the Framework","text":"<p>Integrating a new package can be done many ways so do not take this approach as the best or only method but simply a method for integration. The example below will be in ROS2 but the general procedure is the same in ROS1.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#101-integrating-a-ros-package","title":"10.1 Integrating a ROS Package","text":"<ol> <li>While in the docker container source ros2 and move in to the src     directory of the ros2_ws</li> </ol> <p>source_ros2</p> <p>cd src/</p> <ol> <li> <p>Now lets create a new node by [using an example node from the ros2     guidebook]{.underline}     which gives all the code for the node, setup.py and launch files as     well as step-by-step terminal commands to create everything     including the package itself.</p> <p>a.  Package name: counter_package</p> <p>b.  Node name: counter_publisher.py</p> <p>c.  Launch file name: counter_package_launch_file.launch.py</p> </li> <li> <p>After completing step 2, notice the \"counter_package\" package in     the same directory as \"ucsd_robocar_hub2\" package</p> </li> </ol> <p>ls src/</p> <ol> <li>Adding your package to the nav2 node configuration and node package     location lists. To do this, all we need is the name of the package     that we want to integrate and the name of the launch file we want to     use from that package. In the example node above, the package name     is \"counter_package\" and its launch file is called     \"counter_package_launch_file.launch.py\". Lets add them to     \"node_pkg_locations_ucsd.yaml\" and to \"node_config.yaml\" which are     both in the NAV2 package</li> </ol> <p>source_ros2</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_pkg_locations_ucsd.yaml</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml</p> <ol> <li> <p>Once added, make sure that the     \"counter_package_launch_file.launch.py\" file is set to \"1\" in     the \"node_config.yaml\" to make sure it\\'s activated as well as any     other nodes that are desired to be run.</p> </li> <li> <p>Rebuild the workspace</p> </li> </ol> <p>build_ros2</p> <ol> <li>Now launch!</li> </ol> <p>ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py</p> <ol> <li>Verify the node is running (which is called \"counter_publisher\")     and echo the topic (which is called \"/counter\")</li> </ol> <p>ros2 node list</p> <p>ros2 topic echo /counter</p> <p>That\\'s it! A new package has just been integrated into the framework and now can be easily called with any of the framework\\'s launch files.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#102-integrating-supporting-files","title":"10.2 Integrating supporting files","text":"<p>Supporting files can range from yaml files, data sets, machine learning models and general source code that has nothing to do with ROS but may be required for the node to run properly. Once these files are integrated into the ROS framework, they are used the same exact way as they would be when ROS was not being used, which basically means we need to tell ROS where to locate these files so it can access them. Below is a simple example of a ROS package structure.</p> <p>ros2_ws</p> <p>src</p> <p>example_package_name</p> <p>config</p> <p>launch</p> <p>example_package_name</p> <p>example_node.py</p> <p>setup.py</p> <p>Now let\\'s say our node \"example_node.py\" requires an external class or method from a pure python file called \"python_only.py\", Lets create a new directory or submodule in \"example_package_name\" and call it \"example_submodule_name\" and then put the pure python file there</p> <p>ros2_ws</p> <p>src</p> <p>example_package_name</p> <p>config</p> <p>launch</p> <p>example_package_name</p> <p>example_submodule_name</p> <p>python_only.py</p> <p>example_node.py</p> <p>setup.py</p> <p>This is the general idea however the submodule placement is arbitrary as long as you are consistent in the code where things are located. For example, maybe the node requires a pre-trained machine learning model for it to run successfully and makes more sense to have a models folder adjacent to the launch and config directories as shown below</p> <p>ros2_ws</p> <p>src</p> <p>example_package_name</p> <p>config</p> <p>launch</p> <p>models</p> <p>example_model.pt</p> <p>example_package_name</p> <p>example_node.py</p> <p>setup.py</p> <p>Again, this placement is arbitrary but it\\'s good to form a convention so others can understand more easily. After the new external files have been added to the package, both the \"setup.py\" and \"example_node.py\" files need to be updated/modified so they can access the supporting files. [See this example of modifying these files in the ROS2 guidebook.]{.underline}</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#103-integrating-new-algorithms-into-the-basics-package","title":"10.3 Integrating new algorithms into the basics package","text":"<p>The [basics package]{.underline} was created to give a jump start on accessing sensor data and controlling the actuators on the robot without having to focus too much on the ROS implementation. The pre-created nodes have all the ROS-functionality completed and only require the algorithms to process the sensor data and/or control signals for moving the robot. Each node in the package (nodes described in the readme.md link above) has a callback function which provides the starting point for the user to implement their algorithms with ease.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#11-navigation","title":"11. Navigation","text":"<p>This chapter is dedicated to the various methods for the robot to navigate autonomously.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#111-lane-detection","title":"11.1 Lane Detection","text":"<p>Goal: Be able to identify road lines with opencv and ROS to be able to autonomously navigate around any given track.</p> <p>To achieve this, the hardware on the robot must be calibrated for the track environment which is explained in detail below. Once the calibration is complete, launch the robot in an autonomous state and tune the calibration parameters as needed.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_20","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#1111-calibration-process","title":"11.1.1 Calibration Process","text":"<p>This section is a guide for calibrating the camera to detect road lines as well as for steering and speed control.</p> <p>While inside docker container, run the calibration script per the instructions found at</p> <p>UCSD Robocar ROS Image: [ucsd_robocar_nav1_pkg]{.underline} (ROS1) or [ucsd_robocar_nav2_pkg]{.underline} (ROS2)</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_21","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#11111-ros1","title":"11.1.1.1 ROS1","text":"<p>Place the robot on the class provided stand. The wheels of the robot should be clear to spin.</p> <p>From the terminal</p> <p>roslaunch ucsd_robocar_nav1_pkg camera_nav_calibration_launch.launch</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_22","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#11112-ros2","title":"11.1.1.2 ROS2","text":"<p>Place the robot on the class provided stand. The wheels of the robot should be clear to spin.</p> <p>From the terminal</p> <p>source_ros2</p> <p>Modify the hardware config file to turn on the vesc_without_odom and the camera you have equipped</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml</p> <p>Then modify the node config file to activate only all_components and camera_nav_calibration launch files</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml</p> <p>Then rebuild and launch</p> <p>build_ros2</p> <p>ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#1112-color-calibration","title":"11.1.2 Color Calibration","text":"<p>+---------------------------------+------------------------------------+ | The camera is seeing something  | {width=\"3.5in\" | |                                 | height=\"3.236111111111111in\"}      | | Anything that is accepted     |                                    | | (true) will be passed through |                                    | | as white, everything else   |                                    | | will be black (false). With |                                    | | the default values, everything  |                                    | | is white                        |                                    | |                                 |                                    | | #                               |                                    | |                                 |                                    | | We are going to calibrate the   |                                    | | camera to only keep the color   |                                    | | yellow (the dots in the middle  |                                    | | of the road). To do that we     |                                    | | need to understand how HSV      |                                    | | colors work                     |                                    | +=================================+====================================+ +---------------------------------+------------------------------------+</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_23","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_24","title":"100doc","text":"<p>{width=\"5.182292213473316in\"   height=\"2.1011537620297465in\"}</p> <p>The Hue is often referred to as a degree and goes between 0 and 180</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_25","title":"100doc","text":"<p>+----------------------------------------+-----------------------------+ | So if we chose an H value of 0 we      | { | |                                        | width=\"2.682292213473316in\" | | #                                      | hei                         | |                                        | ght=\"1.9184601924759406in\"} | | All of these values are represented by |                             | | the H value of 0. We would expect      |                             | | something similar to the above square  |                             | | except yellow if we put in the value   |                             | | of 30                                  |                             | +========================================+=============================+ | Because we are trying to filter out    | {w | | we will set the lowH and highH to 25   | idth=\"2.7505752405949258in\" | | and 35 Here is what we see with these  | hei                         | | values                                 | ght=\"2.5364588801399823in\"} | |                                        |                             | | #                                      |                             | |                                        |                             | | the camera is seeing something like    |                             | | this before we filter                  |                             | +----------------------------------------+-----------------------------+</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_26","title":"100doc","text":"<p>We want to keep everything in the    {width=\"2.7552088801399823in\"   upper right corner of these(picture  height=\"1.9664621609798776in\"}   this red square is yellow). As you    can see, if the S value gets too      low(left to right), we only see       white, and if the V value gets too    low(up and down) we only see black    values. There is not a problem with   V or S being too high, as that gives    us a pure color(top right). With      this in mind, we will keep only what    is in the upper right corner by       leaving the highS and highV at the    max and setting the low values to     about half way                       </p> <p>Oops, looks like that filters out    {width=\"2.8177088801399823in\"   too much. We can adjust these values height=\"2.604245406824147in\"}   until we get what we want. (also      notice that in the original image     the yellow looked pretty white so we    will lower the lowS until we see      some good results with little noise) </p> <p>And there you go (while playing with {width=\"2.8388976377952755in\"   these two bars (lowS and lowV) will  height=\"2.6406255468066493in\"}   almost always result in a good image    like this, note that paying           attention to the lighter and darker   parts of the color you are filtering    for will help you set your lowS and   lowV values with a higher accuracy    and speed)(if the bright spots are    disappearing, allow more S, if the    shadows aren\\'t showing up, allow     more V)                              </p> <p>Inverted_filter Whatever color   {width=\"3.2263068678915134in\"   you selected to \"track\" this slider  height=\"1.8697922134733158in\"}   will invert it which is basically     rejecting what you were originally    tracking and now tracking the exact   opposite. This feature is nice when   the road color is very consistent     and you want to track all road        markers, yellow dashed lines, white   lanes etc.                           </p> <p>+-----------------------------------+-----------------------------------+ | If for some reason you still have | {width=\"3.6041666666666665in\" | | to change these settings, the     | height=\"2.611111111111111in\"}     | | following settings can be         |                                   | | adjusted                          |                                   | +===================================+===================================+ | gray_thresh This will put a   | {width=\"3.6041666666666665in\" | | approximately gray such that only | height=\"2.2916666666666665in\"}    | | the white pixels can pass through |                                   | | the filter and potentially        |                                   | | resulting the small noise in the  |                                   | | background to black pixels        |                                   | |                                   |                                   | | Kernel_size This value        |                                   | | represents the size of the kernel |                                   | | to be convolved with the image    |                                   | | with the two transforms, Erosion  |                                   | | &amp; Dilation                        |                                   | |                                   |                                   | | Erosion_itterations The       |                                   | | higher this value, the more times |                                   | | the kernel is convolved with the  |                                   | | image which results in shrinking  |                                   | | pixel noise. This means if there  |                                   | | is some small noise like in the   |                                   | | photo above, the erosion          |                                   | | transform will minimize that      |                                   | | noise further by shrinking its    |                                   | | distribution. The Erosion         |                                   | | transform not only minimizes the  |                                   | | noise but in general all          |                                   | | distributions which means that    |                                   | | even our detected road marker     |                                   | | will shrink! This issue is fixed  |                                   | | with the dilation transform.      |                                   | |                                   |                                   | | Dilation_itterations The      |                                   | | higher this value, the more times |                                   | | the kernel is convolved with the  |                                   | | image which results in enlarging  |                                   | | pixel values. We want to undo the |                                   | | effects of the erosion transform  |                                   | | on the road marker which is ok    |                                   | | because our noise has already     |                                   | | been filtered out so it won\\'t    |                                   | | come back!                        |                                   | +-----------------------------------+-----------------------------------+</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_27","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#1113-linelane-calibration","title":"11.1.3 Line/Lane Calibration","text":"<p>This part of the calibration is now about manipulating the image dimensions, the geometry of the road lines and parameters to adjust the steering behavior of the robot.</p> <p>min_width and max_width {width=\"3.2291666666666665in\"   filter based on the size of the      height=\"3.3115430883639547in\"}   dotted lines found                   </p> <p>For example we can see if we set the {width=\"3.2395833333333335in\"   min to be 15 it will eliminate some  height=\"3.3333333333333335in\"}   of the smaller lines in the distance    because they are too thin (below15)  </p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_28","title":"100doc","text":"<p>+------------------------------------+---------------------------------+ | Number_of_lines correlates to  | !                               | | the number of lines found in the   | {width=\"3.6041666666666665in\" | |                                    | height=\"2.0277777777777777in\"}  | | When it is set to 4, only up to 4  |                                 | | lines will be used                 |                                 | +====================================+=================================+ +------------------------------------+---------------------------------+</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_29","title":"100doc","text":"<p>+------------------------------------+---------------------------------+ | Error_threshold Specify the    | !                               | | acceptable error the robot will    | {width=\"3.6168350831146108in\" | | error\\\". This means that           | height=\"2.0364588801399823in\"}  | | everything inside the two        |                                 | | vertical red bars ([error         | Some intuition, if on a         | | bounds)]{.underline} will be    | curved path,                | | ignored and only the detected road |                                 | | lines outside of the error bounds  | As the distance decreases   | | are used when determining the      | between the error bounds, the   | | steering angle. The simple error   | robot will steer towards the    | | cost function implemented will     | roadlines that are closest  | | determine the minimum error        | to it (basically looking down). | | detected (closets road line) and   |                                 | | steer towards it. Again, in this   | As the distance increases,  | | cost function, the road lines      | the robot will start steering   | | within the error bars are ignored. | towards detected road lines     | |                                    | that are further away (will | | This value also plays a role in    | start looking ahead rather than | | determining throttle values which  | previously).                    | | is discussed in the [actuator     |                                 | | calibration]{.                     |                                 | | underline} |                                 | | section.                           |                                 | |                                    |                                 | | Some intuition, if on a straight |                                 | | path,                            |                                 | |                                    |                                 | | If all the detected lines fall     |                                 | | within the error bounds, then the  |                                 | | algorithm will assume an error of  |                                 | | zero and not change its steering   |                                 | | angle. So, if the error bars are   |                                 | | too wide, this can cause some      |                                 | | \"drifting\" (not like Tokyo         |                                 | | drift..) behavior to occur which   |                                 | | can make the car go unstable and   |                                 | | lose the path.                     |                                 | +====================================+=================================+ +------------------------------------+---------------------------------+</p> <p>Camera_centerline is used to    {width=\"3.6041666666666665in\"   calibrate the actual center         height=\"3.013888888888889in\"}   position of the camera frame.        Fastest way to calibrate this is to    grab a ruler and align it along the    center of the car and then toggle    the slider bar such that the       green vertical bar (true car       center line) is lined up directly    in the middle of the ruler. The      value of slider represents pixel %  </p> <p>Frame_width and                 {width=\"3.2403423009623795in\"   rows_to_watch are used to crop  height=\"5.026042213473316in\"}   the image vertically and             horizontally                        </p> <p>And rows_offset will give a     {width=\"3.3541666666666665in\"   vertical pan adjustment             height=\"4.625in\"}</p> <p>The higher rows_ofset is, the       {width=\"3.3541666666666665in\"   further down it looks               height=\"3.3194444444444446in\"}</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#1114-actuator-calibration","title":"11.1.4 Actuator Calibration","text":"<p>This part of the calibration is now about identifying the speeds the robot should go in different situations and adjusting how much the car steers left and right.</p> <p>+-----------------------------------------+----------------------------+ | A PID controller is implemented on the  | {width=\"2.71875in\" | | in autonomous mode.                     | hei                        | |                                         | ght=\"5.527777777777778in\"} | | Kp_steering is the value for the    |                            | | proportional error term                 |                            | |                                         |                            | | Ki_steering is the value for the    |                            | | integral error term                     |                            | |                                         |                            | | Kd_steering is the value for the    |                            | | derivative error term                   |                            | |                                         |                            | | Steering_mode                       |                            | |                                         |                            | | -   Mode_0 sets max left for    |                            | |     fixing any offset                   |                            | |                                         |                            | | -   Mode_1 sets straight        |                            | |     steering limit                      |                            | |                                         |                            | | -   Mode_2 sets max right       |                            | |     steering limit                      |                            | |                                         |                            | | Steering_value is the value used to |                            | | visualize the steering sensitivity and  |                            | | setting steering constraints.           |                            | |                                         |                            | | Throttle Calibration In order to    |                            | | calibrate the throttle you will want to |                            | | set 3 separate values. See the systems  |                            | | response plot at the end of this        |                            | | section to get more intuition on how    |                            | | throttle scheduling works               |                            | |                                         |                            | | Throttle _mode                     |                            | |                                         |                            | | -   Mode_0 sets zero_throttle   |                            | |     (desired throttle for neutral)  |                            | |                                         |                            | | -   Mode_1 sets max_throttle    |                            | |     (desired throttle when there are  |                            | |     no errors for road line tracking) |                            | |                                         |                            | | -   Mode_2 sets min_throttle    |                            | |     (desired throttle with errors     |                            | |     present for road line tracking,     |                            | |     [NOT REVERSE]{.underline})        |                            | |                                         |                            | | Max RPM for those using a vesc,     |                            | | this sets upper limit on RPM            |                            | |                                         |                            | | Steering _polarity swaps the       |                            | | steering direction                      |                            | |                                         |                            | | Throttle _polarity swaps the       |                            | | throttle direction                      |                            | |                                         |                            | | Test_motor_control tests out the    |                            | | PID steering controller and the         |                            | | throttle scheduling values as the car   |                            | | will start tracking the road lines.     |                            | | [This test should be done on the test   |                            | | stand and not for actual autonomous     |                            | | navigation, that\\'s the next            |                            | | section!]{.underline}                   |                            | +=========================================+============================+ +-----------------------------------------+----------------------------+</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#11141-clarification-on-throttle-modes","title":"11.1.4.1 Clarification on throttle modes","text":"<p>Again we are going to be setting 3 separate values. The default mode to calibrate first is throttle mode_0. When toggling the throttle_mode slider to different modes, whatever value that is currently set for the throttle_value slider will be the value that is taken in for that mode as the values are being saved to the calibration file in real time. [As long as you are on any particular mode, you are editing the values for that mode, whatever the last value you had when you were on that mode will be the value that is saved to that mode when you end the calibration script.]{.mark} For example, if the throttle_value slider is currently set to 1000 now when toggling the throttle_mode slider from mode_0 to mode_1, the value for mode_0 will remain at 1000 and then the process of editing mode_1 will begin.</p> <p>{width=\"5.838205380577428in\" height=\"4.640625546806649in\"}</p> <p>In the system response plot, it shows the relationship between the max throttle, min throttle and error threshold values. This is the idea of throttle scheduling based on the tracking error (x-axis). For example, let\\'s say the tracking error is 0.4, based on the throttle plot, the throttle command will be about 0.37 and if the error is less than the error threshold (0.2 in the example throttle plot) then the throttle response will be max throttle which is 0.4 in this example.</p> <p>From here, exit the calibration script with ctrl+c and the chosen throttle mode (mode_0, mode_1 and mode_2) values (and all other calibration values) will be properly stored in the [calibration file found in the config directory of the lane_detection_pkg. [Don\\'t forget that you need to recompile after calibration.]{.underline}[\\ \\ ]{.underline}]{.mark}</p> <p>[Make sure to always build_ros2 after closing the camera calibration program. The reason is that on startup, the camera calibration overwrites the values in the ros_racer_config.yaml with whatever values it was compiled with. Even though the values you put in will get autosaved in the ros_racer_config.yaml, if they don\\'t get compiled they will just get overwritten the next time the calibration starts up.]{.mark}</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#1115-camera-navigation","title":"11.1.5 Camera Navigation","text":"<p>Only proceed with this section AFTER you have gone through the calibration procedure above. At this point, your robot should be taken off of the test stand and put on to the track so it can move freely. Please be alert of the people around you and be ready to shutdown the robot if it starts drifting off the path.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#11151-ros1","title":"11.1.5.1 ROS1","text":"<p>From the terminal</p> <p>roslaunch ucsd_robocar_nav1_pkg camera_nav_launch.launch</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_30","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#11152-ros2","title":"11.1.5.2 ROS2","text":"<p>Remember, if you make even a single change ANYWHERE in your code (which also includes .yaml files) you must rebuild the package. Check the [Source ROS2]{.underline} section.</p> <p>From the terminal</p> <p>source_ros2</p> <p>Modify the hardware config file to turn on the vesc_without_odom and the camera you have equipped</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml</p> <p>Then modify the node config file to activate only all_components and camera_nav launch files</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml</p> <p>Then rebuild and launch</p> <p>build_ros2</p> <p>ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py</p> <p>If the robot is not responding the way you were expecting, turn on the debugger plots which will show you:</p> <ul> <li> <p>your black and white filter (will show how good your filtering is     working)</p> </li> <li> <p>the detected lines with bounding boxes and error bound etc. like     from [calibration]{.underline}</p> </li> </ul> <p>This is done easily by setting the ros parameter from the terminal.</p> <ul> <li> <p>1:debug on</p> </li> <li> <p>0:debug off</p> </li> </ul> <p>By default, the debugger is set to 0 (off) for performance reasons and is only recommended to turn on when trying to find out why the robot starts deviating from the expected outcome of following the track. Which is most likely due to changes in environment lighting.</p> <p>From [another]{.underline} terminal (turn on debugger)</p> <p>ros2 param set /lane_detection_node debug_cv 1</p> <p>From [another]{.underline} terminal (turn off debugger)</p> <p>ros2 param set /lane_detection_node debug_cv 0</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#112-tubewall-following-coming-soon","title":"11.2 Tube/Wall Following (coming soon)","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#113-slam","title":"11.3 SLAM","text":"<p>Simultaneous Localization and Mapping (SLAM) has been completely integrated with our Docker image but is only currently available in ROS1 and NOT ROS2. Below is a short tutorial of getting SLAM working on the robot using ROS-Bridge.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#1131-requirements","title":"11.3.1 Requirements","text":"<p>Make sure that the following hardware is plugged in and operational before launching the docker container</p> <ul> <li> <p>Lidar</p> </li> <li> <p>Logitech controller (for manual control while mapping)</p> </li> <li> <p>VESC or Adafruit PWM board</p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_31","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#1132-starting-slam","title":"11.3.2 Starting SLAM","text":"<p>We will need 3 terminals to get SLAM working, 1 for the [Hector-SLAM algorithm in ROS1]{.underline}, another for ROS-Bridge and the last one for sensors/hardware and control/path planning algorithms.</p> <p>From terminal 1</p> <p>source_ros2</p> <p>Modify the hardware config file to turn on the vesc_with_odom and the lidar you have equipped</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml</p> <p>Then modify the node config file to activate only all_components, sensor_visualization and f1tenth_vesc_joy_launch launch files</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml</p> <p>Then rebuild</p> <p>build_ros2</p> <p>From terminal 1</p> <p>source_ros1</p> <p>roslaunch ucsd_robocar_nav1_pkg ros_racer_mapping_launch.launch</p> <p>From terminal 2</p> <p>source_ros_bridge</p> <p>From terminal 3</p> <p>ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py</p> <p>Notice RVIZ is launched automatically with a pre-configured setup file to show a URDF of your robot doing SLAM!</p> <p>Now depending on what the robot is trying to achieve with slam, modify the all_nodes.yaml file to turn on which navigation/control algorithms for the robot to use. If unsure, or specifically trying to create a map it\\'s suggested to turn on all_components ([where a lidar and actuator type has been selected]{.underline}), manual_joy_control_launch to have manual control of the robot while creating the map.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#11321-saving-the-map","title":"11.3.2.1 Saving the map","text":"<p>There are a few options to do this step. The first option is from the map_server node and the other is from the hector_mapping node. Each provides different output map formats so it could be useful knowing both commands depending on what projects you'll be working on.</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#113211-map_server","title":"11.3.2.1.1 map_server","text":"<p>For this method, the map files are created in your current working directory so keep that in mind. There is a maps folder in the ucsd_robocar_nav1_pkg that can be used to store all your maps.</p> <p>From another terminal</p> <p>source_ros1</p> <p>rosrun map_server map_saver -f ms_map_test</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#113212-hector_mapping","title":"11.3.2.1.2 hector_mapping","text":"<p>For this method, the maps generated are saved automatically to the maps directory in ucsd_robocar_nav1_pkg with a generic name with some time stamp.</p> <p>From another terminal</p> <p>source_ros1</p> <p>rostopic pub syscommand std_msgs/String \\\"savegeotiff\\\"</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_32","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#1133-localization-in-a-pre-made-map","title":"11.3.3 Localization in a pre-made map","text":"<p>This will only load maps that were created with the map_server node! You will also need to modify the car_type in this launch file just as done previously.</p> <p>From terminal</p> <p>source_ros1</p> <p>roslaunch ucsd_robocar_nav1_pkg ros_racer_nav_launch.launch</p> <p>Notice RVIZ is launched automatically with a pre-configured setup file to show a URDF of your robot, your saved map and it localizing itself!</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#12-data-collection","title":"12. Data Collection","text":"<p>To collect data being broadcasted over the topics that are actively being published, turn on whichever nodes needed to publish that topic information but make sure that the rosbag_launch option in the [node_config]{.underline} is also turned on which is the switch for data collection. This will record ALL topics to the \"rosbag\" which is a unique file type to ROS. Then a package called [bagpy]{.underline} is used to convert the data into csv format which is useful for viewing/analysis.</p> <p>Modify the hardware config file to turn on any sensors you have equipped and need for data collection/moving</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml</p> <p>Then modify the node config file to activate only all_components, rosbag_launch launch files and any other launch file you need to move the robot around (i.e. manual control, camera_nav etc.)</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml</p> <p>Then rebuild and launch</p> <p>build_ros2</p> <p>ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_33","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#13-f1-tenth-simulator","title":"13. F1 Tenth Simulator","text":"<p>A light-weight ROS2 simulator using RVIZ can be used for various scenarios such as model validation, experiment repeatability and general experimentation. The simulator uses a 2D dynamic bicycle-car model to simulate how the car would actually move in an environment. There are several maps that are already made and can be used in the simulator or you can create your own map with the SLAM techniques discussed above and load that map into the simulator as well. Below are the steps to pick the following plug-ins for the simulator: a map, path planning technique, and a controller as an example. Feel free to change any of the plug-ins.</p> <p>NOTE: For the example below, we are going to use the joystick for the controller so you will need a controller plugged into your computer. Since we will be doing manual control, we do not need a path planner activated.</p> <p>NOTE: Only use the simulator on the [X86 docker image]{.underline} and not the Jetson.</p> <p>Modify the hardware config file to turn off any sensors you have</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml</p> <p>Then modify the node config file to activate only the simulator and f1tenth_vesc_joy_launch, launch files and any other launch file you need to move the robot around (i.e. manual control, camera_nav etc.)</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml</p> <p>Modify the f1 tenth simulator config file to update the map (if needed)</p> <p>gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/f1_tenth_sim.yaml</p> <p>Then rebuild and launch</p> <p>build_ros2</p> <p>ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#131-creating-a-map-with-paint-coming-soon","title":"13.1 Creating a Map with Paint (coming soon)","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#132-updating-vehicle-parameters-coming-soon","title":"13.2 Updating Vehicle Parameters (coming soon)","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#133-adding-multiple-vehicles-coming-soon","title":"13.3 Adding Multiple Vehicles (coming soon)","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#_34","title":"100doc","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#14-troubleshooting","title":"14. Troubleshooting","text":"<p>Below are the links to the troubleshooting sections when using either ROS1 or ROS2. There are troubleshooting guides for every single package to potentially help solve any common problems.</p> <ul> <li> <p>[ucsd_robocar_hub1 troubleshooting     links]{.underline}</p> </li> <li> <p>[ucsd_robocar_hub2 troubleshooting     links]{.underline}</p> </li> </ul>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#15-frequently-used-linux-commands","title":"15. Frequently Used Linux commands","text":""},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#151-wifi","title":"15.1 WIFI","text":"<p>[Rescan wifi list:]{.mark} sudo nmcli device wifi rescan</p> <p>[Show wifi list:]{.mark} sudo nmcli device wifi list</p> <p>[Connect to wifi network:]{.mark} sudo nmcli device wifi connect \\&lt;NETWORK_NAME&gt; password \\&lt;NETWORK_PASSWORD&gt;</p> <p>Restart networking: sudo service NetworkManager restart</p> <p>Check network interfaces: nmcli device status</p> <p>Check if connected internet: ping google.com</p> <p>Disable power save mode for wifi: sudo iw dev wlan0 set power_save off</p> <p>Networking info: ifconfig</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#152-hardware-tests","title":"15.2 Hardware Tests","text":"<p>List connected USB devices: lsusb</p> <p>Check if joystick is working: jstest /dev/input/js0</p> <p>Check if x_11 forwarding is working: xeyes</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#153-file-management","title":"15.3 File management","text":"<p>Listing files in a directory: ls</p> <p>Copy file: cp old_file_name new_file_name</p> <p>Copy directory: cp -r old_directory_name new_directory_name</p> <p>Move file: mv file_name /path/to/new/file/location/file_name</p> <p>Move directory: mv -r directory_name /path/to/new/directory/location/directory_name</p> <p>Delete file: rm -f file_name</p> <p>Delete directory: rm -rf directory_name</p> <p>[To copy a file from B to A while logged into B:]{.mark}</p> <p>scp /path/to/file username@A_ip_address:/path/to/destination</p> <p>[To copy a file from B to A while logged into A:]{.mark}</p> <p>scp username@B_ip_address:/path/to/file /path/to/destination</p>"},{"location":"markdown-documentation/100-UCSD%20Robocar%20Framework/100doc/#154-system-control","title":"15.4 System Control","text":"<p>Terminate process by PID: sudo kill -9 PID_number</p>"},{"location":"markdown-documentation/15-VESC%20Setup%20Instructions/15-VESC%20Setup%20Instructions/","title":"UCSD VESC Setup Instructions","text":"<p>Version 2.0 30 Dec 2022</p> <p>These instructions are for different versions of the VESC \u2014 pay attention to what version you have. Choosing incorrect firmware may harm the VESC.</p> <p>We will be using the VESC Tool software (linked here) to upgrade the firmware in the VESC and measure parameters from the brushless DC motor (BLDC).</p> <p>PLACE THE CAR ON THE PROVIDED STAND WHEN WORKING WITH THE VESC AND ENSURE THAT THE WHEELS ARE CLEAR AND CAN SPIN FREELY</p> <p>You will need the battery to power the VESC and a long micro USB cable to connect the VESC to your computer. See the instructor if you need a longer cable.</p> <p>If the VESC is connected to the SBC, disconnect them before connecting to the VESC.</p> <p>AGAIN, THE WHEELS OF THE ROBOT WILL SPIN, PLEASE MAKE SURE THE WHEELS ARE CLEAR TO ROTATE.</p>"},{"location":"markdown-documentation/15-VESC%20Setup%20Instructions/15-VESC%20Setup%20Instructions/#vesc-tools","title":"VESC Tools","text":"<p>You can ignore prompts to update VESC Tools to the latest version if you are using the class-provided install.</p> <p></p> <p>To start, connect the battery voltage checker to the LiPo battery BMS pins. Then connect the main battery terminals to the input for the VESC and a micro USB cable between the VESC and your computer.</p> <p>Select the VESC Tools connect icon:</p> <p></p> <p>or </p> <p></p> <p>Note: When you connect the VESC to VESC Tools, it may warn you that there is a newer firmware version for the VESC.</p> <p>Feel free to update it on VESC 6.x versions. Do not upgrade VESC 4.x versions.</p>"},{"location":"markdown-documentation/15-VESC%20Setup%20Instructions/15-VESC%20Setup%20Instructions/#updating-the-firmware","title":"Updating the firmware","text":"<p>As of 30 Dec, 2022, the VESC 6.x firmware to be used is version 6.00. You do not need to update it if the version is the same.</p> <p>Navigate to the Firmware tab on the left side in the VESC Tool.</p> <p></p> <p>Select the arrow without the text \"All\" next to it. Hovering over it should display the text \"Update firmware on the connected VESC\"</p> <p></p> <p>Do not unplug the VESC while the firmware is updating, you may damage the VESC.</p>"},{"location":"markdown-documentation/15-VESC%20Setup%20Instructions/15-VESC%20Setup%20Instructions/#motor-detection","title":"Motor Detection","text":"<p>Go to the Welcome and Wizards tab.</p> <p></p> <p>In the menu at the bottom, select <code>Autoconnect</code>. Once the VESC is connected, select <code>Setup Motors FOC</code>. </p> <p></p> <p>You will be prompted to load the default parameters. Select <code>Yes</code>. Then select <code>Generic</code> and  <code>Medium Outrunner</code> after that.</p> <p></p> <p></p> <p>Select <code>BATTERY_TYPE_LIION_3_0__4_2</code> as the battery type (3.0-4.2 is the voltage range of the cells).</p> <p></p> <p>The batteries we are using currently have 4 cells in series; input that below with a capacity of 4500 mAh. When prompted to set current limits by going to the next section, just proceed without setting them.</p> <p>Now check the <code>Direct Drive</code> setting. The wheel diameter is 100.00mm and the motor is a 4 pole - 8 magnet setup.</p> <p></p> <p>Then run the detection procedure.</p> <p></p> <p>Now determine if your motor direction needs to be reversed.</p> <p></p> <p>Once you are done, press finish at the bottom.</p> <p>Now, write the motor configuration to the VESC using the button along the right side of the screen.</p> <p></p> <p>Then write the app configuration.</p> <p></p>"},{"location":"markdown-documentation/15-VESC%20Setup%20Instructions/15-VESC%20Setup%20Instructions/#sensor-detection","title":"Sensor Detection","text":"<p>We are now going to measure characteristic parameters of the motor.</p> <p>Resistance and Inductance</p> <p></p> <p>Motor Flux Linkage</p> <p></p> <p>Then press <code>Apply</code> at the right side of the window.</p> <p></p> <p>Now check the Hall Sensor table under the <code>Hall Sensors</code> tab at the top. Your table should look something like this:</p> <p></p> <p>Write the motor configuration and the app configuration:</p> <p></p> <p></p>"},{"location":"markdown-documentation/15-VESC%20Setup%20Instructions/15-VESC%20Setup%20Instructions/#enable-servo-control-for-steering","title":"Enable Servo Control for Steering","text":"<p>We need to enable the VESC PWM output in order to control the servo for steering.</p> <p>In <code>General</code> under <code>App Settings</code>, set <code>Enable Servo Output</code> to <code>True</code> and write the app configuration.</p> <p></p> <p></p> <p></p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/","title":"Jetson Nano Configuration","text":"<p>This document will take you through the process of setting up your Jetson Nano. This document is a re-written version of this document, which may have some useful information if you are ever stuck: https://docs.google.com/document/d/1TF1uGAFeDARNwbPkgg9xg4NvWPJvrsHW2I_IN9dWQH0/edit</p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#flashing-image","title":"Flashing Image","text":"<p>A Jetson Nano runs off of a externally mounted microSD card, which will first need an image flashed onto it. A custom UCSD Jetson Nano Developer Kit image with some pre-configured settings and files can be downloaded here: UCSD Image. You will need to install a program to flash this download onto your microSD card, we recommend Etcher: Etcher Download. </p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#using-etcher-to-write-ucsd-image","title":"Using Etcher to Write UCSD Image","text":"<p>1) Using an adapter, plug your mircoSD card into a computer with the zipped image file downloaded 2) Open Etcher, select the zipped file under \"Flash from file\" and the microSD card under \"Select target\", and click \"Flash!\" to write the image to the microSD card 3) After flashing is complete, eject the microSD card from your computer and plug it into the back of the Jetson Nano. NOTE: this is a push-in-to-lock and push-in-to-unlock the uSD card. Please do not pull the uSD card out of the slot before unlocking it, otherwise you may damage your JTN and or the uSD card  </p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#mobaxterm-installation","title":"MobaXterm Installation","text":"<p>A good tool for Windows machines when doing embedded programming is MobaXterm, which offers more robust SSH communication with single board computers. Here you can find the download link.</p> <ol> <li>Click on the green \"MobaXterm Home Edition v24.2 (Installer Edition)\" to download a zip file.  </li> <li>Extract the zip file and launch the installer from the file explorer (not the .dat file).  </li> <li>Follow the instructions in the installer.  </li> <li>When installation is complete, launch MobaXterm and select \"Start new terminal\" to be placed on the command line.  </li> </ol>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#powering-jetson-nano","title":"Powering Jetson Nano","text":"<p>To power the Jetson Nano you can use the provided 5V 4A power supply. This plugs into the barrel jack port to the left of the USB ports. The Jetson is recieving power and on if there is a green LED near the microUSB port. However, you may need to give it a minute or two to boot and load software. The fan will NOT turn on until you follow the documentation below to install the proper software. If you wish to power the Jetson through a microUSB connection, remove the jumper on the J48 pins near the barrel jack port. Please keep track of the jumper by leaving in connected to only one of the J48 pins.  </p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#jetson-nano-ssh-configuration","title":"Jetson Nano SSH Configuration","text":"<p>Since we do not have any monitors or keyboard connected to our Jetson Nano, we will need to remotely login to the Jetson using Secure Shell (SSH) Protocol. Initially, we will need to remotely login to the Jetson Nano using a wire. While connected over the wire we will do some initial configuration and connect to the UCSDRoboCar wifi network. Once the network is configured, we will then be able to wirelessly SSH into the Jetson over the UCSDRoboCar network.  </p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#wire-ssh-communication","title":"Wire SSH Communication","text":"<p>1) Power your Jetson Nano using the details above. Plug a microUSB cable from your computer into the microUSB port on the Jetson Nano. Give the Jetson a minute or two to boot up. NOTE: Not all microUSB cables contain the proper wiring to transmit data. If you are sruggling to establish a connection with the Jetson Nano in the following steps, try switching to another microUSB cable. 2) Open a terminal on your computer and type in: <code>ssh jetson@192.168.55.1</code>. This command is calling ssh and asking to login to the user \"jetson\" on the device \"192.168.55.1\". This IP address is unique to the wired connection, and the network IP address will be something different. 3) You will be prompted to input a password for the user you are logging in as. The default username is <code>jetson</code>, which you specified in the ssh command in step 1. The default password is <code>jetsonucsd</code>, which you will type in after the ssh command when prompted.  4) If login is successful, you will be placed into the home directory of the Jetson Nano. If you want to return to your local device, you can enter <code>exit</code> and you will logoff the Jetson.   </p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#wifi-configuration","title":"Wifi Configuration","text":"<p>1) Follow the above steps to log into the Jetson over a USB cable.  2) Ensure the USB wifi adapter supplied with the kit is plugged into the Jetson Nano. 3) Once logged in and in the home directory, lets make sure the network service is running properly: <code>sudo systemctl start networking.service</code> 4) List the available wifi networks: <code>sudo nmcli device wifi list</code>. If your desired wifi network is not being listed you can use <code>sudo nmcli device wifi rescan</code> to refresh the list. If needed, you can try to reboot the entire Jetson Nano with <code>sudo reboot now</code> and try scanning again. NOTE: Rebooting will take a couple minutes and will require you to re-ssh into the Jetson Nano. 5) To connect the Jetson Nano to a listed wifi network: <code>sudo nmcli device wifi connect &lt;ssid_name&gt; password &lt;password&gt;</code>. In the case of UCSDRobocar, the command would be <code>sudo nmcli device wifi connect UCSDRoboCar password UCSDrobocars2018</code>. After a few seconds the terminal should print out a success message similar to: <code>Device 'wlan0' successfully activated with 'bab49f3e-b40c-4201-84f9-972ac83ddcb7'</code>  6) To further ensure the Jetson Nano is connected to Wifi, try pinging google using <code>ping google.com</code>. A message should be printing every second with latency information. 7) The Jetson Nano will now have a unique IP address that identifies the machine on the wifi network. You can check what this IP address is by entering <code>ifconfig</code>, which will return the Jetson Nano's network interface configuration. Under the <code>wlan0</code> section, the Jetson Nano's network IP address should be displayed. In the case below, the network IP address is <code>192.168.113.165</code>. If an IP address is not showing up, ensure that the Wifi connection is properly configured and operating using the steps above.  </p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#wifi-power-saving-settings","title":"Wifi Power Saving Settings","text":"<p>If Wifi Power Saving is on, it can cause the Jetson Nano to lag and slow down. Here we show how to turn off power saving mode.</p> <p>1) To temporarily turn off power saving, you can use the command <code>sudo iw dev wlan0 set power_save off</code> 2) If you wish to make the change persistent, you will need to edit a config file using a text editor. Ensure Nano is installed by updating pre-existing packages with <code>sudo apt-get update</code> and installing nano with <code>sudo apt-get install nano</code>. We can edit the file by entering <code>sudo nano /etc/NetworkManager/conf.d/default-wifi-powersave-on.conf</code> and changing <code>wifi.powersave = 3</code> to <code>wifi.powersave = 2</code>.   </p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#testing-wifi-ssh-communication","title":"Testing Wifi SSH Communication","text":"<p>We will quickly test that we can communicate and login to the Jetson Nano over the UCSDRoboCar wifi network. While testing the wifi, we can stay logged in on the wire and simply login in a different terminal on our local computer.</p> <p>1) Ensure your laptop is connected to the UCSDRoboCar wifi.  2) Open a new terminal on your local computer and try <code>ssh jetson@&lt;wlan0-IP-address&gt;</code>, where 'wlan0-IP-address' is the IP address you found in the previous section. The password will be the same as the wire SSH.  </p> <p></p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#configuring-hostname-and-password","title":"Configuring Hostname and Password","text":"<p>To make the SSH process easier and more secure, we should set a unique static hostname for our Jetson Nano and change the password. This new static hostname can then replace the long IP address in the SSH command.  </p> <p>1) Check the current hostname by entering <code>hostnamectl</code>.  2) Change your hostname to something unique using <code>sudo hostnamectl set-hostname &lt;new-hostname&gt;</code>. We recommend including your team name in your Jetson Nano's hostname. Confirm your changes by entering <code>hostnamectl</code> again. You must also change the hostname in /etc/hosts using a text editor: <code>sudo nano /etc/hosts</code>. Change \"jetson\" to the hostname you choose.  </p> <p></p> <p>3) You can change the password by simply entering <code>passwd</code> and following the prompts. 4) Make sure to remember your newly selected hostname and password, you will need them to log into your Jetson Nano.  5) Now when logging into your Jetson Nano using SSH, you can replace the IP address with your static hostname. This is useful, as sometimes the network will change the IP address of the Jetson Nano, while the hostname will stay static. <code>ssh jetson@&lt;new-hostname&gt;</code>. Remember, we are just changing the devices identifier, we are still logging into the device as the \"jetson\" user with the password we set, which is why \"jetson\" remains unchanged in the SSH command. NOTE: If you are having trouble connecting, try adding a .lan or .local to the end of your hostname in the ssh command. If you are wired to the jetson, then you might end up connecting over the wire.   </p>"},{"location":"markdown-documentation/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/30-UCSD%20Robocar%20Jetson%20Nano%20Configuration/#fan-configuration","title":"Fan Configuration","text":"<p>Our Jetson Nano's are outfitted with a heat sink and fan for cooling. The image should come pre-loaded with the fan drivers, but if the fan is not spinning here are the steps to download the drivers.</p> <p>1) Enter the projects directory from the home directory <code>cd ~/projects</code>. If a projects folder doesn't exist, make one in the home directory: <code>cd</code> to go to the home directory, then <code>mkdir projects</code> to create a directory named \"projects\", then enter the new directory <code>cd projects</code>.  2) In ~/projects, enter <code>git clone https://github.com/Pyrestone/jetson-fan-ctl.git</code>. This clones the github repository \"jetson-fan-ctl\" into your projects directory. 3) Make sure your packages are updated using <code>sudo apt-get update</code> and install python3-dev using <code>sudo apt install python3-dev</code>.  4) Enter \"jetson-fan-ctl\" using <code>cd jetson-fan-ctl</code> and enter <code>./install.sh</code> to install the fan firmware.  5) You can customize fan settings in <code>/etc/automagic-fan/config.json</code> using a text editor of your choice. The command to edit in nano is <code>sudo nano /etc/automagic-fan/config.json</code>. You can check the status of the fan by entering <code>sudo service automagic-fan status</code>.   </p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/","title":"Virtual Machine with DonkeyCar/DonkeySim AI Simulator","text":""},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#vmware-player-installation","title":"VMware Player Installation","text":"<p>Virtual machines (VMs) are essentially computers running on computers \u2014 you can simulate a different operating system than the one native to your computer.</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#prerequisites","title":"Prerequisites","text":"<ul> <li>Download the VMware Player installer depending on your OS; check this document for information for additional information if necessary</li> <li>Windows</li> <li> <p>Intel and Apple Silicon Macs, not verified yet</p> </li> <li> <p>Download the Ubuntu VM image \u2014 make sure you have enough space on your disk   (~40 GB zipped, ~50 GB unzipped)</p> </li> <li> <p>https://drive.google.com/file/d/1aGVPzoEPYW0GxUnVGjzkiNsqqJFgZ7hb/view?usp=sharing</p> </li> <li> <p>Minimum 8 GB system RAM on host machine</p> </li> <li>If your system only has 8 GB of RAM, set the ammount of memory allocated to 5120 MB (5 GB)</li> <li>If your system has at least 16 GB of RAM, enter the VM settings for the image and increase the RAM alloted to 8 GB; the VM must not be running to do so</li> </ul> <p></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#vmware-setup","title":"VMware setup","text":"<p>When you install the VMware Player, you will be prompted on options for the install. As you go through the install wizard be sure that you:</p> <ul> <li>Accept the license agreement</li> <li>Install the WHP automatically</li> <li>Don't enable enhanced keyboard</li> <li>Set a custom file path for the Player (if you desire)</li> <li>Select whether or not you want to opt into the diagnostics agreement</li> <li>Select where you want you your shortcuts to be</li> <li>Then press <code>Install</code></li> </ul> <p>Open VMware Player and select Open a Virtual Machine You will be prompted to select an image to be added \u2014 select the image you downloaded with the <code>.vmx</code>  extension</p> <p>It should now appear in the list on the left of the VMware Player window \u2014 single-click the image and select Edit Virtual Machine Settings</p> <p>Here you can edit the memory settings and any other settings required to run the VM</p> <p>If you experience an error with respect to Intel-VT or AMD-V, disable the virualization engine in the Processors tab </p> <p></p> <p></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#initial-boot-up-of-vm","title":"Initial Boot up of VM","text":"<p>If necessary, enable virtualization in your BIOS/UEFI. When you are ready, start the virtual machine.</p> <ul> <li>Login Credentials</li> <li>User: ucsd</li> <li>Password: UcsdStudent </li> </ul>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#cutting-and-pasting","title":"Cutting and Pasting","text":"<ul> <li>If cutting and pasting is not working from the host to the VM, open a terminal in the VM and run the following commands:</li> </ul> <pre><code>    sudo apt-get autoremove open-vm-tools\n    sudo apt-get install open-vm-tools-desktop\n    sudo reboot now\n</code></pre>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#connecting-game-controller","title":"Connecting Game Controller","text":"<p>Connecting a game controller is useful in order to control the car used in the simulations you will be running and other projects (these can include Playstation or Xbox controllers, or the Logitech controller likely included in your kit).</p> <p>These should be connected using via a USB cable, Bluetooth, or a USB dongle.</p> <p>When connecting a controller, the VM should ask if the input device will be connected to the host system or the virtual machine \u2014 connect it to the VM by selecting the name of the VM.</p> <p> </p> <p></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#verify-controller-connection","title":"Verify Controller connection","text":"<p>The controller will be identified as js0 (or js# if there are multiple joysticks connected to the system)</p> <p>Run the following command in a VM terminal:</p> <pre><code>ls /dev/input\n</code></pre> <p>If the controller is connected, it should appear as js0 in the terminal output.</p> <p></p> <p>To test the joystick controls, run in a terminal:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y jstest-gtk\njstest /dev/input/js0\n</code></pre> <p></p> <p>Then interact with the controller to see the values printed to the terminal change (analog inputs should change smoothly, while digital inputs like button presses change between on and off)</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#custom-controller","title":"Custom Controller","text":"<p>If your controller is not behaving correctly, or you need to generate new controller mappings, you can generate custom controllers. </p> <p>See https://docs.donkeycar.com/parts/controllers/ for controller support; custom mapping is linked at the bottom of the page.</p> <p>To setup a new controller or modify input mappings, you can use the Joystick Wizard (described here: https://docs.donkeycar.com/utility/donkey/#joystick-wizard)</p> <p>The joystick wizard creates a custom controller named \"my_joystick.py\" in the <code>mycar</code> folder. To enable it, in the <code>myconfig.py</code> file, set <code>CONTROLLER_TYPE=\"custom\"</code> </p> <p>To run the wizard, from a terminal in the PATH/TO/mycar directory, run </p> <pre><code>donkey createjs\n</code></pre> <p>To determine if the system can see the input device, jstest can be used. If it is not installed, run <code>sudo apt install joystick</code></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#donkeycar-ai-framework","title":"DonkeyCar AI Framework","text":"<p>This software allows you to train an AI model to run simulated or even physical vehicles using computer vision (either virtually or in reality).</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#launching-the-simulator","title":"Launching the Simulator","text":"<p>Using the file explorer in the VM, navigate to <code>~/projects/DonkeySimLinux/</code> and execute the file <code>donkey_sim.x86_64</code></p> <p></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#track-names","title":"Track Names","text":"<p>Depending on the track to be raced on, you need to change the track to train on; those include:</p> <ul> <li>donkey-circuit-launch-track-v0</li> <li>donkey-warren-track-v0</li> <li>donkey-mountain-track-v0</li> </ul>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#customizing-virtual-car","title":"Customizing Virtual Car","text":"<p>From a terminal, run <code>atom myconfig.py</code> from the <code>~/projects/d4_sim/</code> directory.</p> <p>Within the <code>myconfig.py</code> file, change the:</p> <ul> <li>car_name</li> <li>racer_name</li> <li>your country location (under \"country\")</li> <li>a fun fact (under \"bio\")</li> <li>car color (in the dictionary entry for \"body_rgb\")</li> </ul> <p></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#example-config-file","title":"Example Config File","text":"<pre><code># 04Jan22\n# UCSD mods to make easier for the UCSD students to use the Donkey-Sim\n# the following uncommented lines where copied here from the body of myconfig.py below\nDONKEY_GYM = True\n# DONKEY_SIM_PATH = \"remote\"\nDONKEY_SIM_PATH = \"/home/ucsd/projects/DonkeySimLinux/donkey_sim.x86_64\"\n# DONKEY_GYM_ENV_NAME = \"donkey-warren-track-v0\"\nDONKEY_GYM_ENV_NAME = \u201cdonkey-mountain-track-v0\u201d\n# UCSD yellow color in RGB = 255, 205, 0\n# UCSD blue color in RGB = 0, 106, 150\nGYM_CONF = { \"body_style\" : \"car01\", \"body_rgb\" : (255, 205, 0), \"car_name\" : \"UCSD-148-YourName\", \"font_size\" : 30} # body style(donkey|bare|car01) body rgb 0-255\nGYM_CONF[\"racer_name\"] = \"UCSD-148-YourName\"\nGYM_CONF[\"country\"] = \"USA\"\nGYM_CONF[\"bio\"] = \"Something_about_you, ex: Made in Brazil\"\n#\n# SIM_HOST = \"donkey-sim.roboticist.dev\"\n SIM_ARTIFICIAL_LATENCY = 0\nSIM_HOST = \"127.0.0.1\"              # when racing on virtual-race-league use host \"roboticists.dev\"\n# SIM_ARTIFICIAL_LATENCY = 30          # Use the value when you ping roboticists.dev. When racing on virtual-race league, use 0 (zero)\n\n# When racing, to give the ai a boost, configure these values.\nAI_LAUNCH_DURATION = 3            # the ai will output throttle for this many seconds\nAI_LAUNCH_THROTTLE = 1            # the ai will output this throttle value\nAI_LAUNCH_KEEP_ENABLED = True      # when False ( default) you will need to hit the AI_LAUNCH_ENABLE_BUTTON for each use. This is safest. When this True, is active on each trip into \"local\" ai mode.\n#\n# When using a joystick modify these specially USE_JOYSTICK_AS_DEFAULT = True\n# JOYSTICK\n# USE_JOYSTICK_AS_DEFAULT = True     #when starting the manage.py, when True, will not require a --js option to use the joystick\nJOYSTICK_MAX_THROTTLE = 1.0         #this scalar is multiplied with the -1 to 1 throttle value to limit the maximum throttle. This can help if you drop the controller or just don't need the full speed available.\nJOYSTICK_STEERING_SCALE = 0.8       #some people want a steering that is less sensitve. This scalar is multiplied with the steering -1 to 1. It can be negative to reverse dir.\nAUTO_RECORD_ON_THROTTLE = True      #if true, we will record whenever throttle is not zero. if false, you must manually toggle recording with some other trigger. Usually circle button on joystick.\nJOYSTICK_DEADZONE = 0.2             # when non zero, this is the smallest throttle before recording triggered.\n# #Scale the output of the throttle of the ai pilot for all model types.\nAI_THROTTLE_MULT = 1.0              # this multiplier will scale every throttle value for all output from NN models\n#\n</code></pre>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#get-latency-from-remote-server","title":"Get Latency from Remote Server","text":"<p>To get the latency between your computer and the server, ping it using the command </p> <pre><code>  ping donkey-sim.roboticist.dev\n</code></pre> <p></p> <p>Since this computer is on the same network as the server, the delay is much lower than 0.5 ms. When pinging the server within the US, you should expect about 20-60 ms.</p> <p>Replace the value of <code>SIM_ARTIFICIAL_LATENCY</code> with the average ping delay (e.g. <code>SIM_ARTIFICIAL_LATENCY=30</code>)</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#collecting-data","title":"Collecting Data","text":"<p>The AI model works via behavioral cloning. In order to collect data for it, we need to drive the car in the virtual environment.</p> <p>From a terminal, enter the donkey virtual environment with the command:</p> <pre><code>conda activate donkey\n</code></pre> <p>(donkey) should now appear at the beginning of the terminal prompt.</p> <p></p> <p>Enter the donkeycar directory</p> <pre><code>cd ~/projects/d4_sim\n</code></pre> <p>To drive the car in order to collect data, run</p> <pre><code>python manage.py drive\n</code></pre> <p></p> <p>Open a web browser and go to <code>http://localhost:8887</code></p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#driving-using-mouse-and-keyboard","title":"Driving using Mouse and Keyboard","text":"<p>From the web address above, you can control the car using a virtual joystick.</p> <p></p> <p>20 laps is recommended for an initial dataset.</p> <p>To stop the DonkeyCar framework, use CTRL + C in the terminal</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#driving-using-a-gamepad","title":"Driving using a Gamepad","text":"<p>To use a physical joystick without using the web browser, edit this section in <code>myconfig.py</code>.</p> <pre><code># #JOYSTICK\n# USE_JOYSTICK_AS_DEFAULT = False      #when starting the manage.py, when True, will not require a --js option to use the joystick\n# JOYSTICK_MAX_THROTTLE = 1         #this scalar is multiplied with the -1 to 1 throttle value to limit the maximum throttle. This can help if you drop the controller or just don't need the full speed available.\n# JOYSTICK_STEERING_SCALE = 1       #some people want a steering that is less sensitve. This scalar is multiplied with the steering -1 to 1. It can be negative to reverse dir.\n#AUTO_RECORD_ON_THROTTLE = True      #if true, we will record whenever throttle is not zero. if false, you must manually toggle recording with some other trigger. Usually circle button on joystick.\n# CONTROLLER_TYPE = 'ps4'            #(ps3|ps4|xbox|pigpio_rc|nimbus|wiiu|F710|rc3|MM1|custom) custom will run the my_joystick.py controller written by the `donkey createjs` command\n# USE_NETWORKED_JS = False            #should we listen for remote joystick control over the network?\n# NETWORK_JS_SERVER_IP = None         #when listening for network joystick control, which ip is serving this information\n# JOYSTICK_DEADZONE = 0.01            # when non zero, this is the smallest throttle before recording triggered.\n</code></pre> <p>Set <code>USE_JOYSTICK_AS_DEFAULT</code> to <code>True</code> and set the controller type <code>CONTROLLER_TYPE</code> to one from the adjacent list (ps3|ps4|...).</p> <p>You may have to uncomment lines in order for them to take effect.</p> <p>When using a controller, the face buttons can have useful functions:</p> <ul> <li>Deleting 100 data points (@20Hz == 5s)</li> <li>Emergency stop</li> <li>Change operations mode (User control, AI model)</li> </ul> <p>Otherwise you may have to determine the function of each button from the terminal outputs when they are pressed.</p> <p>20 laps is recommended for an initial dataset.</p> <p>To stop the DonkeyCar framework, use CTRL + C in the terminal</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#deleting-data-not-to-be-used-in-training","title":"Deleting Data not to be used in Training","text":"<p>Data for training is stored in the <code>~/projects/d4_sim/data</code> directory.</p> <p></p> <p>We can delete data by removing the <code>data</code> folder and creating a new one.</p> <p>Run this command in the <code>d4_sim</code> directory. Be careful \u2014 there is no undoing this if the command runs successfully.</p> <pre><code>rm -rf data\n</code></pre> <p>Then create a new <code>data</code> directory with:</p> <pre><code>mkdir data\n</code></pre>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#training-and-testing","title":"Training and Testing","text":"<p>Using the data in the <code>data</code> folder, we can train a model and give it a name (e.g. 8july24_sim_160x120_20_1.h5)</p> <p>20 laps is recommended for an initial dataset.</p> <p>To do so, run this command from the <code>d4_sim</code> folder.</p> <pre><code>python train.py --model=models/YOUR_MODEL_NAME.h5 --type=linear --tubs=data/\n</code></pre> <p>To test the model, run:</p> <pre><code>python manage.py drive --model=models/YOUR_MODEL_NAME.h5 --type=linear\n</code></pre> <p>Enabling the model is done by pressing the change operation mode button twice. The terminal should state that the car is in AI mode.</p> <p>If the car does not perform well around corners, it could be that throtte and steering data is not being recorded when navigating them.</p> <p>By default, the program records steering only when a throttle input is detected \u2014 when slowing down to corner, this means steering data may not be recorded. To fix this, you can edit in <code>myconfig.py</code>:</p> <pre><code>AUTO_RECORD_ON_THROTTLE = True\n</code></pre> <p>Set this to false. Now, in order to record data, you must press the record button to begin input recording. The terminal will print out when recording is enabled and the amount of samples.</p> <p>If you increase the number of samples recorded after training a model, you can train a new model that uses all of the data in the <code>data</code> folder (old and new \u2014 be sure to give it a different name).</p> <p>To train data from a specific tub and transfer to a previous model:</p> <pre><code>python train.py --tub ~/projects/d4_sim/data/TUB_NAME  --transfer=models/PREVIOUS_MODEL.h5  --model=models/NEW_MODEL.h5\n</code></pre> <p>Tubs are subsections of the data folder that you may create to separate training data. To use all the data in the <code>data</code> folder, do not include a tub name after <code>~/projects/d4_sim/data/</code> in the tub argument.</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#upgrading-to-the-latest-donkey-sim-and-donkey-gym-if-needed","title":"Upgrading to the latest Donkey-Sim and Donkey-Gym (if needed)","text":""},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#common-issues","title":"Common Issues","text":""},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#slow-fps-locally","title":"Slow FPS Locally","text":""},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#ucsd-gpu-cluster-instructions","title":"UCSD GPU Cluster Instructions","text":"<p>Do not use the cluster until you are told the GPU cluster is ready to use.</p> <p>Do not train on the cluster until you have demonstrated model training on your local machine.</p> <p>Instructions from UCSD IT</p> <p>To train our models faster, we can use more powerful GPUs with higher throughputs.</p> <p>On the virtual machine you will be using 2 terminals:</p> <ol> <li> <p>Local Session: Used to interact with the virtual machine</p> </li> <li> <p>Remote Session: From this terminal you will ssh (secure shell) onto the GPU cluster using the proper account. </p> </li> </ol> <p>In the remote session terminal, ssh into the GPU cluster:</p> <pre><code>ssh &lt;username&gt;@dsmlp-login.ucsd.edu\n</code></pre> <p>You will be prompted for a password (case sensitive). No characters will be shown for security purposes.</p> <p></p> <p>Your shell prompt is replaced with your user login for the GPU cluster.</p> <p>You will have access to two containers \u2014 one with only a CPU, and another with the GPU. The GPU clusters are limited, so only use them for training.</p> <p>Available hardware options:</p> <p>Container for transfering data: (2 CPU 4 GB RAM)</p> <pre><code>launch-scipy-ml.sh -i ucsdets/donkeycar-notebook:latest\n</code></pre> <p>Container for training models: (8 CPU, 1 GPU, 16 GB RAM)</p> <pre><code>launch-scipy-ml.sh -g 1 -i ucsdets/donkeycar-notebook:latest\n</code></pre> <p>When creating the GPU container, the terminal should look like:</p> <p></p> <p>You should only have one container open at a time.</p> <p>When launching a container it creates a \"pod\"; in order to exit the pod, run in the terminal:</p> <pre><code>exit\n</code></pre> <p>To confirm that you have exited the container and the instance has successfully been deleted do</p> <pre><code>kubectl get pods\n</code></pre> <p>This should return \"no resources found\".</p> <p></p> <p>If there is a pod, delete it with:</p> <pre><code>kubectl delete pod &lt;POD_NAME&gt;\n</code></pre>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#transfering-data","title":"Transfering Data","text":"<p>In the Remote Session, prepare DonkeyCar.</p> <p>The donkey virtual environment should automatically be invoked for you; otherwise try </p> <pre><code>conda activate donkey\n</code></pre> <p>If donkey is not found, try </p> <pre><code>conda init\n</code></pre> <p>Relogin to the remote session and try the activate command again.</p> <p>Once you are in the virtual environment,</p> <pre><code>mkdir ~/projects\ncd ~/projects\ndonkey createcar --path d4_sim\ncd d4_sim\n</code></pre> <p>In the Local Session</p> <p>To copy over your <code>myconfig.py</code> file:</p> <pre><code>rsync -avr -e ssh myconfig.py &lt;user_name&gt;@dsmlp-login.ucsd.edu:projects/d4_sim/\n</code></pre> <p>To transfer data collected in the local session to the remote session:</p> <pre><code>rsync -avr -e ssh data/&lt;tub_name&gt; &lt;user_name&gt;@dsmlp-login.ucsd.edu:projects/d4_sim/data/\n</code></pre> <p>This sends specific tubs (e.g. tub_#_21-07-13 in this example) to the remote session.</p> <p></p> <p>The tubs should now appear in the remote session.</p> <p></p> <p>The <code>rsync</code> command syncs directories remotely from one system to another. That means it will only copy the differences between the two directories to save time and reduce load. Since the data does not exist initially on the remote system, the first use of <code>rsync</code> will copy the whole folder over to the remote system.</p> <p>Once the data is transferred, close the CPU pod (and verify that it is closed) and open a GPU pod to train on the data.</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#training-on-data","title":"Training on Data","text":"<p>Once the data is transferred to the remote session, training a model on it is the same as on a local session.</p> <p>In the Remote Session</p> <p>If you didn't use any tubs and just have your data in the data folder, this command should do the trick:</p> <pre><code>python train.py --model=models/yourmodelname.h5 --type=linear --tubs=data/\n</code></pre> <p>You can train multiple tubs at the same time with (the paths to the tubs must be separated by commas, no spaces). </p> <pre><code>python train.py --tub=data/tub1,data/tub2 --model=models/MODEL_NAME.h5 --type=linear\n</code></pre> <p>To alter a previous model with new data:</p> <pre><code>python train.py --tub=data/tub1 --model=models/NEW_MODEL_NAME.h5 --transfer=models/OLD_MODEL_NAME.h5\n</code></pre> <p>Note : If \"imgaug\" is not availible and Donkeysim generates an error, run </p> <pre><code>pip install imgaug\n</code></pre> <p>Once your model training has completed, close the GPU pod (verifying that it has closed) and open a CPU pod to transfer the data back to your local machine.</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#transferring-data-back-to-local-session","title":"Transferring Data back to Local Session","text":"<p>This is done similarly using the <code>rsync</code> command, but the source and destination are flipped since the data is going from the remote session to the local session.</p> <p>In the Local Session</p> <pre><code>rsync -avr -e ssh &lt;user_name&gt;@dsmlp-login.ucsd.edu:projects/d4_sim/models/&lt;model_file&gt; models/\n</code></pre> <p>Now you can test the car in the local session as before.</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/50-Virtual%20Machine%20and%20DonkeyCar%20AI%20Simulator/#using-a-remote-server-for-the-simulator","title":"Using a Remote Server for the Simulator","text":"<p>The simulator for DonkeyCar (that you found in  <code>~/projects/DonkeySimLinux/</code> and executed with the file <code>donkey_sim.x86_64</code>) can be run from a remote server instead of locally on your machine.</p> <p>The server's name is <code>donkey-sim.roboticist.dev</code></p> <p>You can connect to the remote server by changing the simulator host in the <code>myconfig.py</code> file.</p> <p></p> <p>Set the <code>SIM_HOST</code> from the local IP to <code>donkey-sim.roboticist.dev</code> and set the <code>SIM_ARTIFICIAL_LATENCY</code> to <code>0</code>.</p> <p>Don't forget to change the artificial latency, otherwise your car will experience both real and virtual latency and perform poorly.</p> <p>Since the simulator is running on a remote server, how can I view the car on the track?</p> <p>You can see the car on the livestream on</p> <p>https://www.twitch.tv/roboticists or https://www.twitch.tv/roboticists2</p> <p>The car should appear momentarily after you run the same command to start the car if you have configured the host properly.</p>"},{"location":"markdown-documentation/50-Virtual%20Machine%20with%20DonkeyCar%20AI%20Simulator/Donkeysim%20on%20an%20ARM%20Chip%20Mac/","title":"Setting up DonkeySim on a Mac M1 or newer (ARM Based)","text":"<p>The class virtual machine will not run on ARM based macs, but you can install the software natively instead. If you are on a linux or windows machine but are having issues with the virtual machine, following these instructions with the appropriate platform edits could work for you too. Installing donkeysim/donkeycar natively is more work, but it will run faster and use less space than the virtual machine.  We also have lab computers with donkeysim set up available to use, if you run into difficulties with setting up donkeysim on your computer don't hesitate to use the lab computers. We also have joysticks in the lab which make driving much more convenient.  </p> <p>These instructions are new for Fall 2024, and the donkey_sim app was just compiled for ARM macs by me (Alexander). Please let me know about any issues/vague things/improvements!  </p> <p>Download the donkeysim video game from here: https://drive.google.com/file/d/1FRDT7DiuKDhAuoKyqt8KyMnRAYsALx5Y/view?usp=sharing  Note that the arm64 compiled simulator does have a known bug where there are a few invisible cones outside the track that your car can collide with. I am working on fixing it!</p> <p>https://docs.donkeycar.com/guide/deep_learning/simulator/ contains everything you need to set up donkeycar on your computer. For your convenience I will write out the steps here:</p> <ol> <li>Download miniconda from https://conda-forge.org/download/, and run the bash script to install it. You can open your terminal app, and <code>cd Downloads</code> and then <code>bash Miniforge3-MACOSX-arm64.sh</code></li> <li><code>conda create -n donkey python=3.11</code> </li> <li><code>conda activate donkey</code> </li> <li><code>pip install donkeycar\\[pc\\]</code></li> <li><code>pip install git+https://github.com/tawnkramer/gym-donkeycar</code></li> <li><code>donkey createcar --path ./mycar</code></li> <li><code>cd mysim</code></li> <li><code>nano myconfig.py</code></li> <li><code>python manage.py drive</code></li> <li><code>cd mycar</code> then <code>open myconfig.py</code></li> <li>Edit myconfig.py to add in the lines     <code>DONKEY_GYM = True</code></li> </ol> <p><code>DONKEY_SIM_PATH = \"remote\"</code></p> <p><code>DONKEY_GYM_ENV_NAME = \"donkey-warren-track-v0\"</code></p> <p><code>SIM_HOST = \"127.0.0.1\"</code> </p> <p><code>GYM_CONF = { \"body_style\" : \"car01\", \"body_rgb\" : (255, 205, 0), \"car_name\" : \"UCSD-148-YourName\", \"font_size\" : 30} # body style(donkey|bare|car01) body rgb 0-255</code></p> <p><code>GYM_CONF[\"racer_name\"] = \"UCSD-148-YourName\"</code></p> <p><code>GYM_CONF[\"country\"] = \"USA\"</code></p> <p><code>GYM_CONF[\"bio\"] = \"pls work\"</code></p> <p>And then save and exit</p> <ol> <li>Start the donkey_sim app, and then run <code>python3 manage.py drive</code></li> <li>You should be able to go to http://localhost:8887 to control the car. Note, it will be easier to control the car if you hook up a joystick. Instructions to do that are in the other virtual machine document.</li> <li>Follow the other document for the rest of the instructions. For here on everything should work the same.</li> </ol>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60-OpenCV%20CUDA%20Accelerated/","title":"UCSD RoboCar OpenCV CUDA Accelerated","text":"<p>Version 1.8 - 26Nov2022</p> <p>Prepared by Dr. Jack Silberman Department of Electrical and Computer Engineering and  Dominic Nightingale Department of Mechanical and Aerospace Engineering University of California, San Diego 9500 Gilman Dr, La Jolla, CA 92093</p> <p># Installing an Open Source Computer Vision (OpenCV) package  with CUDA Support # As of Jan 2020, NVIDIA is not providing OpenCV optimized to use CUDA (GPU acceleration). # Search the web if you are curious why. # https://forums.developer.nvidia.com/t/opencv-cuda-python-with-jetson-nano/72902</p> <p>Interesting youtube video: https://www.youtube.com/watch?v=art0-99fFa8</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60-OpenCV%20CUDA%20Accelerated/#checking-opencv-build-information","title":"Checking openCV build information","text":"<p># ssh to the Single Board Computer (SBC) # Check to see if OpenCV for CUDA is available, search on the terminal output for CUDA), # if not,  we build OpenCV from source to use CUDA Acceleration</p> <p># From a terminal:  </p> <pre><code>python  \n\\&gt;\\&gt;import cv2  \n\\&gt;\\&gt;print cv2.getBuildInformation()   \n\\&gt;\\&gt;exit ()\n</code></pre> <p># ex:</p> <pre><code>NVIDIA CUDA:                   YES (ver 10.2, CUFFT CUBLAS FAST\\_MATH)  \nNVIDIA GPU arch:             53 62 72  \nNVIDIA PTX archs:\n</code></pre> <p>cuDNN:                         YES (ver 8.0)</p> <p>OpenCL:                        YES (no extra features)     Include path:                /tmp/build_opencv/opencv/3rdparty/include/opencl/1.2     Link libraries:              Dynamic load</p> <p>You can also use jtop to check if OpenCV was compiled with CUDA. jtop 6</p> <p>![][image4]</p> <p># If OpenCV is not using CUDA acceleration, let's build and install the latest OpenCV optimized # for CUDA</p> <p># As of 03Apr2022, I have not seen NVIDIA supplying OpenCV GPU accelerated on their # JetPack. We need to compile it from source. If you ask me, nice experience for you. # Let\u2019s do this</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60-OpenCV%20CUDA%20Accelerated/#_1","title":"UCSD RoboCar OpenCV CUDA Accelerated","text":""},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60-OpenCV%20CUDA%20Accelerated/#installing-opencv-with-cuda","title":"Installing openCV with CUDA","text":"<p># This step may take several hours to compile and build OpenCV in a SBC such as the # Jetson Nano (JTN) or Jetson Xavier NX (JNX). It took my JTN four hours to complete. # For the JTN, please make sure the fan is installed including software to enable high # CPU power mode</p> <p># Better do it overnight. Be patient, please keep in mind that you are using a low power  # single board computer (SBC) # Make sure you are using the external power supply to power the jetson, not a USB cable.</p> <p># https://devtalk.nvidia.com/default/topic/1054949/process-to-install-opencv-4-1-on-nano/ # https://github.com/mdegans/nano_build_opencv</p> <p># Please make sure your computer will not sleep so you maintain an active ssh connection # while you are building OpenCV because you will need to type the user jetson password  # again to complete the install.  # Reconnecting on the same ssh session is not trivial</p> <p>![][image5]</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60-OpenCV%20CUDA%20Accelerated/#using-a-linux-host-use-screen","title":"# Using a Linux Host? Use Screen","text":"<p># You should consider using screen to enable you to reconnect to a previous ssh session. # Note that this is not mandatory for successfully building opencv with cuda, you can also just ssh normally # Do you need to install screen?</p> <p>sudo apt-get install screen</p> <p># From a terminal on Linux host or on your VM: screen # ssh jetson@ name of IP address of your machine ssh jetson@ucsdrobocar-xxx-yy.local</p> <p># If you get disconnected, open a terminal again on your computer then type screen -r</p> <p># screen -r should reconnect to the previous ssh session that was open</p> <p># Here is some more detailed information on using screen https://www.howtogeek.com/howto/ubuntu/keep-your-ssh-session-running-when-you-disconnect # if not installed in your host sudo apt-get install screen</p> <p>Now you can start a new screen session by just typing screen at the command line. You\u2019ll be shown some information about screen. Hit enter, and you\u2019ll be at a normal prompt.</p> <p>To disconnect (but leave the session running)</p> <p>Hit Ctrl + A and then Ctrl + D in immediate succession. You will see the message [detached]</p> <p>To reconnect to an already running session</p> <p>screen -r</p> <p>To reconnect to an existing session, or create a new one if none exists</p> <p>screen -D -r</p> <p># More info on using screen https://ma.ttias.be/screen-a-must-for-ssh/ screen -ls There are screens on: 27111.screen_untarring  (Detached) 27097.screen_disking    (Detached) 2 Sockets in /var/run/screen/S-root. This will show a list of all running screen-sessions at any given time. You can pick up a previous screen-session, by typing</p> <p>screen -r \\&lt;name_of_session&gt; screen -r</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60-OpenCV%20CUDA%20Accelerated/#_2","title":"UCSD RoboCar OpenCV CUDA Accelerated","text":"<p># Since your SBC will be busy, how can you see it is working for you? # # Besides htop or top, there is another cool utility called jetson_stats sudo apt-get install python3-pip sudo -H pip3 install -U jetson-stats</p> <p># reboot sudo reboot now # Then you can run jtop 4</p> <p>You can use jtop to add more swap space using the left and right keys and clicking the plus button ![][image6]</p> <p>Add 4G of swap and press \\&lt;S&gt; to enable it.</p> <p>![][image7]</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60-OpenCV%20CUDA%20Accelerated/#removing-old-version-of-opencv","title":"Removing old version of OpenCV","text":"<p># First lets remove the current OpenCV version and reboot the JTN</p> <p>sudo sudo apt-get purge *libopencv* sudo reboot now</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60-OpenCV%20CUDA%20Accelerated/#compiling-opencv-from-source","title":"Compiling OpenCV from source","text":"<p># screen then ssh back to the SBC # If needed, create the \\~/projects directory cd\\~ mkdir projects cd projects</p> <p># For Jetson Nano  git clone https://github.com/mdegans/nano_build_opencv.git cd nano_build_opencv</p> <p># For the Jetson Nano, lets modify the script to use 2 CPU cores vs. 1.  # Note: for the TX2 and Xaviers the script automatically uses more CPU cores. # Need to modify the configuration for the CUDNN we are using # First see what you have installed using jtop use option INFO press 7</p> <p>![][image8] ![][image9] #Remove the 8.7 from the CUDA_ARCH_BIN version, it causes a compatibility issue #Also, make sure the CUDNN_VERSION=\u20198.0\u2019, it will fail otherwise</p> <p>nano build_opencv.sh</p> <p># modify this line to have JOBS=2 else     JOBS=1  # you can set this to 4 if you have a swap file</p> <p>JOBS=2 # save the file / exit nano</p> <p># https://opencv.org/releases/ # look for the latest version</p> <p>![][image10] # As of 24Nov23, 4.8.0 is the latest version</p> <p># You can use jtop to have Jetson Clocks enabled to improve the performance of the Jetson # 6 CTRL  and then press s # To make it enabled at boot too, press e</p> <p>![][image11]</p> <p># From a terminal ./build_opencv.sh 4.8.0</p> <p># Again, this will take a good while. You may consider doing this at night before you go to bed. It is really boring # after a while (hours) looking that the SBC compile and build OpenCV from source #  Please make sure the computer you are using to SSH, disable sleep, and if using a notebook computer  # have in connected to the charger  # Preferable you use screen before ssh to be able to reconnect in  # case your computer lose the SSH connection. #  If you wish, you can see that the SBC cores of the CPU are #  busy by opening another terminal or a new tab on the same terminal window then #  ssh to the SBC and run the command top #  I like better jtop (if needed, install htop or jtop). # # ex: ![][image12]</p> <p># After a few hours you see that you need to enter a little information to finish the build. That is when we have # problems if you get disconnected from your SSH</p> <p>#  You can leave the temporary builds in place Do you wish to remove temporary build files in /tmp/build_opencv ?  (Doing so may make running tests on the build later impossible) Y/N y # Enter y # save some space on disc </p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60-OpenCV%20CUDA%20Accelerated/#checking-the-install","title":"Checking the Install","text":"<p># When the installation is complete, let's check the version of OpenCV that was installed  </p> <p># First reboot SBC sudo reboot now</p> <p># ssh back to the SBC</p> <p>#Using jtop to confirm the OpenCV GPU (CUDA)  compiled</p> <p>jtop 7 INFO</p> <p># look for the OpenCV version and that it was compiled with CUDA</p> <p>![][image13] ![][image14]</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60-OpenCV%20CUDA%20Accelerated/#lets-make-sure-python3-finds-opencv-cv2","title":"# Let\u2019s make sure Python3 finds OpenCV (CV2)","text":"<p># Need to adapt this for the python and OpenCV versions that you are using</p> <pre><code>python3 \\-c 'import cv2 as cv; print(cv.\\_\\_version\\_\\_)'\n</code></pre> <p>#Traceback (most recent call last):   #File \"\\&lt;string&gt;\", line 1, in \\&lt;module&gt; #ModuleNotFoundError: No module named 'cv2'</p> <p># If you don\u2019t see the version of the OpenCV that you built listed, it might be because you forgot to delete the previous # OpenCV version that was installed, go back to the section \u201cRemoving old version of OpenCV\u201d and then # build and  install OpenCV again.</p> <p># What version of python3 do you have?</p> <p>python3 #Python 3.8.10 (default, May 26 2023, 14:05:08)  #[GCC 9.4.0] on linux #Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; exit()</p> <p>ex:  </p> <pre><code>cd /usr/local/lib/python3.6/site-packages/cv2/python-3.6  \n</code></pre> <p># or  </p> <pre><code>cd /usr/local/lib/python3.8/site-packages/cv2/python-3.8\n</code></pre> <p># remove the previous cv2.so  in case it is still there. </p> <pre><code>sudo rm \\-rf cv2.so  \n\\# mv cv2.cpython-36m-xxx\\-linux-gnu.so cv2.so\n</code></pre> <pre><code>ls cv2.\\*\n</code></pre> <pre><code>\\# cv2.cpython-38-aarch64-linux-gnu.so\n</code></pre> <pre><code>ln \\-s /usr/local/lib/python3.8/site-packages/cv2/python-3.8cv2.cpython-38-aarch64-linux-gnu.so cv2.so cv2.so\n</code></pre> <pre><code>sudo cp cv2.cpython-36m-aarch64-linux-gnu.so cv2.so\n</code></pre> <p># or  </p> <pre><code>sudo cp cv2.cpython-38-aarch64-linux-gnu.so cv2.so  \n</code></pre> <p># Alternatively you could just rename the file  </p> <pre><code>\\# sudo mv cv2.cpython-36m-aarch64-linux-gnu.so cv2.so  \n</code></pre> <pre><code>\\# sudo cp cv2.cpython-38-aarch64-linux-gnu.so cv2.so\n</code></pre> <p># Lets create a link at the home directory for cv2</p> <pre><code>cd \\~  \nsudo rm \\-rf cv2.so  \nln \\-s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so cv2.so  \n\\# or   \nln \\-s /usr/local/lib/python3.8/site-packages/cv2/python-3.8/cv2.so cv2.so\n</code></pre> <p>Checking install for python3  </p> <pre><code>python3  \nimport cv2  \ncv2.\\_\\_version\\_\\_  \nexit ()\n</code></pre> <p># Resulting on something similar to this </p> <p>jetson@ucsdrobocar-xxx-yy:\\~$ python3 Python 3.6.9 (default, Jun 29 2022, 11:45:57)  [GCC 8.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import cv2 &gt;&gt;&gt; cv2.__version__ '4.6.0' &gt;&gt;&gt; exit() jetson@ucsdrobocar-xxx-yy:\\~$ </p> <p>Python 3.8.10 (default, May 26 2023, 14:05:08)  [GCC 9.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import cv2 &gt;&gt;&gt; cv2.__version__ '4.8.0' &gt;&gt;&gt; exit ()</p> <p># Here is quicker way to test cv2 on Python 3</p> <pre><code>python3 \\-c 'import cv2 as cv; print(cv.\\_\\_version\\_\\_)'\n</code></pre> <pre><code>4.6.0\n</code></pre> <p># or  </p> <pre><code>4.8.0\n</code></pre> <p>end of Installing an Open Source Computer Vision (OpenCV) package  with CUDA Support</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/","title":"Installing an Open Source Computer Vision (OpenCV) package with CUDA","text":"<p>Support</p> <p># As of Jan 2020, NVIDIA is not providing OpenCV optimized to use CUDA (GPU acceleration).</p> <p># Search the web if you are curious why.</p> <p># [https://forums.developer.nvidia.com/t/opencv-cuda-python-with-jetson-nano/72902]{.underline}</p> <p>Youtube Video about the process: https://www.youtube.com/watch?v=art0-99fFa8</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#checking-opencv-build-information","title":"Checking openCV build information","text":"<p>ssh to the Single Board Computer (SBC)</p> <p>Check to see if OpenCV for CUDA is available, search on the terminal output for CUDA),</p> <p>if not, we build OpenCV from source to use CUDA Acceleration</p> <p>From a terminal:</p> <pre><code>python\n\n\\&gt;\\&gt;import cv2\n\n\\&gt;\\&gt;print cv2.getBuildInformation()\n\n\\&gt;\\&gt;exit ()\n</code></pre> <p>ex:</p> <p>NVIDIA CUDA: YES (ver 10.2, CUFFT CUBLAS FAST_MATH)</p> <p>NVIDIA GPU arch: 53 62 72</p> <p>NVIDIA PTX archs:</p> <p>cuDNN: YES (ver 8.0)</p> <p>OpenCL: YES (no extra features)</p> <p>Include path: /tmp/build_opencv/opencv/3rdparty/include/opencl/1.2</p> <p>Link libraries: Dynamic load</p> <p>You can also use jtop to check if OpenCV was compiled with CUDA.</p> <p>jtop</p> <p>6</p> <p> # If OpenCV is not using CUDA acceleration, let\\'s build and install the latest OpenCV optimized</p> <p># for CUDA</p> <p># As of 03Apr2022, I have not seen NVIDIA supplying OpenCV GPU accelerated on their</p> <p># JetPack. We need to compile it from source. If you ask me, nice experience for you.</p> <p># Let's do this</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#_1","title":"OpenCV With CUDA Acceleration","text":""},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#installing-opencv-with-cuda","title":"Installing openCV with CUDA","text":"<p># This step may take several hours to compile and build OpenCV in a SBC such as the</p> <p># Jetson Nano (JTN) or Jetson Xavier NX (JNX). It took my JTN four hours to complete.</p> <p># For the JTN, please make sure the fan is installed including software to enable high CPU power mode</p> <p># Better do it overnight. Be patient, please keep in mind that you are using a low power single board computer (SBC)</p> <p># Make sure you are using the external power supply to power the jetson, not a USB cable.</p> <p># [https://devtalk.nvidia.com/default/topic/1054949/process-to-install-opencv-4-1-on-nano/]{.underline}</p> <p># [https://github.com/mdegans/nano_build_opencv]{.underline}</p> <p># Please make sure your computer will not sleep so you maintain an active ssh connection</p> <p># while you are building OpenCV because you will need to type the user jetson password</p> <p># again to complete the install.</p> <p># Reconnecting on the same ssh session is not trivial</p> <p></p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#using-a-linux-host-use-screen","title":"# Using a Linux Host? Use Screen","text":"<p># You should consider using screen to enable you to reconnect to a previous ssh session.</p> <p># Do you need to install screen?</p> <p>sudo apt-get install screen</p> <p># From a terminal on Linux host or on your VM:</p> <p>screen</p> <p># ssh jetson@ name of IP address of your machine</p> <p>ssh jetson@ucsdrobocar-xxx-yy.local</p> <p># If you get disconnected, open a terminal again on your computer then type</p> <p>screen -r</p> <p># screen -r should reconnect to the previous ssh session that was open</p> <p># Here is some more detailed information on using screen</p> <p>[https://www.howtogeek.com/howto/ubuntu/keep-your-ssh-session-running-when-you-disconnect]{.underline}</p> <p># if not installed in your host</p> <p>sudo apt-get install screen</p> <p>Now you can start a new screen session by just typing screen at the command line. You'll be shown some information about screen. Hit enter, and you'll be at a normal prompt.</p> <p>To disconnect (but leave the session running)</p> <p>Hit Ctrl + A and then Ctrl + D in immediate succession. You will see the message [detached]</p> <p>To reconnect to an already running session</p> <p>screen -r</p> <p>To reconnect to an existing session, or create a new one if none exists</p> <p>screen -D -r</p> <p># More info on using screen</p> <p>[https://ma.ttias.be/screen-a-must-for-ssh/]{.underline}</p> <p>screen -ls</p> <p>There are screens on:</p> <p>27111.screen_untarring (Detached)</p> <p>27097.screen_disking (Detached)</p> <p>2 Sockets in /var/run/screen/S-root.</p> <p>This will show a list of all running screen-sessions at any given time. You can pick up a previous screen-session, by typing</p> <p>screen -r \\&lt;name_of_session&gt;</p> <p>screen -r</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#_2","title":"OpenCV With CUDA Acceleration","text":"<p># Since your SBC will be busy, how can you see it is working for you?</p> <p>#</p> <p># Besides htop or top, there is another cool utility called jetson_stats</p> <p>sudo apt-get install python3-pip</p> <p>sudo -H pip3 install -U jetson-stats</p> <p># reboot</p> <p>sudo reboot now</p> <p># Then you can run</p> <p>jtop</p> <p>4</p> <p>You can use jtop to add more swap space using the left and right keys and clicking the plus button</p> <p></p> <p>Add 4G of swap and press \\&lt;S&gt; to enable it.</p> <p></p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#removing-old-version-of-opencv","title":"Removing old version of OpenCV","text":"<p># First lets remove the current OpenCV version and reboot the JTN</p> <p>sudo sudo apt-get purge *libopencv*</p> <p>sudo reboot now</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#compiling-opencv-from-source","title":"Compiling OpenCV from source","text":"<p># screen then ssh back to the SBC</p> <p># If needed, create the \\~/projects directory</p> <p>cd\\~</p> <p>mkdir projects</p> <p>cd projects</p> <p># For Jetson Nano</p> <p>git clone https://github.com/mdegans/nano_build_opencv.git</p> <p>cd nano_build_opencv</p> <p># For the Jetson Nano, lets modify the script to use 2 CPU cores vs. 1.</p> <p># Note: for the TX2 and Xaviers the script automatically uses more CPU cores.</p> <p># Need to modify the configuration for the CUDNN we are using</p> <p># First see what you have installed using jtop use option INFO press 7</p> <p></p> <p></p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#remove-the-87-from-the-cuda_arch_bin-version-it-causes-a","title":"Remove the 8.7 from the CUDA_ARCH_BIN version, it causes a","text":"<p>compatibility issue</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#also-make-sure-the-cudnn_version80-it-will-fail-otherwise","title":"Also, make sure the CUDNN_VERSION='8.0', it will fail otherwise","text":"<p>nano build_opencv.sh</p> <p># modify this line to have JOBS=2</p> <p>else</p> <p>JOBS=1 # you can set this to 4 if you have a swap file</p> <p>JOBS=2</p> <p># save the file / exit nano</p> <p># https://opencv.org/releases/</p> <p># look for the latest version</p> <p></p> <p># As of 24Nov23, 4.8.0 is the latest version</p> <p># You can use jtop to have Jetson Clocks enabled to improve the performance of the Jetson</p> <p># 6 CTRL and then press s</p> <p># To make it enabled at boot too, press e</p> <p></p> <p># From a terminal</p> <pre><code>./build_opencv.sh 4.8.0\n</code></pre> <p># Again, this will take a good while. You may consider doing this at night before you go to bed. It is really boring</p> <p># after a while (hours) looking that the SBC compile and build OpenCV from source</p> <p># Please make sure the computer you are using to SSH, disable sleep, and if using a notebook computer</p> <p># have in connected to the charger</p> <p># Preferable you use screen before ssh to be able to reconnect in</p> <p># case your computer lose the SSH connection.</p> <p># If you wish, you can see that the SBC cores of the CPU are</p> <p># busy by opening another terminal or a new tab on the same terminal window then</p> <p># ssh to the SBC and run the command top</p> <p># I like better jtop (if needed, install htop or jtop).</p> <p>#</p> <p># ex:</p> <p></p> <p># After a few hours you see that you need to enter a little information to finish the build. That is when we have</p> <p># problems if you get disconnected from your SSH</p> <p># You can leave the temporary builds in place</p> <p>Do you wish to remove temporary build files in /tmp/build_opencv ?</p> <p>(Doing so may make running tests on the build later impossible)</p> <p>Y/N y</p> <p># Enter y</p> <p># save some space on disc</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#checking-the-install","title":"Checking the Install","text":"<p># When the installation is complete, let\\'s check the version of OpenCV that was installed</p> <p># First reboot SBC</p> <p>sudo reboot now</p> <p># ssh back to the SBC</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#using-jtop-to-confirm-the-opencv-gpu-cuda-compiled","title":"Using jtop to confirm the OpenCV GPU (CUDA) compiled","text":"<p>jtop</p> <p>7 INFO</p> <p># look for the OpenCV version and that it was compiled with CUDA</p> <p></p> <p>{</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#lets-make-sure-python3-finds-opencv-cv2","title":"# Let's make sure Python3 finds OpenCV (CV2)","text":"<p># Need to adapt this for the python and OpenCV versions that you are using</p> <p>python3 -c \\'import cv2 as cv; print(cv.__version__)\\'</p> <p># If you don't see the version of the OpenCV that you built listed, it might be because you forgot to delete the previous OpenCV version that was installed, go back to this document \"Removing old version of OpenCV\" and then build and install OpenCV again. If OpenCV shows up in jtop then disregard this, there is some other issue preventing it from showing up.</p> <p># What version of python3 do you have?</p> <p>python3</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#traceback-most-recent-call-last","title":"Traceback (most recent call last):","text":""},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#file-string-line-1-in-module","title":"File \\\"\\&lt;string&gt;\\\", line 1, in \\&lt;module&gt;","text":""},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#modulenotfounderror-no-module-named-cv2","title":"ModuleNotFoundError: No module named \\'cv2\\'","text":""},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#python-3810-default-may-26-2023-140508","title":"Python 3.8.10 (default, May 26 2023, 14:05:08)","text":""},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#gcc-940-on-linux","title":"[GCC 9.4.0] on linux","text":""},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doc/#type-help-copyright-credits-or-license-for-more","title":"Type \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more","text":"<p>information.</p> <p>&gt;&gt;&gt; exit()</p> <p>ex:</p> <pre><code>cd /usr/local/lib/python3.6/site-packages/cv2/python-3.6\n</code></pre> <p># or</p> <pre><code>cd /usr/local/lib/python3.8/site-packages/cv2/python-3.8\n</code></pre> <p># remove the previous cv2.so in case it is still there.</p> <pre><code>sudo rm -rf cv2.so\n</code></pre> <pre><code>mv cv2.cpython-36m-[xxx]-linux-gnu.so cv2.so\n</code></pre> <pre><code>ls cv2.\\*\n</code></pre> <pre><code>cv2.cpython-38-aarch64-linux-gnu.so\n</code></pre> <pre><code>ln -s\n/usr/local/lib/python3.8/site-packages/cv2/python-3.8cv2.cpython-38-aarch64-linux-gnu.so\ncv2.so cv2.so\n</code></pre> <pre><code>sudo cp cv2.cpython-36m-aarch64-linux-gnu.so cv2.so\n</code></pre> <p># or</p> <pre><code>sudo cp cv2.cpython-38-aarch64-linux-gnu.so cv2.so\n</code></pre> <p># Alternatively you could just rename the file</p> <pre><code>sudo mv cv2.cpython-36m-aarch64-linux-gnu.so cv2.so\n</code></pre> <pre><code>sudo cp cv2.cpython-38-aarch64-linux-gnu.so cv2.so\n</code></pre> <p># Lets create a link at the home directory for cv2</p> <pre><code>cd \\~\n</code></pre> <pre><code>sudo rm -rf cv2.so\n</code></pre> <pre><code>ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so\ncv2.so\n</code></pre> <p># or</p> <pre><code>ln -s /usr/local/lib/python3.8/site-packages/cv2/python-3.8/cv2.so\ncv2.so\n</code></pre> <p>Checking install for python3</p> <pre><code>python3\n\nimport cv2\n\ncv2.\\_\\_version\\_\\_\n\nexit ()\n</code></pre> <p># Resulting on something similar to this</p> <p>jetson@ucsdrobocar-xxx-yy:\\~\\$ python3</p> <p>Python 3.6.9 (default, Jun 29 2022, 11:45:57)</p> <p>[GCC 8.4.0] on linux</p> <p>Type \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more information.</p> <p>&gt;&gt;&gt; import cv2</p> <p>&gt;&gt;&gt; cv2.__version__</p> <p>\\'4.6.0\\'</p> <p>&gt;&gt;&gt; exit()</p> <p>jetson@ucsdrobocar-xxx-yy:\\~\\$</p> <p>Python 3.8.10 (default, May 26 2023, 14:05:08)</p> <p>[GCC 9.4.0] on linux</p> <p>Type \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more information.</p> <p>&gt;&gt;&gt; import cv2</p> <p>&gt;&gt;&gt; cv2.__version__</p> <p>\\'4.8.0\\'</p> <p>&gt;&gt;&gt; exit ()</p> <p># Here is quicker way to test cv2 on Python 3</p> <pre><code>python3 -c \\'import cv2 as cv; print(cv.\\_\\_version\\_\\_)\\'\n</code></pre> <p>4.6.0</p> <p># or</p> <p>4.8.0</p> <p>end of Installing an Open Source Computer Vision (OpenCV) package with CUDA Support</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doccommonmark/","title":"60doccommonmark","text":"<p>Version 1.8 - 26Nov2022</p> <p>Prepared by</p> <p>Dr. Jack Silberman</p> <p>Department of Electrical and Computer Engineering</p> <p>and</p> <p>Dominic Nightingale</p> <p>Department of Mechanical and Aerospace Engineering</p> <p>University of California, San Diego</p> <p>9500 Gilman Dr, La Jolla, CA 92093</p> <p></p> <p></p> <p></p> <p># Installing an Open Source Computer Vision (OpenCV) package with CUDA Support</p> <p># As of Jan 2020, NVIDIA is not providing OpenCV optimized to use CUDA (GPU acceleration).</p> <p># Search the web if you are curious why.</p> <p># https://forums.developer.nvidia.com/t/opencv-cuda-python-with-jetson-nano/72902</p> <p>\u201c</p> <p>Hi,</p> <p>Replied your question inline:</p> <p>Jetson Nano GPU does not support OpenCV acceleration (through opencl) with Python</p> <p>Our default OpenCV does support GPU acceleration.</p> <p>The common issue is there are some features that have not been enabled.</p> <p>(This feature often requires third-party library installation)</p> <p>\u201c</p> <p>https://www.youtube.com/watch?v=art0-99fFa8</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doccommonmark/#checking-opencv-build-information","title":"Checking openCV build information","text":"<p># ssh to the Single Board Computer (SBC)</p> <p># Check to see if OpenCV for CUDA is available, search on the terminal output for CUDA),</p> <p># if not, we build OpenCV from source to use CUDA Acceleration</p> <p># From a terminal:</p> <p>python</p> <p>&gt;&gt;import cv2</p> <p>&gt;&gt;print cv2.getBuildInformation()</p> <p>&gt;&gt;exit ()</p> <p># ex:</p> <p>NVIDIA CUDA: YES (ver 10.2, CUFFT CUBLAS FAST_MATH)</p> <p>NVIDIA GPU arch: 53 62 72</p> <p>NVIDIA PTX archs:</p> <p>cuDNN: YES (ver 8.0)</p> <p>OpenCL: YES (no extra features)</p> <p>Include path: /tmp/build_opencv/opencv/3rdparty/include/opencl/1.2</p> <p>Link libraries: Dynamic load</p> <p>You can also use jtop to check if OpenCV was compiled with CUDA.</p> <p>jtop</p> <p>6</p> <p></p> <p># If OpenCV is not using CUDA acceleration, let's build and install the latest OpenCV optimized</p> <p># for CUDA</p> <p># As of 03Apr2022, I have not seen NVIDIA supplying OpenCV GPU accelerated on their</p> <p># JetPack. We need to compile it from source. If you ask me, nice experience for you.</p> <p># Let\u2019s do this</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doccommonmark/#_1","title":"60doccommonmark","text":""},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doccommonmark/#installing-opencv-with-cuda","title":"Installing openCV with CUDA","text":"<p># This step may take several hours to compile and build OpenCV in a SBC such as the</p> <p># Jetson Nano (JTN) or Jetson Xavier NX (JNX). It took my JTN four hours to complete.</p> <p># For the JTN, please make sure the fan is installed including software to enable high</p> <p># CPU power mode</p> <p># Better do it overnight. Be patient, please keep in mind that you are using a low power</p> <p># single board computer (SBC)</p> <p># Make sure you are using the external power supply to power the jetson, not a USB cable.</p> <p># https://devtalk.nvidia.com/default/topic/1054949/process-to-install-opencv-4-1-on-nano/</p> <p># https://github.com/mdegans/nano_build_opencv</p> <p># Please make sure your computer will not sleep so you maintain an active ssh connection</p> <p># while you are building OpenCV because you will need to type the user jetson password</p> <p># again to complete the install.</p> <p># Reconnecting on the same ssh session is not trivial</p> <p></p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doccommonmark/#using-a-linux-host-use-screen","title":"# Using a Linux Host? Use Screen","text":"<p># You should consider using screen to enable you to reconnect to a previous ssh session.</p> <p># Do you need to install screen?</p> <p>sudo apt-get install screen</p> <p># From a terminal on Linux host or on your VM:</p> <p>screen</p> <p># ssh jetson@ name of IP address of your machine</p> <p>ssh jetson@ucsdrobocar-xxx-yy.local</p> <p># If you get disconnected, open a terminal again on your computer then type</p> <p>screen -r</p> <p># screen -r should reconnect to the previous ssh session that was open</p> <p># Here is some more detailed information on using screen</p> <p>https://www.howtogeek.com/howto/ubuntu/keep-your-ssh-session-running-when-you-disconnect</p> <p># if not installed in your host</p> <p>sudo apt-get install screen</p> <p>Now you can start a new screen session by just typing screen at the command line. You\u2019ll be shown some information about screen. Hit enter, and you\u2019ll be at a normal prompt.</p> <p>To disconnect (but leave the session running)</p> <p>Hit Ctrl + A and then Ctrl + D in immediate succession. You will see the message [detached]</p> <p>To reconnect to an already running session</p> <p>screen -r</p> <p>To reconnect to an existing session, or create a new one if none exists</p> <p>screen -D -r</p> <p># More info on using screen</p> <p>https://ma.ttias.be/screen-a-must-for-ssh/</p> <p>screen -ls</p> <p>There are screens on:</p> <p>27111.screen_untarring (Detached)</p> <p>27097.screen_disking (Detached)</p> <p>2 Sockets in /var/run/screen/S-root.</p> <p>This will show a list of all running screen-sessions at any given time. You can pick up a previous screen-session, by typing</p> <p>screen -r \\&lt;name_of_session&gt;</p> <p>screen -r</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doccommonmark/#_2","title":"60doccommonmark","text":"<p># Since your SBC will be busy, how can you see it is working for you?</p> <p>#</p> <p># Besides htop or top, there is another cool utility called jetson_stats</p> <p>sudo apt-get install python3-pip</p> <p>sudo -H pip3 install -U jetson-stats</p> <p># reboot</p> <p>sudo reboot now</p> <p># Then you can run</p> <p>jtop</p> <p>4</p> <p>You can use jtop to add more swap space using the left and right keys and clicking the plus button</p> <p></p> <p>Add 4G of swap and press \\&lt;S&gt; to enable it.</p> <p></p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doccommonmark/#removing-old-version-of-opencv","title":"Removing old version of OpenCV","text":"<p># First lets remove the current OpenCV version and reboot the JTN</p> <p>sudo sudo apt-get purge *libopencv*</p> <p>sudo reboot now</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doccommonmark/#compiling-opencv-from-source","title":"Compiling OpenCV from source","text":"<p># screen then ssh back to the SBC</p> <p># If needed, create the ~/projects directory</p> <p>cd~</p> <p>mkdir projects</p> <p>cd projects</p> <p># For Jetson Nano</p> <p>git clone https://github.com/mdegans/nano_build_opencv.git</p> <p>cd nano_build_opencv</p> <p># For the Jetson Nano, lets modify the script to use 2 CPU cores vs. 1.</p> <p># Note: for the TX2 and Xaviers the script automatically uses more CPU cores.</p> <p># Need to modify the configuration for the CUDNN we are using</p> <p># First see what you have installed using jtop use option INFO press 7</p> <p></p> <p></p> <p>#Remove the 8.7 from the CUDA_ARCH_BIN version, it causes a compatibility issue</p> <p>#Also, make sure the CUDNN_VERSION=\u20198.0\u2019, it will fail otherwise</p> <p>nano build_opencv.sh</p> <p># modify this line to have JOBS=2</p> <p>else</p> <p>JOBS=1 # you can set this to 4 if you have a swap file</p> <p>JOBS=2</p> <p># save the file / exit nano</p> <p># https://opencv.org/releases/</p> <p># look for the latest version</p> <p></p> <p># As of 24Nov23, 4.8.0 is the latest version</p> <p># You can use jtop to have Jetson Clocks enabled to improve the performance of the Jetson</p> <p># 6 CTRL and then press s</p> <p># To make it enabled at boot too, press e</p> <p></p> <p># From a terminal</p> <p>./build_opencv.sh 4.8.0</p> <p># Again, this will take a good while. You may consider doing this at night before you go to bed. It is really boring</p> <p># after a while (hours) looking that the SBC compile and build OpenCV from source</p> <p># Please make sure the computer you are using to SSH, disable sleep, and if using a notebook computer</p> <p># have in connected to the charger</p> <p># Preferable you use screen before ssh to be able to reconnect in</p> <p># case your computer lose the SSH connection.</p> <p># If you wish, you can see that the SBC cores of the CPU are</p> <p># busy by opening another terminal or a new tab on the same terminal window then</p> <p># ssh to the SBC and run the command top</p> <p># I like better jtop (if needed, install htop or jtop).</p> <p>#</p> <p># ex:</p> <p></p> <p># After a few hours you see that you need to enter a little information to finish the build. That is when we have</p> <p># problems if you get disconnected from your SSH</p> <p># You can leave the temporary builds in place</p> <p>Do you wish to remove temporary build files in /tmp/build_opencv ?</p> <p>(Doing so may make running tests on the build later impossible)</p> <p>Y/N y</p> <p># Enter y</p> <p># save some space on disc</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doccommonmark/#checking-the-install","title":"Checking the Install","text":"<p># When the installation is complete, let's check the version of OpenCV that was installed</p> <p># First reboot SBC</p> <p>sudo reboot now</p> <p># ssh back to the SBC</p> <p>#Using jtop to confirm the OpenCV GPU (CUDA) compiled</p> <p>jtop</p> <p>7 INFO</p> <p># look for the OpenCV version and that it was compiled with CUDA</p> <p></p> <p></p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60doccommonmark/#lets-make-sure-python3-finds-opencv-cv2","title":"# Let\u2019s make sure Python3 finds OpenCV (CV2)","text":"<p># Need to adapt this for the python and OpenCV versions that you are using</p> <p>python3 -c 'import cv2 as cv; print(cv.__version__)'</p> <p>#Traceback (most recent call last):</p> <p>#File \"\\&lt;string&gt;\", line 1, in \\&lt;module&gt;</p> <p>#ModuleNotFoundError: No module named 'cv2'</p> <p># If you don\u2019t see the version of the OpenCV that you built listed, it is because you forgot to delete the previous</p> <p># OpenCV version that was installed, go back to this document \u201cRemoving old version of OpenCV\u201d and then</p> <p># build and install OpenCV again.</p> <p># What version of python3 do you have?</p> <p>python3</p> <p>#Python 3.8.10 (default, May 26 2023, 14:05:08)</p> <p>#[GCC 9.4.0] on linux</p> <p>#Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p> <p>&gt;&gt;&gt; exit()</p> <p>ex:</p> <p>cd /usr/local/lib/python3.6/site-packages/cv2/python-3.6</p> <p># or</p> <p>cd /usr/local/lib/python3.8/site-packages/cv2/python-3.8</p> <p># remove the previous cv2.so in case it is still there.</p> <p>sudo rm -rf cv2.so</p> <p># mv cv2.cpython-36m-xxx-linux-gnu.so cv2.so</p> <p>ls cv2.*</p> <p># cv2.cpython-38-aarch64-linux-gnu.so</p> <p>ln -s /usr/local/lib/python3.8/site-packages/cv2/python-3.8cv2.cpython-38-aarch64-linux-gnu.so cv2.so cv2.so</p> <p>sudo cp cv2.cpython-36m-aarch64-linux-gnu.so cv2.so</p> <p># or</p> <p>sudo cp cv2.cpython-38-aarch64-linux-gnu.so cv2.so</p> <p># Alternatively you could just rename the file</p> <p># sudo mv cv2.cpython-36m-aarch64-linux-gnu.so cv2.so</p> <p># sudo cp cv2.cpython-38-aarch64-linux-gnu.so cv2.so</p> <p># Lets create a link at the home directory for cv2</p> <p>cd ~</p> <p>sudo rm -rf cv2.so</p> <p>ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so cv2.so</p> <p># or</p> <p>ln -s /usr/local/lib/python3.8/site-packages/cv2/python-3.8/cv2.so cv2.so</p> <p>Checking install for python3</p> <p>python3</p> <p>import cv2</p> <p>cv2.__version__</p> <p>exit ()</p> <p># Resulting on something similar to this</p> <p>jetson@ucsdrobocar-xxx-yy:~$ python3</p> <p>Python 3.6.9 (default, Jun 29 2022, 11:45:57)</p> <p>[GCC 8.4.0] on linux</p> <p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p> <p>&gt;&gt;&gt; import cv2</p> <p>&gt;&gt;&gt; cv2.__version__</p> <p>'4.6.0'</p> <p>&gt;&gt;&gt; exit()</p> <p>jetson@ucsdrobocar-xxx-yy:~$</p> <p>Python 3.8.10 (default, May 26 2023, 14:05:08)</p> <p>[GCC 9.4.0] on linux</p> <p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p> <p>&gt;&gt;&gt; import cv2</p> <p>&gt;&gt;&gt; cv2.__version__</p> <p>'4.8.0'</p> <p>&gt;&gt;&gt; exit ()</p> <p># Here is quicker way to test cv2 on Python 3</p> <p>python3 -c 'import cv2 as cv; print(cv.__version__)'</p> <p>4.6.0</p> <p># or</p> <p>4.8.0</p> <p>end of Installing an Open Source Computer Vision (OpenCV) package with CUDA Support</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60docfinal/","title":"60docfinal","text":"<p>Version 1.8 - 26Nov2022</p> <p>Prepared by</p> <p>Dr. Jack Silberman</p> <p>Department of Electrical and Computer Engineering</p> <p>and</p> <p>Dominic Nightingale</p> <p>Department of Mechanical and Aerospace Engineering</p> <p>University of California, San Diego</p> <p>9500 Gilman Dr, La Jolla, CA 92093</p> <p></p> <p></p> <p></p> <p># Installing an Open Source Computer Vision (OpenCV) package with CUDA Support</p> <p># As of Jan 2020, NVIDIA is not providing OpenCV optimized to use CUDA (GPU acceleration).</p> <p># Search the web if you are curious why.</p> <p># https://forums.developer.nvidia.com/t/opencv-cuda-python-with-jetson-nano/72902</p> <p>\u201c</p> <p>Hi,</p> <p>Replied your question inline:</p> <p>Jetson Nano GPU does not support OpenCV acceleration (through opencl) with Python</p> <p>Our default OpenCV does support GPU acceleration.</p> <p>The common issue is there are some features that have not been enabled.</p> <p>(This feature often requires third-party library installation)</p> <p>\u201c</p> <p>https://www.youtube.com/watch?v=art0-99fFa8</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60docfinal/#checking-opencv-build-information","title":"Checking openCV build information","text":"<p># ssh to the Single Board Computer (SBC)</p> <p># Check to see if OpenCV for CUDA is available, search on the terminal output for CUDA),</p> <p># if not, we build OpenCV from source to use CUDA Acceleration</p> <p># From a terminal:</p> <p>python</p> <p>&gt;&gt;import cv2</p> <p>&gt;&gt;print cv2.getBuildInformation()</p> <p>&gt;&gt;exit ()</p> <p># ex:</p> <p>NVIDIA CUDA: YES (ver 10.2, CUFFT CUBLAS FAST_MATH)</p> <p>NVIDIA GPU arch: 53 62 72</p> <p>NVIDIA PTX archs:</p> <p>cuDNN: YES (ver 8.0)</p> <p>OpenCL: YES (no extra features)</p> <p>Include path: /tmp/build_opencv/opencv/3rdparty/include/opencl/1.2</p> <p>Link libraries: Dynamic load</p> <p>You can also use jtop to check if OpenCV was compiled with CUDA.</p> <p>jtop</p> <p>6</p> <p></p> <p># If OpenCV is not using CUDA acceleration, let's build and install the latest OpenCV optimized</p> <p># for CUDA</p> <p># As of 03Apr2022, I have not seen NVIDIA supplying OpenCV GPU accelerated on their</p> <p># JetPack. We need to compile it from source. If you ask me, nice experience for you.</p> <p># Let\u2019s do this</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60docfinal/#_1","title":"60docfinal","text":""},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60docfinal/#installing-opencv-with-cuda","title":"Installing openCV with CUDA","text":"<p># This step may take several hours to compile and build OpenCV in a SBC such as the</p> <p># Jetson Nano (JTN) or Jetson Xavier NX (JNX). It took my JTN four hours to complete.</p> <p># For the JTN, please make sure the fan is installed including software to enable high</p> <p># CPU power mode</p> <p># Better do it overnight. Be patient, please keep in mind that you are using a low power</p> <p># single board computer (SBC)</p> <p># Make sure you are using the external power supply to power the jetson, not a USB cable.</p> <p># https://devtalk.nvidia.com/default/topic/1054949/process-to-install-opencv-4-1-on-nano/</p> <p># https://github.com/mdegans/nano_build_opencv</p> <p># Please make sure your computer will not sleep so you maintain an active ssh connection</p> <p># while you are building OpenCV because you will need to type the user jetson password</p> <p># again to complete the install.</p> <p># Reconnecting on the same ssh session is not trivial</p> <p></p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60docfinal/#using-a-linux-host-use-screen","title":"# Using a Linux Host? Use Screen","text":"<p># You should consider using screen to enable you to reconnect to a previous ssh session.</p> <p># Do you need to install screen?</p> <p>sudo apt-get install screen</p> <p># From a terminal on Linux host or on your VM:</p> <p>screen</p> <p># ssh jetson@ name of IP address of your machine</p> <p>ssh jetson@ucsdrobocar-xxx-yy.local</p> <p># If you get disconnected, open a terminal again on your computer then type</p> <p>screen -r</p> <p># screen -r should reconnect to the previous ssh session that was open</p> <p># Here is some more detailed information on using screen</p> <p>https://www.howtogeek.com/howto/ubuntu/keep-your-ssh-session-running-when-you-disconnect</p> <p># if not installed in your host</p> <p>sudo apt-get install screen</p> <p>Now you can start a new screen session by just typing screen at the command line. You\u2019ll be shown some information about screen. Hit enter, and you\u2019ll be at a normal prompt.</p> <p>To disconnect (but leave the session running)</p> <p>Hit Ctrl + A and then Ctrl + D in immediate succession. You will see the message [detached]</p> <p>To reconnect to an already running session</p> <p>screen -r</p> <p>To reconnect to an existing session, or create a new one if none exists</p> <p>screen -D -r</p> <p># More info on using screen</p> <p>https://ma.ttias.be/screen-a-must-for-ssh/</p> <p>screen -ls</p> <p>There are screens on:</p> <p>27111.screen_untarring (Detached)</p> <p>27097.screen_disking (Detached)</p> <p>2 Sockets in /var/run/screen/S-root.</p> <p>This will show a list of all running screen-sessions at any given time. You can pick up a previous screen-session, by typing</p> <p>screen -r \\&lt;name_of_session&gt;</p> <p>screen -r</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60docfinal/#_2","title":"60docfinal","text":"<p># Since your SBC will be busy, how can you see it is working for you?</p> <p>#</p> <p># Besides htop or top, there is another cool utility called jetson_stats</p> <p>sudo apt-get install python3-pip</p> <p>sudo -H pip3 install -U jetson-stats</p> <p># reboot</p> <p>sudo reboot now</p> <p># Then you can run</p> <p>jtop</p> <p>4</p> <p>You can use jtop to add more swap space using the left and right keys and clicking the plus button</p> <p></p> <p>Add 4G of swap and press \\&lt;S&gt; to enable it.</p> <p></p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60docfinal/#removing-old-version-of-opencv","title":"Removing old version of OpenCV","text":"<p># First lets remove the current OpenCV version and reboot the JTN</p> <p>sudo sudo apt-get purge *libopencv*</p> <p>sudo reboot now</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60docfinal/#compiling-opencv-from-source","title":"Compiling OpenCV from source","text":"<p># screen then ssh back to the SBC</p> <p># If needed, create the ~/projects directory</p> <p>cd~</p> <p>mkdir projects</p> <p>cd projects</p> <p># For Jetson Nano</p> <p>git clone https://github.com/mdegans/nano_build_opencv.git</p> <p>cd nano_build_opencv</p> <p># For the Jetson Nano, lets modify the script to use 2 CPU cores vs. 1.</p> <p># Note: for the TX2 and Xaviers the script automatically uses more CPU cores.</p> <p># Need to modify the configuration for the CUDNN we are using</p> <p># First see what you have installed using jtop use option INFO press 7</p> <p></p> <p></p> <p>#Remove the 8.7 from the CUDA_ARCH_BIN version, it causes a compatibility issue</p> <p>#Also, make sure the CUDNN_VERSION=\u20198.0\u2019, it will fail otherwise</p> <p>nano build_opencv.sh</p> <p># modify this line to have JOBS=2</p> <p>else</p> <p>JOBS=1 # you can set this to 4 if you have a swap file</p> <p>JOBS=2</p> <p># save the file / exit nano</p> <p># https://opencv.org/releases/</p> <p># look for the latest version</p> <p></p> <p># As of 24Nov23, 4.8.0 is the latest version</p> <p># You can use jtop to have Jetson Clocks enabled to improve the performance of the Jetson</p> <p># 6 CTRL and then press s</p> <p># To make it enabled at boot too, press e</p> <p></p> <p># From a terminal</p> <p>./build_opencv.sh 4.8.0</p> <p># Again, this will take a good while. You may consider doing this at night before you go to bed. It is really boring</p> <p># after a while (hours) looking that the SBC compile and build OpenCV from source</p> <p># Please make sure the computer you are using to SSH, disable sleep, and if using a notebook computer</p> <p># have in connected to the charger</p> <p># Preferable you use screen before ssh to be able to reconnect in</p> <p># case your computer lose the SSH connection.</p> <p># If you wish, you can see that the SBC cores of the CPU are</p> <p># busy by opening another terminal or a new tab on the same terminal window then</p> <p># ssh to the SBC and run the command top</p> <p># I like better jtop (if needed, install htop or jtop).</p> <p>#</p> <p># ex:</p> <p></p> <p># After a few hours you see that you need to enter a little information to finish the build. That is when we have</p> <p># problems if you get disconnected from your SSH</p> <p># You can leave the temporary builds in place</p> <p>Do you wish to remove temporary build files in /tmp/build_opencv ?</p> <p>(Doing so may make running tests on the build later impossible)</p> <p>Y/N y</p> <p># Enter y</p> <p># save some space on disc</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60docfinal/#checking-the-install","title":"Checking the Install","text":"<p># When the installation is complete, let's check the version of OpenCV that was installed</p> <p># First reboot SBC</p> <p>sudo reboot now</p> <p># ssh back to the SBC</p> <p>#Using jtop to confirm the OpenCV GPU (CUDA) compiled</p> <p>jtop</p> <p>7 INFO</p> <p># look for the OpenCV version and that it was compiled with CUDA</p> <p></p> <p></p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/60docfinal/#lets-make-sure-python3-finds-opencv-cv2","title":"# Let\u2019s make sure Python3 finds OpenCV (CV2)","text":"<p># Need to adapt this for the python and OpenCV versions that you are using</p> <p>python3 -c 'import cv2 as cv; print(cv.__version__)'</p> <p>#Traceback (most recent call last):</p> <p>#File \"\\&lt;string&gt;\", line 1, in \\&lt;module&gt;</p> <p>#ModuleNotFoundError: No module named 'cv2'</p> <p># If you don\u2019t see the version of the OpenCV that you built listed, it is because you forgot to delete the previous</p> <p># OpenCV version that was installed, go back to this document \u201cRemoving old version of OpenCV\u201d and then</p> <p># build and install OpenCV again.</p> <p># What version of python3 do you have?</p> <p>python3</p> <p>#Python 3.8.10 (default, May 26 2023, 14:05:08)</p> <p>#[GCC 9.4.0] on linux</p> <p>#Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p> <p>&gt;&gt;&gt; exit()</p> <p>ex:</p> <p>cd /usr/local/lib/python3.6/site-packages/cv2/python-3.6</p> <p># or</p> <p>cd /usr/local/lib/python3.8/site-packages/cv2/python-3.8</p> <p># remove the previous cv2.so in case it is still there.</p> <p>sudo rm -rf cv2.so</p> <p># mv cv2.cpython-36m-xxx-linux-gnu.so cv2.so</p> <p>ls cv2.*</p> <p># cv2.cpython-38-aarch64-linux-gnu.so</p> <p>ln -s /usr/local/lib/python3.8/site-packages/cv2/python-3.8cv2.cpython-38-aarch64-linux-gnu.so cv2.so cv2.so</p> <p>sudo cp cv2.cpython-36m-aarch64-linux-gnu.so cv2.so</p> <p># or</p> <p>sudo cp cv2.cpython-38-aarch64-linux-gnu.so cv2.so</p> <p># Alternatively you could just rename the file</p> <p># sudo mv cv2.cpython-36m-aarch64-linux-gnu.so cv2.so</p> <p># sudo cp cv2.cpython-38-aarch64-linux-gnu.so cv2.so</p> <p># Lets create a link at the home directory for cv2</p> <p>cd ~</p> <p>sudo rm -rf cv2.so</p> <p>ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so cv2.so</p> <p># or</p> <p>ln -s /usr/local/lib/python3.8/site-packages/cv2/python-3.8/cv2.so cv2.so</p> <p>Checking install for python3</p> <p>python3</p> <p>import cv2</p> <p>cv2.__version__</p> <p>exit ()</p> <p># Resulting on something similar to this</p> <p>jetson@ucsdrobocar-xxx-yy:~\\$ python3</p> <p>Python 3.6.9 (default, Jun 29 2022, 11:45:57)</p> <p>[GCC 8.4.0] on linux</p> <p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p> <p>&gt;&gt;&gt; import cv2</p> <p>&gt;&gt;&gt; cv2.__version__</p> <p>'4.6.0'</p> <p>&gt;&gt;&gt; exit()</p> <p>jetson@ucsdrobocar-xxx-yy:~\\$</p> <p>Python 3.8.10 (default, May 26 2023, 14:05:08)</p> <p>[GCC 9.4.0] on linux</p> <p>Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.</p> <p>&gt;&gt;&gt; import cv2</p> <p>&gt;&gt;&gt; cv2.__version__</p> <p>'4.8.0'</p> <p>&gt;&gt;&gt; exit ()</p> <p># Here is quicker way to test cv2 on Python 3</p> <p>python3 -c 'import cv2 as cv; print(cv.__version__)'</p> <p>4.6.0</p> <p># or</p> <p>4.8.0</p> <p>end of Installing an Open Source Computer Vision (OpenCV) package with CUDA Support</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/output/","title":"Output","text":"[OpenCV]{.c21}\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  ------------------------------------------------------------------------  []{.c2}  []{.c2}   <p>[UCSD RoboCar OpenCV CUDA Accelerated]{.c21 .c28}</p> <p>[]{.c2}</p> <p>[Version 1.8 - 26Nov2022]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[Prepared by]{.c2}</p> <p>[Dr. Jack Silberman]{.c2}</p> <p>[Department of Electrical and Computer Engineering]{.c2}</p> <p>[and ]{.c2}</p> <p>[Dominic Nightingale]{.c2}</p> <p>[Department of Mechanical and Aerospace Engineering]{.c2}</p> <p>[University of California, San Diego]{.c2}</p> <p>[9500 Gilman Dr, La Jolla, CA 92093]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[{style=\"width: 300.61px; height: 71.02px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 300.61px; height: 71.02px;\"}</p> <p>[{style=\"width: 134.50px; height: 134.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 134.50px; height: 134.50px;\"}</p> <p>[{style=\"width: 302.87px; height: 64.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 302.87px; height: 64.50px;\"}</p> <p>[\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0]{.c2}</p> <p>[]{.c2}</p> <p>[# Installing an Open Source Computer Vision (OpenCV) package \u00a0with CUDA Support]{.c2}</p> <p>[# As of Jan 2020, NVIDIA is not providing OpenCV optimized to use CUDA (GPU acceleration).]{.c2}</p> <p>[# Search the web if you are curious why.]{.c2}</p> <p># [https://forums.developer.nvidia.com/t/opencv-cuda-python-with-jetson-nano/72902{.c11}]{.c13}</p> <p>[]{.c2}</p> <p>[\"]{.c2}</p> <p>[Hi,]{.c12}</p> <p>[Replied your question inline:]{.c12}</p> <p>[Jetson Nano GPU does not support OpenCV acceleration (through opencl) with Python]{.c12}</p> <p>[Our default OpenCV does support GPU acceleration.]{.c12}</p> <p>[The common issue is there are some features that have not been enabled.]{.c12}</p> <p>[(This feature often requires third-party library installation)]{.c25}</p> <p>[\"]{.c2}</p> <p>[https://www.youtube.com/watch?v=art0-99fFa8{.c11}]{.c13}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/output/#checking-opencv-build-informationc14-h3boutaiszk8f-c16","title":"[Checking openCV build information]{.c14} {#h.3boutaiszk8f .c16}","text":"<p>[# ssh to the Single Board Computer (SBC)]{.c2}</p> <p>[# Check to see if OpenCV for CUDA is available, search on the terminal output for CUDA),]{.c2}</p> <p>[# if not, \u00a0we build OpenCV from source to use CUDA Acceleration]{.c2}</p> <p>[]{.c2}</p> <p>[# From a terminal:]{.c2}</p> <p>[python]{.c2}</p> <p>[&gt;&gt;import cv2]{.c2}</p> <p>[&gt;&gt;print cv2.getBuildInformation() ]{.c2}</p> <p>[&gt;&gt;exit ()]{.c2}</p> <p>[\u00a0]{.c2}</p> <p>[# ex:]{.c2}</p> <p>[]{.c2}</p> <p>[\u00a0 \u00a0 NVIDIA CUDA: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 YES (ver 10.2, CUFFT CUBLAS FAST_MATH)]{.c2}</p> <p>[\u00a0 \u00a0 NVIDIA GPU arch: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 53 62 72]{.c2}</p> <p>[\u00a0 \u00a0 NVIDIA PTX archs:]{.c2}</p> <p>[]{.c2}</p> <p>[\u00a0 cuDNN: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 YES (ver 8.0)]{.c2}</p> <p>[]{.c2}</p> <p>[\u00a0 OpenCL: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0YES (no extra features)]{.c2}</p> <p>[\u00a0 \u00a0 Include path: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0/tmp/build_opencv/opencv/3rdparty/include/opencl/1.2]{.c2}</p> <p>[\u00a0 \u00a0 Link libraries: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Dynamic load]{.c2}</p> <p>[]{.c2}</p> <p>[You can also use jtop to check if OpenCV was compiled with CUDA.]{.c2}</p> <p>[jtop]{.c2}</p> <p>[6]{.c2}</p> <p>[]{.c2}</p> <p>[{style=\"width: 435.50px; height: 248.73px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 435.50px; height: 248.73px;\"}</p> <p>[]{.c2}</p> <p>[# If OpenCV is not using CUDA acceleration, let\\'s build and install the latest OpenCV optimized]{.c2}</p> <p>[# for CUDA]{.c2}</p> <p>[]{.c2}</p> <p>[# As of 03Apr2022, I have not seen NVIDIA supplying OpenCV GPU accelerated on their]{.c2}</p> <p>[# JetPack. We need to compile it from source. If you ask me, nice experience for you.]{.c2}</p> <p>[# Let's do this]{.c2}</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/output/#c14-hjxhoa7kcd9g4-c16-c24","title":"[]{.c14} {#h.jxhoa7kcd9g4 .c16 .c24}","text":""},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/output/#installing-opencv-with-cudac14-hk7mmblytcair-c16","title":"[Installing openCV with CUDA]{.c14} {#h.k7mmblytcair .c16}","text":"<p>[# This step may take several hours]{.c21}\u00a0[to compile and build OpenCV in a SBC such as the]{.c10}</p> <p>[# Jetson Nano (JTN) or Jetson Xavier NX (JNX). It took my JTN four hours to complete.]{.c10}</p> <p>[# For the JTN, please make sure the fan is installed including software to enable high ]{.c10}</p> <p>[# CPU power mode]{.c10}</p> <p>[]{.c2}</p> <p>[# Better do it overnight. Be patient, please keep in mind that you are using a low power ]{.c2}</p> <p>[# single board computer (SBC)]{.c2}</p> <p>[# Make sure you are using the external power supply to power the jetson, not a USB cable.]{.c2}</p> <p>[]{.c2}</p> <p># [https://devtalk.nvidia.com/default/topic/1054949/process-to-install-opencv-4-1-on-nano/{.c11}]{.c13}</p> <p># [https://github.com/mdegans/nano_build_opencv{.c11}]{.c13}</p> <p>[]{.c2}</p> <p>[# Please make sure your computer will not sleep so you maintain an active ssh connection]{.c2}</p> <p>[# while you are building OpenCV because you will need to type the user jetson password ]{.c2}</p> <p>[# again to complete the install. ]{.c2}</p> <p>[# Reconnecting on the same ssh session is not trivial]{.c2}</p> <p>[]{.c2}</p> <p>[{style=\"width: 624.00px; height: 165.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 165.33px;\"}</p> <p>[]{.c2}</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/output/#using-a-linux-host-use-screenc17-hcbsfos7szrlx-c20","title":"[# Using a Linux Host? Use Screen]{.c17} {#h.cbsfos7szrlx .c20}","text":"<p># You should consider using [screen{.c11}]{.c13}[\u00a0to enable you to reconnect to a previous ssh session. ]{.c2}</p> <p>[# Do you need to install screen?]{.c2}</p> <p>[]{.c2}</p> <p>[sudo apt-get install screen]{.c2}</p> <p>[]{.c2}</p> <p>[# From a terminal on Linux host or on your VM:]{.c2}</p> <p>[screen]{.c2}</p> <p>[# ssh jetson@ name of IP address of your machine]{.c2}</p> <p>[ssh jetson@ucsdrobocar-xxx-yy.local]{.c2}</p> <p>[]{.c2}</p> <p>[# If you get disconnected, open a terminal again on your computer then type]{.c2}</p> <p>[screen -r]{.c2}</p> <p>[]{.c2}</p> <p>[# screen -r should reconnect to the previous ssh session that was open]{.c2}</p> <p>[]{.c2}</p> <p>[# Here is some more detailed information on using screen]{.c2}</p> <p>[https://www.howtogeek.com/howto/ubuntu/keep-your-ssh-session-running-when-you-disconnect{.c11}]{.c13}</p> <p>[# if not installed in your host]{.c6}</p> <p>[sudo apt-get install screen]{.c6}</p> <p>[]{.c6}</p> <p>[Now you can start a new screen session by just typing screen at the command line. You'll be shown some information about screen. Hit enter, and you'll be at a normal prompt.]{.c6}</p> <p>[]{.c6}</p> <p>[To disconnect (but leave the session running)]{.c6}</p> <p>[]{.c6}</p> <p>[Hit Ctrl + A and then Ctrl + D in immediate succession. You will see the message [detached]]{.c6}</p> <p>[]{.c6}</p> <p>[To reconnect to an already running session]{.c6}</p> <p>[]{.c6}</p> <p>[screen -r]{.c6}</p> <p>[]{.c6}</p> <p>[To reconnect to an existing session, or create a new one if none exists]{.c6}</p> <p>[]{.c6}</p> <p>[screen -D -r]{.c6}</p> <p>[]{.c2}</p> <p>[# More info on using screen]{.c2}</p> <p>[https://ma.ttias.be/screen-a-must-for-ssh/{.c11}]{.c13}</p> <p>[screen -ls]{.c6}</p> <p>[There are screens on:]{.c6}</p> <p>[27111.screen_untarring \u00a0(Detached)]{.c6}</p> <p>[27097.screen_disking \u00a0 \u00a0(Detached)]{.c6}</p> <p>[2 Sockets in /var/run/screen/S-root.]{.c6}</p> <p>[This will show a list of all running screen-sessions at any given time. You can pick up a previous screen-session, by typing]{.c6}</p> <p>[]{.c6}</p> <p>[screen -r \\&lt;name_of_session&gt;]{.c6}</p> <p>[screen -r]{.c30}</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/output/#c17-hzdkduk3j5481-c19","title":"[]{.c17} {#h.zdkduk3j5481 .c19}","text":"<p>[# Since your SBC will be busy, how can you see it is working for you?]{.c2}</p> <p>[#]{.c2}</p> <p># Besides htop or top, there is another cool utility called [jetson_stats{.c11}]{.c13}</p> <p>[sudo apt-get install python3-pip]{.c2}</p> <p>[sudo -H pip3 install -U jetson-stats]{.c2}</p> <p>[]{.c2}</p> <p>[# reboot]{.c2}</p> <p>[sudo reboot now]{.c2}</p> <p>[# Then you can run]{.c2}</p> <p>[jtop]{.c2}</p> <p>[4]{.c2}</p> <p>[]{.c2}</p> <p>[You can use jtop to add more swap space using the left and right keys and clicking the plus button]{.c2}</p> <p>[{style=\"width: 384.50px; height: 170.58px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 384.50px; height: 170.58px;\"}</p> <p>[]{.c2}</p> <p>[Add 4G of swap and press \\&lt;S&gt; to enable it.]{.c2}</p> <p>[]{.c2}</p> <p>[{style=\"width: 289.50px; height: 207.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 289.50px; height: 207.53px;\"}</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/output/#removing-old-version-of-opencv-c17-hqbolrby1hrw-c20","title":"[Removing old version of OpenCV ]{.c17} {#h.qbolrby1hrw .c20}","text":"<p>[# First lets remove the current OpenCV version and reboot the JTN]{.c2}</p> <p>[]{.c2}</p> <p>[sudo sudo apt-get purge *libopencv*]{.c2}</p> <p>[sudo reboot now]{.c2}</p> <p>[]{.c2}</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/output/#compiling-opencv-from-sourcec17-hdqmd5h5g4njr-c20","title":"[Compiling OpenCV from source]{.c17} {#h.dqmd5h5g4njr .c20}","text":"<p>[# screen then ssh back to the SBC]{.c2}</p> <p>[# If needed, create the \\~/projects directory]{.c2}</p> <p>[cd\\~]{.c2}</p> <p>[mkdir projects]{.c2}</p> <p>[cd projects]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[# For Jetson Nano ]{.c2}</p> <p>git clone [https://github.com/mdegans/nano_build_opencv.git{.c11}]{.c13}</p> <p>[cd nano_build_opencv]{.c2}</p> <p>[]{.c2}</p> <p>[# For the Jetson Nano, lets modify the script to use 2 CPU cores vs. 1. ]{.c2}</p> <p>[# Note: for the TX2 and Xaviers the script automatically uses more CPU cores.]{.c2}</p> <p>[# Need to modify the configuration for the CUDNN we are using]{.c2}</p> <p>[# First see what you have installed using jtop use option INFO press 7]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[{style=\"width: 412.28px; height: 288.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 412.28px; height: 288.60px;\"}</p> <p>[{style=\"width: 486.57px; height: 309.69px; margin-left: -10.61px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 465.36px; height: 309.69px;\"}</p> <p>[#Remove the 8.7 from the CUDA_ARCH_BIN version, it causes a compatibility issue]{.c2}</p> <p>[#Also, make sure the CUDNN_VERSION='8.0', it will fail otherwise]{.c2}</p> <p>[]{.c2}</p> <p>[nano build_opencv.sh]{.c2}</p> <p>[]{.c2}</p> <p>[# modify this line to have JOBS=2]{.c2}</p> <p>[else]{.c2}</p> <p>[\u00a0 \u00a0 JOBS=1 \u00a0# you can set this to 4 if you have a swap file]{.c2}</p> <p>[]{.c2}</p> <p>[JOBS=2]{.c2}</p> <p>[# save the file / exit nano]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p># [https://opencv.org/releases/{.c11}]{.c13}</p> <p>[# look for the latest version]{.c2}</p> <p>[]{.c2}</p> <p>[{style=\"width: 365.09px; height: 255.06px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 365.09px; height: 255.06px;\"}</p> <p># As of 24Nov23, [4.8.0]{.c21}[\u00a0is the latest version]{.c2}</p> <p>[]{.c2}</p> <p>[# You can use jtop to have Jetson Clocks enabled to improve the performance of the Jetson]{.c2}</p> <p>[# 6 CTRL \u00a0and then press s]{.c2}</p> <p>[# To make it enabled at boot too, press e]{.c2}</p> <p>[]{.c2}</p> <p>[{style=\"width: 380.73px; height: 281.31px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 380.73px; height: 281.31px;\"}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[# From a terminal]{.c2}</p> <p>./build_opencv.sh [4.8.0]{.c21}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p># Again, this will take a good while.[\u00a0You may consider doing this at night before you go to bed. It is really boring]{.c2}</p> <p>[# after a while (hours) looking that the SBC compile and build OpenCV from source]{.c2}</p> <p>[# \u00a0Please make sure the computer you are using to SSH, disable sleep, and if using a notebook computer ]{.c2}</p> <p>[# have in connected to the charger ]{.c2}</p> <p>[# Preferable you use screen before ssh to be able to reconnect in ]{.c2}</p> <p>[# case your computer lose the SSH connection.]{.c2}</p> <p>[# \u00a0If you wish, you can see that the SBC cores of the CPU are]{.c2}</p> <p>[# \u00a0busy by opening another terminal or a new tab on the same terminal window then]{.c2}</p> <p>[# \u00a0ssh to the SBC and run the command top]{.c2}</p> <p>[# \u00a0I like better jtop (if needed, install htop or jtop).]{.c2}</p> <p>[#]{.c2}</p> <p>[# ex:]{.c2}</p> <p>[{style=\"width: 378.50px; height: 248.09px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 378.50px; height: 248.09px;\"}</p> <p>[]{.c2}</p> <p>[# After a few hours you see that you need to enter a little information to finish the build. That is when we have]{.c2}</p> <p>[# problems if you get disconnected from your SSH]{.c2}</p> <p>[]{.c2}</p> <p>[# \u00a0You can leave the temporary builds in place]{.c2}</p> <p>[Do you wish to remove temporary build files in /tmp/build_opencv ? ]{.c2}</p> <p>[(Doing so may make running tests on the build later impossible)]{.c2}</p> <p>Y/N[\u00a0y]{.c10}</p> <p>[# Enter y]{.c10}</p> <p>[# save some space on disc ]{.c2}</p> <p>[]{.c10}</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/output/#checking-the-installc14-h8nyvs9xjvw3u-c16","title":"[Checking the Install]{.c14} {#h.8nyvs9xjvw3u .c16}","text":"<p>[# When the installation is complete, let\\'s check the version of OpenCV that was installed]{.c2}</p> <p>[\u00a0 \u00a0]{.c2}</p> <p>[# First reboot SBC]{.c2}</p> <p>[sudo reboot now]{.c2}</p> <p>[]{.c2}</p> <p>[# ssh back to the SBC]{.c2}</p> <p>[]{.c2}</p> <p>[#Using jtop to confirm the OpenCV GPU (CUDA) \u00a0compiled]{.c2}</p> <p>[]{.c2}</p> <p>[jtop]{.c2}</p> <p>[7 INFO]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[# look for the OpenCV version and that it was compiled with CUDA]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[{style=\"width: 442.88px; height: 322.31px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 442.88px; height: 322.31px;\"}</p> <p>[{style=\"width: 720.00px; height: 36.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 720.00px; height: 36.00px;\"}</p> <p>[]{.c2}</p>"},{"location":"markdown-documentation/60-OpenCV%20CUDA%20Accelerated/output/#lets-make-sure-python3-finds-opencv-cv2c17-hlwhoncvefnzl-c20","title":"[# Let's make sure Python3 finds OpenCV (CV2)]{.c17} {#h.lwhoncvefnzl .c20}","text":"<p>[# Need to adapt this for the python and OpenCV versions that you are using]{.c2}</p> <p>[]{.c2}</p> <p>[python3 -c \\'import cv2 as cv; print(cv.__version__)\\']{.c2}</p> <p>[#Traceback (most recent call last):]{.c2}</p> <p>[\u00a0 #File \\\"\\&lt;string&gt;\\\", line 1, in \\&lt;module&gt;]{.c2}</p> <p>[#ModuleNotFoundError: No module named \\'cv2\\']{.c2}</p> <p>[]{.c2}</p> <p>[# If you don't see the version of the OpenCV that you built listed, it is because you forgot to delete the previous]{.c2}</p> <p>[# OpenCV version that was installed, go back to this document \"Removing old version of OpenCV\" and then]{.c2}</p> <p>[# build and \u00a0install OpenCV again.]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[# What version of python3 do you have?]{.c2}</p> <p>[]{.c2}</p> <p>[python3]{.c2}</p> <p>[#Python 3.8.10 (default, May 26 2023, 14:05:08) ]{.c2}</p> <p>[#[GCC 9.4.0] on linux]{.c2}</p> <p>[#Type \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more information.]{.c2}</p> <p>[&gt;&gt;&gt; exit()]{.c2}</p> <p>[]{.c2}</p> <p>[ex:]{.c2}</p> <p>[cd /usr/local/lib/python3.6/site-packages/cv2/python-3.6]{.c2}</p> <p>[# or]{.c2}</p> <p>[cd /usr/local/lib/python3.8/site-packages/cv2/python-3.8]{.c2}</p> <p>[]{.c2}</p> <p>[# remove the previous cv2.so \u00a0in case it is still there.]{.c2}</p> <p>[sudo rm -rf cv2.so]{.c2}</p> <p># mv cv2.cpython-36m-[xxx]{.c26}[-linux-gnu.so cv2.so]{.c2}</p> <p>[]{.c2}</p> <p>[ls cv2.*]{.c2}</p> <p>[]{.c2}</p> <p>[# cv2.cpython-38-aarch64-linux-gnu.so]{.c2}</p> <p>[]{.c2}</p> <p>[ln -s /usr/local/lib/python3.8/site-packages/cv2/python-3.8cv2.cpython-38-aarch64-linux-gnu.so cv2.so cv2.so]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[sudo cp cv2.cpython-36m-aarch64-linux-gnu.so cv2.so]{.c2}</p> <p>[# or]{.c2}</p> <p>[sudo cp cv2.cpython-38-aarch64-linux-gnu.so cv2.so]{.c2}</p> <p>[\u00a0]{.c2}</p> <p>[# Alternatively you could just rename the file]{.c2}</p> <p>[# sudo mv cv2.cpython-36m-aarch64-linux-gnu.so cv2.so]{.c2}</p> <p>[# sudo cp cv2.cpython-38-aarch64-linux-gnu.so cv2.so]{.c2}</p> <p>[]{.c2}</p> <p>[# Lets create a link at the home directory for cv2]{.c2}</p> <p>[]{.c2}</p> <p>[cd \\~]{.c2}</p> <p>[sudo rm -rf cv2.so]{.c2}</p> <p>[ln -s /usr/local/lib/python3.6/site-packages/cv2/python-3.6/cv2.so cv2.so]{.c2}</p> <p>[# or ]{.c2}</p> <p>[ln -s /usr/local/lib/python3.8/site-packages/cv2/python-3.8/cv2.so cv2.so]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[Checking install for python3]{.c2}</p> <p>[python3]{.c2}</p> <p>[import cv2]{.c2}</p> <p>[cv2.__version__]{.c2}</p> <p>[exit ()]{.c2}</p> <p>[]{.c2}</p> <p>[# Resulting on something similar to this ]{.c2}</p> <p>[]{.c2}</p> <p>[jetson@ucsdrobocar-xxx-yy]{.c29 .c23 .c21}[:]{.c23}[\\~]{.c18}[\\$ python3]{.c9}</p> <p>[Python 3.6.9 (default, Jun 29 2022, 11:45:57) ]{.c9}</p> <p>[[GCC 8.4.0] on linux]{.c9}</p> <p>[Type \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more information.]{.c9}</p> <p>[&gt;&gt;&gt; import cv2]{.c9}</p> <p>[&gt;&gt;&gt; cv2.__version__]{.c9}</p> <p>[\\'4.6.0\\']{.c9}</p> <p>[&gt;&gt;&gt; exit()]{.c9}</p> <p>[jetson@ucsdrobocar-xxx-yy]{.c23 .c21 .c29}[:]{.c23}[\\~]{.c18}[\\$ ]{.c9}</p> <p>[]{.c2}</p> <p>[Python 3.8.10 (default, May 26 2023, 14:05:08) ]{.c2}</p> <p>[[GCC 9.4.0] on linux]{.c2}</p> <p>[Type \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more information.]{.c2}</p> <p>[&gt;&gt;&gt; import cv2]{.c2}</p> <p>[&gt;&gt;&gt; cv2.__version__]{.c2}</p> <p>[\\'4.8.0\\']{.c2}</p> <p>[&gt;&gt;&gt; exit ()]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[# Here is quicker way to test cv2 on Python 3]{.c2}</p> <p>[]{.c2}</p> <p>[python3 -c \\'import cv2 as cv; print(cv.__version__)\\']{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[4.6.0]{.c9}</p> <p>[# or]{.c9}</p> <p>[4.8.0]{.c9}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[]{.c2}</p> <p>[end of Installing an Open Source Computer Vision (OpenCV) package \u00a0with CUDA Support]{.c10}</p> <p>[]{.c10}</p> <p>[]{.c10}</p>   ------------------------------------------------------------------------  []{.c2}  [![](images/image5.png){style=\"width: 181.50px; height: 42.61px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 181.50px; height: 42.61px;\"}\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[![](images/image7.png){style=\"width: 49.86px; height: 49.86px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 49.86px; height: 49.86px;\"}\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[![](images/image3.png){style=\"width: 215.50px; height: 46.37px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\"}]{style=\"overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 215.50px; height: 46.37px;\"}  []{.c2}"}]}